<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>013_1MSongsKMeans_Stage1ETL - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-sydney.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":false,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":true,"enableLargeResultDownload":false,"nameAndEmail":"Raazesh Sainudiin (r.sainudiin@math.canterbury.ac.nz)","enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":true,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-30-9-162-U0c2673ac85-Sa2ee4664b2-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-30-9-162-U0c2673ac85-S33a1e4b9c6-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-30-9-162-U0c2673ac85-S5917a1044d-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.0","packageLabel":"spark-1.6-jenkins-ip-10-30-9-162-U0c2673ac85-Scabba801f3-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"prefetchSidebarNodes":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.12.3","accountsLimit":-1,"enableNotebookGitBranching":true,"local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableUserInviteWorkflow":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableCssTransitions":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"displayRowLimit":1000,"defaultSparkVersion":{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-30-9-162-U0c2673ac85-S5917a1044d-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"disallowAddingAdmins":false,"enableSparkConfUI":true,"enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"csrfToken":"1f2013f6-c2fd-4ab5-b68c-a2ff4e325639","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"someName":"Raazesh Sainudiin","enableWorkspaceAcls":true,"gitHash":"0c2673ac858e227cad536fdb45d140aeded238db","userFullname":"Raazesh Sainudiin","enableClusterCreatePage":false,"enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100005,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/","enableSparkPackages":true,"enableHybridClusterType":false,"enableNotebookHistoryUI":true,"availableWorkspaces":[{"name":"Workspace 0","orgId":0}],"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","enableHybridClusters":true,"notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"r.sainudiin@math.canterbury.ac.nz","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"enableNewInputWidgetUI":false,"accounts":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":63879,"name":"013_1MSongsKMeans_Stage1ETL","language":"scala","commands":[{"version":"CommandV1","origId":64562,"guid":"5f55b94e-4cf3-4261-b6c4-f12c74e6ab2d","subtype":"command","commandType":"auto","position":0.25,"command":"%md\n\n# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### prepared by [Raazesh Sainudiin](https://nz.linkedin.com/in/raazesh-sainudiin-45955845) and [Sivanand Sivaram](https://www.linkedin.com/in/sivanand)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a3d71a38-b535-4ee9-92ef-c419a8ad1056"},{"version":"CommandV1","origId":129724,"guid":"5e8df3ed-dc43-42e5-a1f1-ad9e1f070f42","subtype":"command","commandType":"auto","position":0.375,"command":"%md\nThe [html source url](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/week4/07_UnsupervisedClusteringKMeans_1MSongs/013_1MSongsKMeans_Stage1ETL.html) of this databricks notebook and its recorded Uji ![Image of Uji, Dogen's Time-Being](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/UjiTimeBeingDogen.png \"uji\"):\n\n[![sds/uji/week4/07_UnsupervisedClustering/013_KMeans_Stage1ETL](http://img.youtube.com/vi/_Lxtxmn0L-w/0.jpg)](https://www.youtube.com/v/_Lxtxmn0L-w?rel=0&autoplay=1&modestbranding=1&start=4823&end=5370)\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"7719583f-75a8-4549-b5f7-a94d40bf3bc0"},{"version":"CommandV1","origId":64467,"guid":"a00b24d1-3307-4d37-8f45-c4cb8a5fb803","subtype":"command","commandType":"auto","position":0.5,"command":"%md\n**SOURCE:** This is the scala version of the python notebook from the databricks Community Edition that has been added to this databricks shard at [Workspace -> scalable-data-science -> xtraResources -> dbCE -> MLlib -> unsupervised -> clustering -> k-means -> 1MSongsPy_ETLExploreModel](/#workspace/scalable-data-science/xtraResources/dbCE/MLlib/unsupervised/clustering/k-means/1MSongsPy_ETLExploreModel) as extra resources for this project-focussed course [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e8228cf8-307e-4570-ab15-0e5e230be1b7"},{"version":"CommandV1","origId":63881,"guid":"55e1b0fd-6c0c-4a82-97c7-a49b00225ce7","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n# Stage 1: Parsing songs data\n\n![ETL](http://training.databricks.com/databricks_guide/end-to-end-01.png)\n\nThis is the first notebook in this tutorial. In this notebook we will read data from DBFS (DataBricks FileSystem). We will parse data and load it as a table that can be readily used in following notebooks.\n\nBy going through this notebook you can expect to learn how to read distributed data as an RDD, how to transform RDDs, and how to construct a Spark DataFrame from an RDD and register it as a table.\n\nWe first explore different files in our distributed file system. We use a header file to construct a Spark `Schema` object. We write a function that takes the header and casts strings in each line of our data to corresponding types. Once we run this function on the data we find that it fails on some corner caes. We update our function and finally get a parsed RDD. We combine that RDD and the Schema to construct a DataFame and register it as a temporary table in SparkSQL.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4586a471-0c55-4007-9a19-541e5f5ecb28"},{"version":"CommandV1","origId":63882,"guid":"d9d84e42-4e9c-4d52-838b-8339277fa4a6","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n### Text data files are stored in `dbfs:/databricks-datasets/songs/data-001` \nYou can conveniently list files on distributed file system (DBFS, S3 or HDFS) using `%fs` commands.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"5cead6f1-2029-43e3-8be1-73db4badff60"},{"version":"CommandV1","origId":63887,"guid":"b557cc23-6cde-4b78-b094-ad8a3d5545f1","subtype":"command","commandType":"auto","position":2.0625,"command":"%fs ls /databricks-datasets/songs/data-001/","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/databricks-datasets/songs/data-001/header.txt","header.txt",377.0],["dbfs:/databricks-datasets/songs/data-001/part-00000","part-00000",52837.0],["dbfs:/databricks-datasets/songs/data-001/part-00001","part-00001",52469.0],["dbfs:/databricks-datasets/songs/data-001/part-00002","part-00002",51778.0],["dbfs:/databricks-datasets/songs/data-001/part-00003","part-00003",50551.0],["dbfs:/databricks-datasets/songs/data-001/part-00004","part-00004",53449.0],["dbfs:/databricks-datasets/songs/data-001/part-00005","part-00005",53301.0],["dbfs:/databricks-datasets/songs/data-001/part-00006","part-00006",54184.0],["dbfs:/databricks-datasets/songs/data-001/part-00007","part-00007",50924.0],["dbfs:/databricks-datasets/songs/data-001/part-00008","part-00008",52533.0],["dbfs:/databricks-datasets/songs/data-001/part-00009","part-00009",54570.0],["dbfs:/databricks-datasets/songs/data-001/part-00010","part-00010",54338.0],["dbfs:/databricks-datasets/songs/data-001/part-00011","part-00011",51836.0],["dbfs:/databricks-datasets/songs/data-001/part-00012","part-00012",52297.0],["dbfs:/databricks-datasets/songs/data-001/part-00013","part-00013",52044.0],["dbfs:/databricks-datasets/songs/data-001/part-00014","part-00014",50704.0],["dbfs:/databricks-datasets/songs/data-001/part-00015","part-00015",54158.0],["dbfs:/databricks-datasets/songs/data-001/part-00016","part-00016",50080.0],["dbfs:/databricks-datasets/songs/data-001/part-00017","part-00017",47708.0],["dbfs:/databricks-datasets/songs/data-001/part-00018","part-00018",8858.0],["dbfs:/databricks-datasets/songs/data-001/part-00019","part-00019",53323.0],["dbfs:/databricks-datasets/songs/data-001/part-00020","part-00020",57877.0],["dbfs:/databricks-datasets/songs/data-001/part-00021","part-00021",52491.0],["dbfs:/databricks-datasets/songs/data-001/part-00022","part-00022",54791.0],["dbfs:/databricks-datasets/songs/data-001/part-00023","part-00023",50682.0],["dbfs:/databricks-datasets/songs/data-001/part-00024","part-00024",52863.0],["dbfs:/databricks-datasets/songs/data-001/part-00025","part-00025",47416.0],["dbfs:/databricks-datasets/songs/data-001/part-00026","part-00026",50130.0],["dbfs:/databricks-datasets/songs/data-001/part-00027","part-00027",53462.0],["dbfs:/databricks-datasets/songs/data-001/part-00028","part-00028",54179.0],["dbfs:/databricks-datasets/songs/data-001/part-00029","part-00029",52738.0],["dbfs:/databricks-datasets/songs/data-001/part-00030","part-00030",54159.0],["dbfs:/databricks-datasets/songs/data-001/part-00031","part-00031",51247.0],["dbfs:/databricks-datasets/songs/data-001/part-00032","part-00032",51610.0],["dbfs:/databricks-datasets/songs/data-001/part-00033","part-00033",53895.0],["dbfs:/databricks-datasets/songs/data-001/part-00034","part-00034",53125.0],["dbfs:/databricks-datasets/songs/data-001/part-00035","part-00035",54066.0],["dbfs:/databricks-datasets/songs/data-001/part-00036","part-00036",54265.0],["dbfs:/databricks-datasets/songs/data-001/part-00037","part-00037",54264.0],["dbfs:/databricks-datasets/songs/data-001/part-00038","part-00038",50540.0],["dbfs:/databricks-datasets/songs/data-001/part-00039","part-00039",55193.0],["dbfs:/databricks-datasets/songs/data-001/part-00040","part-00040",54537.0],["dbfs:/databricks-datasets/songs/data-001/part-00041","part-00041",52402.0],["dbfs:/databricks-datasets/songs/data-001/part-00042","part-00042",54673.0],["dbfs:/databricks-datasets/songs/data-001/part-00043","part-00043",53009.0],["dbfs:/databricks-datasets/songs/data-001/part-00044","part-00044",51789.0],["dbfs:/databricks-datasets/songs/data-001/part-00045","part-00045",52986.0],["dbfs:/databricks-datasets/songs/data-001/part-00046","part-00046",54442.0],["dbfs:/databricks-datasets/songs/data-001/part-00047","part-00047",52971.0],["dbfs:/databricks-datasets/songs/data-001/part-00048","part-00048",53331.0],["dbfs:/databricks-datasets/songs/data-001/part-00049","part-00049",44263.0],["dbfs:/databricks-datasets/songs/data-001/part-00050","part-00050",54841.0],["dbfs:/databricks-datasets/songs/data-001/part-00051","part-00051",54306.0],["dbfs:/databricks-datasets/songs/data-001/part-00052","part-00052",53610.0],["dbfs:/databricks-datasets/songs/data-001/part-00053","part-00053",53573.0],["dbfs:/databricks-datasets/songs/data-001/part-00054","part-00054",53854.0],["dbfs:/databricks-datasets/songs/data-001/part-00055","part-00055",54236.0],["dbfs:/databricks-datasets/songs/data-001/part-00056","part-00056",54455.0],["dbfs:/databricks-datasets/songs/data-001/part-00057","part-00057",52307.0],["dbfs:/databricks-datasets/songs/data-001/part-00058","part-00058",52313.0],["dbfs:/databricks-datasets/songs/data-001/part-00059","part-00059",52446.0],["dbfs:/databricks-datasets/songs/data-001/part-00060","part-00060",51958.0],["dbfs:/databricks-datasets/songs/data-001/part-00061","part-00061",53859.0],["dbfs:/databricks-datasets/songs/data-001/part-00062","part-00062",53698.0],["dbfs:/databricks-datasets/songs/data-001/part-00063","part-00063",54482.0],["dbfs:/databricks-datasets/songs/data-001/part-00064","part-00064",40182.0],["dbfs:/databricks-datasets/songs/data-001/part-00065","part-00065",54410.0],["dbfs:/databricks-datasets/songs/data-001/part-00066","part-00066",49123.0],["dbfs:/databricks-datasets/songs/data-001/part-00067","part-00067",50796.0],["dbfs:/databricks-datasets/songs/data-001/part-00068","part-00068",49561.0],["dbfs:/databricks-datasets/songs/data-001/part-00069","part-00069",52294.0],["dbfs:/databricks-datasets/songs/data-001/part-00070","part-00070",51250.0],["dbfs:/databricks-datasets/songs/data-001/part-00071","part-00071",58942.0],["dbfs:/databricks-datasets/songs/data-001/part-00072","part-00072",54589.0],["dbfs:/databricks-datasets/songs/data-001/part-00073","part-00073",54233.0],["dbfs:/databricks-datasets/songs/data-001/part-00074","part-00074",54725.0],["dbfs:/databricks-datasets/songs/data-001/part-00075","part-00075",54877.0],["dbfs:/databricks-datasets/songs/data-001/part-00076","part-00076",54333.0],["dbfs:/databricks-datasets/songs/data-001/part-00077","part-00077",51927.0],["dbfs:/databricks-datasets/songs/data-001/part-00078","part-00078",51744.0],["dbfs:/databricks-datasets/songs/data-001/part-00079","part-00079",53187.0],["dbfs:/databricks-datasets/songs/data-001/part-00080","part-00080",43246.0],["dbfs:/databricks-datasets/songs/data-001/part-00081","part-00081",54269.0],["dbfs:/databricks-datasets/songs/data-001/part-00082","part-00082",48464.0],["dbfs:/databricks-datasets/songs/data-001/part-00083","part-00083",52144.0],["dbfs:/databricks-datasets/songs/data-001/part-00084","part-00084",53375.0],["dbfs:/databricks-datasets/songs/data-001/part-00085","part-00085",55139.0],["dbfs:/databricks-datasets/songs/data-001/part-00086","part-00086",50924.0],["dbfs:/databricks-datasets/songs/data-001/part-00087","part-00087",52013.0],["dbfs:/databricks-datasets/songs/data-001/part-00088","part-00088",54262.0],["dbfs:/databricks-datasets/songs/data-001/part-00089","part-00089",53007.0],["dbfs:/databricks-datasets/songs/data-001/part-00090","part-00090",55142.0],["dbfs:/databricks-datasets/songs/data-001/part-00091","part-00091",52049.0],["dbfs:/databricks-datasets/songs/data-001/part-00092","part-00092",54714.0],["dbfs:/databricks-datasets/songs/data-001/part-00093","part-00093",52906.0],["dbfs:/databricks-datasets/songs/data-001/part-00094","part-00094",52188.0],["dbfs:/databricks-datasets/songs/data-001/part-00095","part-00095",50768.0],["dbfs:/databricks-datasets/songs/data-001/part-00096","part-00096",55242.0],["dbfs:/databricks-datasets/songs/data-001/part-00097","part-00097",52059.0],["dbfs:/databricks-datasets/songs/data-001/part-00098","part-00098",52982.0],["dbfs:/databricks-datasets/songs/data-001/part-00099","part-00099",52015.0],["dbfs:/databricks-datasets/songs/data-001/part-00100","part-00100",51467.0],["dbfs:/databricks-datasets/songs/data-001/part-00101","part-00101",50926.0],["dbfs:/databricks-datasets/songs/data-001/part-00102","part-00102",55018.0],["dbfs:/databricks-datasets/songs/data-001/part-00103","part-00103",50043.0],["dbfs:/databricks-datasets/songs/data-001/part-00104","part-00104",51936.0],["dbfs:/databricks-datasets/songs/data-001/part-00105","part-00105",57311.0],["dbfs:/databricks-datasets/songs/data-001/part-00106","part-00106",55090.0],["dbfs:/databricks-datasets/songs/data-001/part-00107","part-00107",54396.0],["dbfs:/databricks-datasets/songs/data-001/part-00108","part-00108",56594.0],["dbfs:/databricks-datasets/songs/data-001/part-00109","part-00109",53260.0],["dbfs:/databricks-datasets/songs/data-001/part-00110","part-00110",42007.0],["dbfs:/databricks-datasets/songs/data-001/part-00119","part-00119",0.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\""},{"name":"name","type":"\"string\""},{"name":"size","type":"\"long\""}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"startTime":1.458276394084E12,"submitTime":1.458276286203E12,"finishTime":1.458276397109E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"40f40ac7-ee7e-4f4b-9f44-60dde214438e"},{"version":"CommandV1","origId":63886,"guid":"6b4cf88a-e833-414c-a968-d1da61b10b96","subtype":"command","commandType":"auto","position":2.125,"command":"%md\nAs you can see in the listing we have data files and a single header file. The header file seems interesting and worth a first inspection at first. The file is 377 bytes, therefore it is safe to collect the entire content of the file in the notebook. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"3a779876-85aa-4553-ba6d-a645d831bc81"},{"version":"CommandV1","origId":63885,"guid":"275a7de8-2a7e-4fd4-bbc3-27a55dff621a","subtype":"command","commandType":"auto","position":2.25,"command":"sc.textFile(\"databricks-datasets/songs/data-001/header.txt\").collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res1: Array[String] = Array(artist_id:string, artist_latitude:double, artist_longitude:double, artist_location:string, artist_name:string, duration:double, end_of_fade_in:double, key:int, key_confidence:double, loudness:double, release:string, song_hotnes:double, song_id:string, start_of_fade_out:double, tempo:double, time_signature:double, time_signature_confidence:double, title:string, year:double, partial_sequence:int)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.458276411034E12,"submitTime":1.458276303157E12,"finishTime":1.458276412585E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"5c1bb2ff-70a8-442f-bd45-8d3595ed273c"},{"version":"CommandV1","origId":63971,"guid":"ee17562f-8ce7-4dc7-ac3d-6ff107f597d5","subtype":"command","commandType":"auto","position":2.375,"command":"//sc.textFile(\"databricks-datasets/songs/data-001/header.txt\").collect.map(println) // uncomment to see line-by-line","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:36: error: value head is not a member of org.apache.spark.rdd.RDD[String]\n              sc.textFile(&quot;databricks-datasets/songs/data-001/header.txt&quot;).head(5)\n                                                                           ^\n</div>","error":null,"startTime":1.458190469695E12,"submitTime":1.458190366086E12,"finishTime":1.458190469849E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"3a14f67f-4097-426f-a67d-1ccc5c4aec7e"},{"version":"CommandV1","origId":63884,"guid":"4d4d35e6-f9c9-409e-ab3a-0a4aac880703","subtype":"command","commandType":"auto","position":2.5,"command":"%md\nAs seen above each line in the header consists of a name and a type separated by colon. We will need to parse the header file as follows:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4fe05bcd-ad24-43db-ad0a-d875b6705ae8"},{"version":"CommandV1","origId":63883,"guid":"b4d44d49-26c5-4ae9-a692-f26d11d4e9f1","subtype":"command","commandType":"auto","position":3.0,"command":"val header = sc.textFile(\"/databricks-datasets/songs/data-001/header.txt\").map(line => {\n                val headerElement = line.split(\":\")\n                (headerElement(0), headerElement(1))\n            }\n           ).collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">header: Array[(String, String)] = Array((artist_id,string), (artist_latitude,double), (artist_longitude,double), (artist_location,string), (artist_name,string), (duration,double), (end_of_fade_in,double), (key,int), (key_confidence,double), (loudness,double), (release,string), (song_hotnes,double), (song_id,string), (start_of_fade_out,double), (tempo,double), (time_signature,double), (time_signature_confidence,double), (title,string), (year,double), (partial_sequence,int))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:4: error: identifier expected but integer literal found.\n                       (headerElement[0], headerElement[1])\n                                      ^\n&lt;console&gt;:5: error: ']' expected but '}' found.\n                   }\n                   ^\n</div>","error":null,"startTime":1.458276437614E12,"submitTime":1.458276329718E12,"finishTime":1.458276438432E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"75409b4f-5251-4166-b131-4cb38b7aa31f"},{"version":"CommandV1","origId":63972,"guid":"80f7bd1c-63cc-4ab5-97d1-6bbd73a84e07","subtype":"command","commandType":"auto","position":3.0078125,"command":"%md\nLet's define a `case class` called `Song` that will be used to represent each row of data in the files:\n* `/databricks-datasets/songs/data-001/part-00000` through `/databricks-datasets/songs/data-001/part-00119` or the last `.../part-*****` file.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a540d294-ac0f-4c95-8767-67cc4038b8ea"},{"version":"CommandV1","origId":63895,"guid":"8cf9c329-47ee-4907-9b68-5fd0bba9fa13","subtype":"command","commandType":"auto","position":3.015625,"command":"case class Song(artist_id: String, artist_latitude: Double, artist_longitude: Double, artist_location: String, artist_name: String, duration: Double, end_of_fade_in: Double, key: Int, key_confidence: Double, loudness: Double, release: String, song_hotness: Double, song_id: String, start_of_fade_out: Double, tempo: Double, time_signature: Double, time_signature_confidence: Double, title: String, year: Double, partial_sequence: Int)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">defined class Song\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:2: error: identifier expected but '(' found.\n       case class(artist_id: String, artist_latitude: Double, artist_longitude: Double, artist_location: String, artist_name: String, duration: Double, end_of_fade_in: Double, key: Int, key_confidence: Double, loudness: Double, release: String, song_hotness: Double, song_id: String, start_of_fade_out: Double, tempo: Double, time_signature: Double, time_signature_confidence: Double, title: String, year: Double, partial_sequence: Int)\n                 ^\n&lt;console&gt;:2: warning: case classes without a parameter list have been deprecated;\nuse either case objects or case classes with `()' as parameter list.\n       case class(artist_id: String, artist_latitude: Double, artist_longitude: Double, artist_location: String, artist_name: String, duration: Double, end_of_fade_in: Double, key: Int, key_confidence: Double, loudness: Double, release: String, song_hotness: Double, song_id: String, start_of_fade_out: Double, tempo: Double, time_signature: Double, time_signature_confidence: Double, title: String, year: Double, partial_sequence: Int)\n                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^\n</div>","error":null,"startTime":1.458276451426E12,"submitTime":1.458276343546E12,"finishTime":1.458276452052E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"632c119b-7e9c-4526-bc39-1ef433e5bba3"},{"version":"CommandV1","origId":63893,"guid":"e9c48870-9339-4827-9ea2-dbda809d9307","subtype":"command","commandType":"auto","position":3.03125,"command":"%md\nNow we turn to data files. First, step is inspecting the first line of data to inspect its format.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"ee09fd39-6750-4d0c-9384-47d31704b3df"},{"version":"CommandV1","origId":63894,"guid":"9e7432a7-7794-436c-bfe1-94847f034586","subtype":"command","commandType":"auto","position":3.046875,"command":"// this is loads all the data - a subset of the 1M songs dataset\nval dataRDD = sc.textFile(\"/databricks-datasets/songs/data-001/part-*\") ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">dataRDD: org.apache.spark.rdd.RDD[String] = /databricks-datasets/songs/data-001/part-* MapPartitionsRDD[28478] at textFile at &lt;console&gt;:35\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.458336780955E12,"submitTime":1.458336670051E12,"finishTime":1.458336781033E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"37721041-0ab9-4469-b86c-8b6f1ab1486e"},{"version":"CommandV1","origId":76108,"guid":"08e9b5ec-ac8c-4b3e-9f85-0979cb8f22d1","subtype":"command","commandType":"auto","position":3.0546875,"command":"dataRDD.count // number of songs","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res6: Long = 31369\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.458336783938E12,"submitTime":1.458336673046E12,"finishTime":1.458336788918E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"876da4d3-2795-4404-968b-91b14ec0f0cb"},{"version":"CommandV1","origId":63892,"guid":"cb4da76c-0f53-4bc9-95ae-cb4ed7715d16","subtype":"command","commandType":"auto","position":3.0625,"command":"dataRDD.take(3)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res7: Array[String] = Array(AR81V6H1187FB48872\tnan\tnan\t\tEarl Sixteen\t213.7073\t0.0\t11\t0.419\t-12.106\tSoldier of Jah Army\tnan\tSOVNZSZ12AB018A9B8\t208.289\t125.882\t1\t0.0\tRastaman\t2003\t--, ARVVZQP11E2835DBCB\tnan\tnan\t\tWavves\t133.25016\t0.0\t0\t0.282\t0.596\tWavvves\t0.471578247701\tSOJTQHQ12A8C143C5F\t128.116\t89.519\t1\t0.0\tI Want To See You (And Go To The Movies)\t2009\t--, ARFG9M11187FB3BBCB\tnan\tnan\tNashua USA\tC-Side\t247.32689\t0.0\t9\t0.612\t-4.896\tSanta Festival Compilation 2008 vol.1\tnan\tSOAJSQL12AB0180501\t242.196\t171.278\t5\t1.0\tLoose on the Dancefloor\t0\t225261)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.458336799299E12,"submitTime":1.458336688412E12,"finishTime":1.458336799524E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"673bf3d7-5cf6-4d6c-9674-2db04c2da111"},{"version":"CommandV1","origId":63891,"guid":"b938b9d5-4732-4cbc-b29e-f9913ea060c7","subtype":"command","commandType":"auto","position":3.125,"command":"%md\nEach line of data consists of multiple fields separated by `\\t`. With that information and what we learned from the header file, we set out to parse our data.\n* We have already created a case class based on the header (which seems to agree with the 3 lines above).\n* Next, we will create a function that takes each line as input and returns the case class as output.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"423552d4-34f2-4ffa-8dcc-35ebc9d504a9"},{"version":"CommandV1","origId":63890,"guid":"b9f8ddc1-fc6d-40e1-a57d-fe009e37c042","subtype":"command","commandType":"auto","position":3.25,"command":"def parseLine(line: String): Song = {\n  \n  val tokens = line.split(\"\\t\")\n  Song(tokens(0), tokens(1).toDouble, tokens(2).toDouble, tokens(3), tokens(4), tokens(5).toDouble, tokens(6).toDouble, tokens(7).toInt, tokens(8).toDouble, tokens(9).toDouble, tokens(10), tokens(11).toDouble, tokens(12), tokens(13).toDouble, tokens(14).toDouble, tokens(15).toDouble, tokens(16).toDouble, tokens(17), tokens(18).toDouble, tokens(19).toInt)\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">parseLine: (line: String)Song\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:39: error: too many arguments for method apply: (artist_id: String, artist_latitude: Double, artist_longitude: Double, artist_location: String, artist_name: String, duration: Double, end_of_fade_in: Double, key: Int, key_confidence: Double, loudness: Double, release: String, song_hotness: Double, song_id: String, start_of_fade_out: Double, tempo: Double, time_signature: Double, time_signature_confidence: Double, title: String, year: Double, partial_sequence: Int)Song in object Song\n         Song(tokens(0), tokens(1).toDouble, tokens(2).toDouble, tokens(3), tokens(4), tokens(5).toDouble, tokens(6).toDouble, tokens(7).toInt, tokens(8).toDouble, tokens(9).toDouble, tokens(10).toDouble, tokens(11), tokens(12).toDouble, tokens(13), tokens(14).toDouble, tokens(15).toDouble, tokens(16).toDouble, tokens(17).toDouble, tokens(18), tokens(19).toDouble, tokens(20).toInt)\n             ^\n</div>","error":null,"startTime":1.4583368049E12,"submitTime":1.458336693994E12,"finishTime":1.45833680508E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"c2fb1b75-0414-408b-bb34-4c37142137d8"},{"version":"CommandV1","origId":63897,"guid":"6fc1c29a-1813-499b-83dc-b9dc01ed2038","subtype":"command","commandType":"auto","position":3.375,"command":"%md\nWith this function we can transform the dataRDD to another RDD that consists of Song case classes","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"49217868-fcf4-4005-9eb7-ed8acb89334a"},{"version":"CommandV1","origId":63889,"guid":"1d190aaa-a83c-42bd-a844-41018bb3226a","subtype":"command","commandType":"auto","position":3.5,"command":"val parsedRDD = dataRDD.map(parseLine)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">parsedRDD: org.apache.spark.rdd.RDD[Song] = MapPartitionsRDD[28485] at map at &lt;console&gt;:40\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:40: error: type mismatch;\n found   : Song\n required: Song\n       val parsedRDD: RDD[Song] = dataRDD.map(parseLine)\n                                              ^\n</div>","error":null,"startTime":1.45833680781E12,"submitTime":1.458336696922E12,"finishTime":1.458336807927E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"38026ebf-ef68-4344-867e-c551225251e4"},{"version":"CommandV1","origId":63898,"guid":"a98c4c72-8ac5-48bd-8c84-dea3290a0262","subtype":"command","commandType":"auto","position":3.75,"command":"%md\nTo convert an RDD of case classes to a DataFrame, we just need to call the toDF method","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"60d7982c-e29b-4c34-9e15-64a9058d04dd"},{"version":"CommandV1","origId":63888,"guid":"8a56f071-1769-4bb6-ac33-ef7c136b7007","subtype":"command","commandType":"auto","position":4.0,"command":"val df = parsedRDD.toDF","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [artist_id: string, artist_latitude: double, artist_longitude: double, artist_location: string, artist_name: string, duration: double, end_of_fade_in: double, key: int, key_confidence: double, loudness: double, release: string, song_hotness: double, song_id: string, start_of_fade_out: double, tempo: double, time_signature: double, time_signature_confidence: double, title: string, year: double, partial_sequence: int]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.458336810161E12,"submitTime":1.458336699261E12,"finishTime":1.45833681081E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"ba68ac24-09ea-45e4-b3a2-c45489f15893"},{"version":"CommandV1","origId":63903,"guid":"da7c5077-cd98-4c59-81d2-3af59ab51dc5","subtype":"command","commandType":"auto","position":4.03125,"command":"%md\nOnce we get a DataFrame we can register it as a temporary table. That will allow us to use its name in SQL queries.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"1f55a223-1e65-4451-89f2-eacd73aeaf7c"},{"version":"CommandV1","origId":63902,"guid":"c6be4bb9-1a54-4f81-8fb7-39a354e89f3c","subtype":"command","commandType":"auto","position":4.0625,"command":"df.registerTempTable(\"songsTable\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.458336814142E12,"submitTime":1.458336703237E12,"finishTime":1.458336814207E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"cec1fc51-efb0-4b92-87ae-bdcbbb4ea1a5"},{"version":"CommandV1","origId":63901,"guid":"37040d1c-9b84-4de0-bacb-9a90314a913a","subtype":"command","commandType":"auto","position":4.125,"command":"%md\nWe can now cache our table. So far all operations have been lazy. This is the first time Spark will attempt to actually read all our data and apply the transformations. \n\n**If you are running Spark 1.6+ the next command will throw a parsing error.**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"65f762dc-9cdb-4918-9a0b-45c3e4427008"},{"version":"CommandV1","origId":63900,"guid":"35981a1b-75d9-4691-9f7c-6fa9ac0e7dac","subtype":"command","commandType":"auto","position":4.25,"command":"%sql cache table songsTable","commandVersion":0,"state":"error","results":null,"errorSummary":"Error in SQL statement: SparkException: Job aborted due to stage failure: Task 2 in stage 454.0 failed 4 times, most recent failure: Lost task 2.3 in stage 454.0 (TID 2653, ip-10-135-216-118.ap-southeast-2.compute.internal): java.lang.NumberFormatException: For input string: \"nan\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:232)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:31)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d25.$read$$iwC$$iwC$$iwC$$iwC.parseLine(<console>:37)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d26.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:40)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d26.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:40)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:140)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:130)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:283)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:","error":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 454.0 failed 4 times, most recent failure: Lost task 2.3 in stage 454.0 (TID 2653, ip-10-135-216-118.ap-southeast-2.compute.internal): java.lang.NumberFormatException: For input string: \"nan\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:232)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:31)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d25.$read$$iwC$$iwC$$iwC$$iwC.parseLine(<console>:37)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d26.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:40)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d26.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:40)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:140)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:130)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:283)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1861)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1932)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1537)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1544)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1554)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1553)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2138)\n\tat org.apache.spark.sql.DataFrame.count(DataFrame.scala:1553)\n\tat org.apache.spark.sql.execution.CacheTableCommand.run(commands.scala:266)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:816)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$5.apply(DriverLocal.scala:304)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$5.apply(DriverLocal.scala:284)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:284)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:462)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:364)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:195)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NumberFormatException: For input string: \"nan\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:232)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:31)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d25.$read$$iwC$$iwC$$iwC$$iwC.parseLine(<console>:37)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d26.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:40)\n\tat linecc674ffa90bb4cad8e1e6a53bead3e3d26.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:40)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:140)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:130)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:283)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:319)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:462)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:364)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:195)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.458336706929E12,"submitTime":1.458336706929E12,"finishTime":1.458336707981E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"70974705-8f8e-457d-84da-d6475d2d7dd0"},{"version":"CommandV1","origId":63899,"guid":"b7cc4b35-be90-411e-bd35-4dd47a8c199e","subtype":"command","commandType":"auto","position":4.5,"command":"%md\nThe error means that we are trying to convert a missing value to a Double. Here is an updated version of the parseLine function to deal with missing values","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"3280bc3c-96ed-4b20-94ae-07e947e870f0"},{"version":"CommandV1","origId":63896,"guid":"cc2ac312-f9f0-452b-8c42-2e8f6ac172b5","subtype":"command","commandType":"auto","position":5.0,"command":"def parseLine(line: String): Song = {\n  \n  \n  def toDouble(value: String, defaultVal: Double): Double = {\n    try {\n       value.toDouble\n    } catch {\n      case e: Exception => defaultVal\n    }\n  }\n\n  def toInt(value: String, defaultVal: Int): Int = {\n    try {\n       value.toInt\n      } catch {\n      case e: Exception => defaultVal\n    }\n  }\n  \n  val tokens = line.split(\"\\t\")\n  Song(tokens(0), toDouble(tokens(1), 0.0), toDouble(tokens(2), 0.0), tokens(3), tokens(4), toDouble(tokens(5), 0.0), toDouble(tokens(6), 0.0), toInt(tokens(7), -1), toDouble(tokens(8), 0.0), toDouble(tokens(9), 0.0), tokens(10), toDouble(tokens(11), 0.0), tokens(12), toDouble(tokens(13), 0.0), toDouble(tokens(14), 0.0), toDouble(tokens(15), 0.0), toDouble(tokens(16), 0.0), tokens(17), toDouble(tokens(18), 0.0), toInt(tokens(19), -1))\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">parseLine: (line: String)Song\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:9: error: ';' expected but 'catch' found.\n           } catch {\n             ^\n&lt;console&gt;:18: error: ';' expected but 'catch' found.\n           } catch {\n             ^\n</div>","error":null,"startTime":1.45833682808E12,"submitTime":1.458336717176E12,"finishTime":1.4583368282E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"81ebdc05-2f0e-483f-923f-4ff8960a9ab7"},{"version":"CommandV1","origId":63906,"guid":"1eb84741-d314-456f-8272-e3bc71d410d1","subtype":"command","commandType":"auto","position":5.5,"command":"val df = dataRDD.map(parseLine).toDF\ndf.registerTempTable(\"songsTable\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [artist_id: string, artist_latitude: double, artist_longitude: double, artist_location: string, artist_name: string, duration: double, end_of_fade_in: double, key: int, key_confidence: double, loudness: double, release: string, song_hotness: double, song_id: string, start_of_fade_out: double, tempo: double, time_signature: double, time_signature_confidence: double, title: string, year: double, partial_sequence: int]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:47: error: package schema is not a value\n              val df = sqlContext.createDataFrame(dataRDD.map(parseLine), schema)\n                                                                          ^\n</div>","error":null,"startTime":1.458336831112E12,"submitTime":1.458336720207E12,"finishTime":1.458336831434E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"b10788af-3c81-430d-8ee3-cd96bf01e9e9"},{"version":"CommandV1","origId":63907,"guid":"c604d988-84d9-4886-bcf4-db536f2dd8a6","subtype":"command","commandType":"auto","position":5.75,"command":"%md\nAnd let's try caching the table. We are going to access this data multiple times in following notebooks, therefore it is a good idea to cache it in memory for faster subsequent access.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"d0969e16-57d5-4fc4-8a74-c74b736c9ad1"},{"version":"CommandV1","origId":63904,"guid":"bf50a7cc-b9d3-4b4c-a224-2f064b2e34c4","subtype":"command","commandType":"auto","position":6.0,"command":"%sql cache table songsTable","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"Error in SQL statement: SparkException: Job aborted due to stage failure: Task 1 in stage 444.0 failed 4 times, most recent failure: Lost task 1.3 in stage 444.0 (TID 12942, ip-10-135-216-120.ap-southeast-2.compute.internal): java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer\n\tat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)\n\tat line4f91d3b69ca34227b283b01ad9782f9047.$read$$iwC$$iwC$$iwC$$iwC.parseLine(<console>:50)\n\tat line4f91d3b69ca34227b283b01ad9782f9048.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:45)\n\tat line4f91d3b69ca34227b283b01ad9782f9048.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:45)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:140)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:130)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:283)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:","error":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 444.0 failed 4 times, most recent failure: Lost task 1.3 in stage 444.0 (TID 12942, ip-10-135-216-120.ap-southeast-2.compute.internal): java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer\n\tat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)\n\tat line4f91d3b69ca34227b283b01ad9782f9047.$read$$iwC$$iwC$$iwC$$iwC.parseLine(<console>:50)\n\tat line4f91d3b69ca34227b283b01ad9782f9048.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:45)\n\tat line4f91d3b69ca34227b283b01ad9782f9048.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:45)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:140)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:130)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:283)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1861)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1932)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1537)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1544)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1554)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1553)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2138)\n\tat org.apache.spark.sql.DataFrame.count(DataFrame.scala:1553)\n\tat org.apache.spark.sql.execution.CacheTableCommand.run(commands.scala:266)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:816)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$5.apply(DriverLocal.scala:304)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$5.apply(DriverLocal.scala:284)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:284)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:462)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:364)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:195)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer\n\tat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)\n\tat line4f91d3b69ca34227b283b01ad9782f9047.$read$$iwC$$iwC$$iwC$$iwC.parseLine(<console>:50)\n\tat line4f91d3b69ca34227b283b01ad9782f9048.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:45)\n\tat line4f91d3b69ca34227b283b01ad9782f9048.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:45)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:140)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:130)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:283)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:319)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:465)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:462)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:364)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:195)\n\tat java.lang.Thread.run(Thread.java:745)\n","startTime":1.458336833244E12,"submitTime":1.458336722339E12,"finishTime":1.458336838997E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4fc9dff6-cb5e-4c43-86a6-c1f4142ccf53"},{"version":"CommandV1","origId":63905,"guid":"3199defd-1d16-4043-bd54-ec3a5f462281","subtype":"command","commandType":"auto","position":7.0,"command":"%md\nFrom now on we can easily query our data using the temporary table we just created and cached in memory. Since it is registered as a table we can conveniently use SQL as well as Spark API to access it.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"f677c570-b57b-4bcc-9bb1-2569344911e8"},{"version":"CommandV1","origId":63915,"guid":"113d8468-7165-4983-adaf-3f2c728fc266","subtype":"command","commandType":"auto","position":7.0078125,"command":"%sql select * from songsTable limit 10","commandVersion":0,"state":"finished","results":{"type":"table","data":[["AR81V6H1187FB48872",0.0,0.0,"","Earl Sixteen",213.7073,0.0,11.0,0.419,-12.106,"Soldier of Jah Army",0.0,"SOVNZSZ12AB018A9B8",208.289,125.882,1.0,0.0,"Rastaman",2003.0,-1.0],["ARVVZQP11E2835DBCB",0.0,0.0,"","Wavves",133.25016,0.0,0.0,0.282,0.596,"Wavvves",0.471578247701,"SOJTQHQ12A8C143C5F",128.116,89.519,1.0,0.0,"I Want To See You (And Go To The Movies)",2009.0,-1.0],["ARFG9M11187FB3BBCB",0.0,0.0,"Nashua USA","C-Side",247.32689,0.0,9.0,0.612,-4.896,"Santa Festival Compilation 2008 vol.1",0.0,"SOAJSQL12AB0180501",242.196,171.278,5.0,1.0,"Loose on the Dancefloor",0.0,225261.0],["ARK4Z2O1187FB45FF0",0.0,0.0,"","Harvest",337.05751,0.247,4.0,0.46,-9.092,"Underground Community",0.0,"SOTDRVW12AB018BEB9",327.436,84.986,4.0,0.673,"No Return",0.0,101619.0],["AR4VQSG1187FB57E18",35.25082,-91.74015,"Searcy, AR","Gossip",430.23628,0.0,2.0,0.034,-6.846,"Yr  Mangled Heart",0.0,"SOTVOCL12A8AE478DD",424.06,121.998,4.0,0.847,"Yr Mangled Heart",2006.0,740623.0],["ARNBV1X1187B996249",0.0,0.0,"","Alex",186.80118,0.0,4.0,0.641,-16.108,"Jolgaledin",0.0,"SODTGRY12AB0182438",166.156,140.735,4.0,0.055,"Mariu Sonur Jesus",0.0,673970.0],["ARXOEZX1187B9B82A1",0.0,0.0,"","Elie Attieh",361.89995,0.0,7.0,0.863,-4.919,"ELITE",0.0,"SOIINTJ12AB0180BA6",354.476,128.024,4.0,0.399,"Fe Yom We Leila",0.0,280304.0],["ARXPUIA1187B9A32F1",0.0,0.0,"Rome, Italy","Simone Cristicchi",220.00281,2.119,4.0,0.486,-6.52,"Dall'Altra Parte Del Cancello",0.484225272411,"SONHXJK12AAF3B5290",214.761,99.954,1.0,0.928,"L'Italiano",2007.0,745962.0],["ARNPPTH1187B9AD429",51.4855,-0.37196,"Heston, Middlesex, England","Jimmy Page",156.86485,0.334,7.0,0.493,-9.962,"No Introduction Necessary [Deluxe Edition]",0.0,"SOGUHGW12A58A80E06",149.269,162.48,4.0,0.534,"Wailing Sounds",2004.0,599250.0],["AROGWRA122988FEE45",0.0,0.0,"","Christos Dantis",256.67873,2.537,9.0,0.742,-13.404,"Daktilika Apotipomata",0.0,"SOJJOYI12A8C13399D",248.912,134.944,4.0,0.162,"Stin Proigoumeni Zoi",0.0,611396.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"artist_id","type":"\"string\""},{"name":"artist_latitude","type":"\"double\""},{"name":"artist_longitude","type":"\"double\""},{"name":"artist_location","type":"\"string\""},{"name":"artist_name","type":"\"string\""},{"name":"duration","type":"\"double\""},{"name":"end_of_fade_in","type":"\"double\""},{"name":"key","type":"\"integer\""},{"name":"key_confidence","type":"\"double\""},{"name":"loudness","type":"\"double\""},{"name":"release","type":"\"string\""},{"name":"song_hotness","type":"\"double\""},{"name":"song_id","type":"\"string\""},{"name":"start_of_fade_out","type":"\"double\""},{"name":"tempo","type":"\"double\""},{"name":"time_signature","type":"\"double\""},{"name":"time_signature_confidence","type":"\"double\""},{"name":"title","type":"\"string\""},{"name":"year","type":"\"double\""},{"name":"partial_sequence","type":"\"integer\""}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"startTime":1.458336841421E12,"submitTime":1.458336730481E12,"finishTime":1.458336841534E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"002d7dc4-55a7-4339-85d2-a1a21c020821"},{"version":"CommandV1","origId":63914,"guid":"5a232afe-ad90-4c19-a7c7-dc16f1dbb126","subtype":"command","commandType":"auto","position":7.015625,"command":"%md\nNext up is exploring this data. Click on the Exploration notebook to continue the tutorial.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4fbc196c-599b-4a4e-b7d1-712c7fd74962"},{"version":"CommandV1","origId":64563,"guid":"6d961402-edd7-4603-be59-6da714106a01","subtype":"command","commandType":"auto","position":8.015625,"command":"%md\n\n# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### prepared by [Raazesh Sainudiin](https://nz.linkedin.com/in/raazesh-sainudiin-45955845) and [Sivanand Sivaram](https://www.linkedin.com/in/sivanand)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a3c6e8b3-9f29-4413-bb12-d9c4f83f56e9"}],"dashboards":[],"guid":"20769851-4cff-48c9-8d88-e666fe20638f","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
