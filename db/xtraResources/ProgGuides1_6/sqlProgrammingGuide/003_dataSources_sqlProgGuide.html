<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>003_dataSources_sqlProgGuide - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-sydney.s3.amazonaws.com","enableThirdPartyApplicationsUI":false,"enableClusterAcls":false,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":true,"enableLargeResultDownload":false,"nameAndEmail":"Raazesh Sainudiin (r.sainudiin@math.canterbury.ac.nz)","enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":true,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-30-9-162-U0c2673ac85-Sa2ee4664b2-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-30-9-162-U0c2673ac85-S33a1e4b9c6-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-30-9-162-U0c2673ac85-S5917a1044d-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.0","packageLabel":"spark-1.6-jenkins-ip-10-30-9-162-U0c2673ac85-Scabba801f3-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"prefetchSidebarNodes":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.12.3","accountsLimit":-1,"enableNotebookGitBranching":true,"local":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":false,"enableUserInviteWorkflow":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableCssTransitions":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":false,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"displayRowLimit":1000,"defaultSparkVersion":{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-30-9-162-U0c2673ac85-S5917a1044d-2016-02-09-02:05:59.455061","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":5,"enableLatestJobRunResultPermalink":true,"disallowAddingAdmins":false,"enableSparkConfUI":true,"enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"csrfToken":"1f2013f6-c2fd-4ab5-b68c-a2ff4e325639","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"someName":"Raazesh Sainudiin","enableWorkspaceAcls":true,"gitHash":"0c2673ac858e227cad536fdb45d140aeded238db","userFullname":"Raazesh Sainudiin","enableClusterCreatePage":false,"enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100005,"dbcSupportURL":"","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/","enableSparkPackages":true,"enableHybridClusterType":false,"enableNotebookHistoryUI":true,"availableWorkspaces":[{"name":"Workspace 0","orgId":0}],"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","enableHybridClusters":true,"notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"r.sainudiin@math.canterbury.ac.nz","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"tablesPublisherRootId":7,"enableNewInputWidgetUI":false,"accounts":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":46792,"name":"003_dataSources_sqlProgGuide","language":"scala","commands":[{"version":"CommandV1","origId":47685,"guid":"d4e178a4-9a87-4a4b-b8d3-c39b0dba0610","subtype":"command","commandType":"auto","position":0.5,"command":"%md\n\n# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### prepared by [Raazesh Sainudiin](https://nz.linkedin.com/in/raazesh-sainudiin-45955845) and [Sivanand Sivaram](https://www.linkedin.com/in/sivanand)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"1eb707f8-f8f8-4202-90ff-ceb6d8eab623"},{"version":"CommandV1","origId":60685,"guid":"b15e7107-75e1-474e-b315-91a2c3183140","subtype":"command","commandType":"auto","position":0.75,"command":"%md\nThis is an elaboration of the [Apache Spark 1.6 sql-progamming-guide](http://spark.apache.org/docs/latest/sql-programming-guide.html).\n\n# [Data Sources](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide)\n\n## [Spark Sql Programming Guide](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/000_sqlProgGuide)\n\n-   [Overview](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide)\n    -   SQL\n    -   DataFrames\n    -   Datasets\n-   [Getting Started](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide)\n    -   Starting Point: SQLContext\n    -   Creating DataFrames\n    -   DataFrame Operations\n    -   Running SQL Queries Programmatically\n    -   Creating Datasets\n    -   Interoperating with RDDs\n        -   Inferring the Schema Using Reflection\n        -   Programmatically Specifying the Schema\n-   [Data Sources](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide)\n    -   Generic Load/Save Functions\n        -   Manually Specifying Options\n        -   Run SQL on files directly\n        -   Save Modes\n        -   Saving to Persistent Tables\n    -   Parquet Files\n        -   Loading Data Programmatically\n        -   Partition Discovery\n        -   Schema Merging\n        -   Hive metastore Parquet table conversion\n            -   Hive/Parquet Schema Reconciliation\n            -   Metadata Refreshing\n        -   Configuration\n    -   JSON Datasets\n    -   Hive Tables\n        -   Interacting with Different Versions of Hive Metastore\n    -   JDBC To Other Databases\n    -   Troubleshooting\n-   [Performance Tuning](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide)\n    -   Caching Data In Memory\n    -   Other Configuration Options\n-   [Distributed SQL Engine](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide)\n    -   Running the Thrift JDBC/ODBC server\n    -   Running the Spark SQL CLI\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"fbd53cac-16a2-4efb-aa7e-fef279ee29fb"},{"version":"CommandV1","origId":46794,"guid":"44675a85-c161-4dbe-b322-68ca286dd1c3","subtype":"command","commandType":"auto","position":1.0,"command":"%md \n# [Data Sources](/#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide)\n\nSpark SQL supports operating on a variety of data sources through the `DataFrame` interface. A DataFrame can be operated on as normal RDDs and can also be registered as a temporary table. Registering a DataFrame as a table allows you to run SQL queries over its data. But from time to time you would need to either load or save DataFrame. Spark SQL provides built-in data sources as well as Data Source API to define your own data source and use it read / write data into Spark.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"831dc867-2a0f-4b16-9408-830b340be8a6"},{"version":"CommandV1","origId":49208,"guid":"e293730f-4811-47a0-94c0-60445b38d1af","subtype":"command","commandType":"auto","position":1.5,"command":"%md \n## Overview\nSpark provides some built-in datasources that you can use straight out of the box, such as [Parquet](https://parquet.apache.org/), [JSON](http://www.json.org/), [JDBC](https://en.wikipedia.org/wiki/Java_Database_Connectivity), [ORC](https://orc.apache.org/) (available with HiveContext), and Text (since Spark 1.6) and CSV (since Spark 2.0, before that it is accessible as a package).\n\n## Third-party datasource packages\nCommunity also have built quite a few datasource packages to provide easy access to the data from other formats. You can find list of those packages on http://spark-packages.org/, e.g. [Avro](http://spark-packages.org/package/databricks/spark-avro), [CSV](http://spark-packages.org/package/databricks/spark-csv), [Amazon Redshit](http://spark-packages.org/package/databricks/spark-redshift) (for Spark < 2.0), [XML](http://spark-packages.org/package/HyukjinKwon/spark-xml), [NetFlow](http://spark-packages.org/package/sadikovi/spark-netflow) and many others. \n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"51402816-eb9b-4bc0-9441-e571c833a7fe"},{"version":"CommandV1","origId":49209,"guid":"6cf9b554-7b3c-42b0-b98c-324dd8b0a4ec","subtype":"command","commandType":"auto","position":1.75,"command":"%md\n## Generic Load/Save functions\nIn order to load or save DataFrame you have to call either ``read`` or ``write``. This will return [DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader) or [DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter) depending on what you are trying to achieve. Essentially these classes are entry points to the reading / writing actions. They allow you to specify writing mode or provide additional options to read data source. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"46c4545d-4096-4745-8dec-4eda81ec1476"},{"version":"CommandV1","origId":49213,"guid":"f7061227-b6bb-44a2-b886-be4dbb6162e9","subtype":"command","commandType":"auto","position":1.90625,"command":"// This will return DataFrameReader to read data source\nsqlContext.read\n\nval df = sqlContext.range(0, 10)\n\n// This will return DataFrameWriter to save DataFrame\ndf.write","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [id: bigint]\nres1: org.apache.spark.sql.DataFrameWriter = org.apache.spark.sql.DataFrameWriter@4a153296\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929082443E12,"submitTime":1.457928991907E12,"finishTime":1.457929082552E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"8437bc8d-1ec7-47a7-8e1a-dcf5aa3190ff"},{"version":"CommandV1","origId":49217,"guid":"9f6c3ba2-bf5e-49a9-b006-22a63ff8e9d5","subtype":"command","commandType":"auto","position":1.9296875,"command":"// Loading Parquet table in Scala\nval df = sqlContext.read.parquet(\"/tmp/platforms.parquet\")\ndf.show(5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------+------+---------+\n|      name|status|   visits|\n+----------+------+---------+\n|Foursquare| false| 516954.0|\n|   Twitter| false|8095134.0|\n|  Facebook| false|6870824.0|\n| Instagram| false| 134617.0|\n|     Vimeo| false|     null|\n+----------+------+---------+\nonly showing top 5 rows\n\ndf: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:60: error: value read is not a member of org.apache.spark.sql.DataFrame\n              df.read.parquet(&quot;/tmp/platforms.parquet&quot;)\n                 ^\n</div>","error":null,"startTime":1.457929151153E12,"submitTime":1.457929060595E12,"finishTime":1.457929172338E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e2d892ed-bcf1-4006-9000-aadfae456455"},{"version":"CommandV1","origId":49218,"guid":"d47071a0-0957-4d8d-8717-e200e53dd1a3","subtype":"command","commandType":"auto","position":1.93359375,"command":"%py\n# Loading Parquet table in Python\ndfPy = sqlContext.read.parquet(\"/tmp/platforms.parquet\")\ndfPy.show(5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------+------+---------+\n|      name|status|   visits|\n+----------+------+---------+\n|Foursquare| false| 516954.0|\n|   Twitter| false|8095134.0|\n|  Facebook| false|6870824.0|\n| Instagram| false| 134617.0|\n|     Vimeo| false|     null|\n+----------+------+---------+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929183804E12,"submitTime":1.457929093261E12,"finishTime":1.457929203222E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a9bf91f2-7814-4cb2-ba7b-c2054b25eac2"},{"version":"CommandV1","origId":49221,"guid":"528d3fa4-06ed-4208-b85f-73143b1eab9f","subtype":"command","commandType":"auto","position":1.9345703125,"command":"// Loading JSON dataset in Scala\nval df = sqlContext.read.json(\"/tmp/platforms.json\")\ndf.show(5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------+------+---------+\n|      name|status|   visits|\n+----------+------+---------+\n|Foursquare| false| 516954.0|\n|   Twitter| false|8095134.0|\n|  Facebook| false|6870824.0|\n| Instagram| false| 134617.0|\n|     Vimeo| false|     null|\n+----------+------+---------+\nonly showing top 5 rows\n\ndf: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929222431E12,"submitTime":1.457929131873E12,"finishTime":1.457929239001E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"02757226-ef9d-4d17-a9f8-f705da7912ab"},{"version":"CommandV1","origId":49222,"guid":"fdcd23f0-4df9-4b06-9bba-905cbb81af77","subtype":"command","commandType":"auto","position":1.93505859375,"command":"%py\n# Loading JSON dataset in Python\ndfPy = sqlContext.read.json(\"/tmp/platforms.json\")\ndfPy.show(5)\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------+------+---------+\n|      name|status|   visits|\n+----------+------+---------+\n|Foursquare| false| 516954.0|\n|   Twitter| false|8095134.0|\n|  Facebook| false|6870824.0|\n| Instagram| false| 134617.0|\n|     Vimeo| false|     null|\n+----------+------+---------+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929246524E12,"submitTime":1.457929155972E12,"finishTime":1.457929262319E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"aea9944a-b02d-4862-9c79-74fa7abc75b5"},{"version":"CommandV1","origId":49211,"guid":"d0e4d285-5932-45dc-bb6c-28f96d83068e","subtype":"command","commandType":"auto","position":1.9375,"command":"%md\n### Manually Specifying Options\n\nYou can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., `org.apache.spark.sql.parquet`), but for built-in sources you can also use their short names (`json`, `parquet`, `jdbc`). DataFrames of any type can be converted into other types using this syntax.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"93a731a2-94c0-4aa8-ac9b-5d5a242e219c"},{"version":"CommandV1","origId":49223,"guid":"a53cc358-d9b3-4b81-bef5-74cb95a8b1fd","subtype":"command","commandType":"auto","position":1.953125,"command":"val json = sqlContext.read.format(\"json\").load(\"/tmp/platforms.json\")\njson.select(\"name\").show()\n\nval parquet = sqlContext.read.format(\"parquet\").load(\"/tmp/platforms.parquet\")\nparquet.select(\"name\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------+\n|      name|\n+----------+\n|Foursquare|\n|   Twitter|\n|  Facebook|\n| Instagram|\n|     Vimeo|\n|    Flickr|\n|    Tumblr|\n+----------+\n\n+----------+\n|      name|\n+----------+\n|Foursquare|\n|   Twitter|\n|  Facebook|\n| Instagram|\n|     Vimeo|\n|    Flickr|\n|    Tumblr|\n+----------+\n\njson: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\nparquet: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929270262E12,"submitTime":1.457929179705E12,"finishTime":1.457929307811E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"2c9f2385-0d53-4448-a39d-6fe4f40fb8a3"},{"version":"CommandV1","origId":49215,"guid":"af1eef1a-0c61-44f8-895f-a69add9855be","subtype":"command","commandType":"auto","position":1.96875,"command":"%md \n### Run SQL on files directly\nInstead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.457426305935E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"ivan.sadikov@gmail.com","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"52240ac3-0803-4db3-b6a2-6b3629bc474a"},{"version":"CommandV1","origId":49224,"guid":"e9c38c1c-da07-48ec-8e86-eab076927791","subtype":"command","commandType":"auto","position":1.984375,"command":"val df = sqlContext.sql(\"SELECT * FROM parquet.`/tmp/platforms.parquet`\")\ndf.printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- name: string (nullable = true)\n |-- status: boolean (nullable = true)\n |-- visits: double (nullable = true)\n\ndf: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929314378E12,"submitTime":1.457929223829E12,"finishTime":1.457929315377E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"450aca1c-b248-4025-be8b-f56960a67392"},{"version":"CommandV1","origId":46795,"guid":"5055cc7e-8f08-4e06-9a7f-43ce023879da","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n### Save Modes\nSave operations can optionally take a `SaveMode`, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a `Overwrite`, the data will be deleted before writing out the new data.\n\n| Scala/Java | Any language | Meaning |\n| --- | --- | --- |\n| `SaveMode.ErrorIfExists` (default) | `\"error\"` (default) | When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.\n| `SaveMode.Append` | `\"append\"` | When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.\n| `SaveMode.Overwrite` | `\"overwrite\"` | Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.\n| `SaveMode.Ignore` | `\"ignore\"` | Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a `CREATE TABLE IF NOT EXISTS` in SQL.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e30d4c3a-720d-4443-b7f0-3bf45da75aad"},{"version":"CommandV1","origId":49225,"guid":"ec81a5b3-f1c7-4b21-8ddf-0dc84640715c","subtype":"command","commandType":"auto","position":2.5,"command":"%md\n### Saving to Persistent Tables\nWhen working with a `HiveContext`, `DataFrames` can also be saved as persistent tables using the `saveAsTable` command. Unlike the `registerTempTable` command, `saveAsTable` will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the `table` method on a `SQLContext` with the name of the table.\n\nBy default `saveAsTable` will create a ?managed table?, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"18177eb7-7cac-4c7d-91a8-37e8493d077c"},{"version":"CommandV1","origId":49229,"guid":"5dcd2b02-aa8b-47d3-aebc-808137cf7c43","subtype":"command","commandType":"auto","position":2.625,"command":"// First of all list tables to see that table we are about to create does not exist\nsqlContext.tables.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+------------------+-----------+\n|         tableName|isTemporary|\n+------------------+-----------+\n|            hubots|       true|\n| power_plant_table|       true|\n|           percent|       true|\n|            anonym|       true|\n|      simple_range|      false|\n|social_media_usage|      false|\n+------------------+-----------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:65: error: not found: value sqlContet\n              sqlContet.tables()\n              ^\n</div>","error":null,"startTime":1.457929467716E12,"submitTime":1.457929377148E12,"finishTime":1.457929467888E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"3908ed4e-6c03-4e95-a4f7-2c0635b45007"},{"version":"CommandV1","origId":49230,"guid":"9ec0e2b8-a2f6-4eb1-878f-358649606987","subtype":"command","commandType":"auto","position":2.6875,"command":"%sql\ndrop table simple_range","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"startTime":1.457929474715E12,"submitTime":1.457929384121E12,"finishTime":1.457929475204E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"711c8d22-55b4-40a6-891d-fc82c70ca65c"},{"version":"CommandV1","origId":49226,"guid":"255b52b1-92d1-4954-8142-cda61d376d39","subtype":"command","commandType":"auto","position":2.75,"command":"val df = sqlContext.range(0, 100)\ndf.write.saveAsTable(\"simple_range\")\n\n// Verify that table is saved and it is marked as persistent (\"isTemporary\" value should be \"false\")\nsqlContext.tables.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+------------------+-----------+\n|         tableName|isTemporary|\n+------------------+-----------+\n|            hubots|       true|\n| power_plant_table|       true|\n|           percent|       true|\n|            anonym|       true|\n|      simple_range|      false|\n|social_media_usage|      false|\n+------------------+-----------+\n\ndf: org.apache.spark.sql.DataFrame = [id: bigint]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.sql.AnalysisException: Table `simple_range` already exists.;","error":"<div class=\"ansiout\">\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:232)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:221)</div>","startTime":1.457929483541E12,"submitTime":1.457929392978E12,"finishTime":1.457929486229E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"753d7633-2c25-4e30-ab68-66de3867035a"},{"version":"CommandV1","origId":49227,"guid":"61d373b7-e496-48fc-8b8a-ea6ff986ba0c","subtype":"command","commandType":"auto","position":2.875,"command":"%md\n## Parquet Files\n[Parquet](http://parquet.io) is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"436507e3-297b-4271-bf8c-28bc2a61c07b"},{"version":"CommandV1","origId":63434,"guid":"b9676f4b-6b41-425f-85ce-2b5ec3f6b263","subtype":"command","commandType":"auto","position":2.90625,"command":"%md\n### More on Parquet\n[Apache Parquet](https://parquet.apache.org/) is a [columnar storage](http://en.wikipedia.org/wiki/Column-oriented_DBMS) format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. It is a more efficient way to store data frames.\n\n* To understand the ideas read [Dremel: Interactive Analysis of Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton and Theo Vassilakis,Proc. of the 36th Int'l Conf on Very Large Data Bases (2010), pp. 330-339](http://research.google.com/pubs/pub36632.html), whose Abstract is as follows:\n    * Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layouts it is **capable of running aggregation queries over trillion-row tables in seconds**. The system **scales to thousands of CPUs and petabytes of data, and has thousands of users at Google**. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"cf404c6d-29ad-4ca4-82c7-4e028e035bdc"},{"version":"CommandV1","origId":63435,"guid":"e23b5fdc-e622-4b46-b6f8-b91efb22408f","subtype":"command","commandType":"auto","position":2.921875,"command":"//This allows easy embedding of publicly available information into any other notebook\n//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(\"URL\").\n//Example usage:\n// displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA\",250))\ndef frameIt( u:String, h:Int ) : String = {\n      \"\"\"<iframe \n src=\"\"\"\"+ u+\"\"\"\"\n width=\"95%\" height=\"\"\"\" + h + \"\"\"\"\n sandbox>\n  <p>\n    <a href=\"http://spark.apache.org/docs/latest/index.html\">\n      Fallback link for browsers that, unlikely, don't support frames\n    </a>\n  </p>\n</iframe>\"\"\"\n   }\ndisplayHTML(frameIt(\"https://parquet.apache.org/documentation/latest/\",500))","commandVersion":0,"state":"finished","results":{"type":"htmlSandbox","data":"<iframe \n src=\"https://parquet.apache.org/documentation/latest/\"\n width=\"95%\" height=\"500\"\n sandbox>\n  <p>\n    <a href=\"http://spark.apache.org/docs/latest/ml-features.html\">\n      Fallback link for browsers that, unlikely, don't support frames\n    </a>\n  </p>\n</iframe>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929706075E12,"submitTime":1.457929615513E12,"finishTime":1.457929706142E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":true,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"00d62236-b042-4612-b215-fb190cb5c9fe"},{"version":"CommandV1","origId":49232,"guid":"bedab160-673b-4729-95db-42f6ca38ac9c","subtype":"command","commandType":"auto","position":2.9375,"command":"%md\n### Loading Data Programmatically","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a45e0f00-885e-4146-b68e-f4a63b53aa3e"},{"version":"CommandV1","origId":46796,"guid":"123ef787-1598-428d-9693-0d72bd1bd22f","subtype":"command","commandType":"auto","position":3.0,"command":"// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.\n// The result of loading a Parquet file is also a DataFrame.\nval parquetFile = sqlContext.read.parquet(\"/tmp/platforms.parquet\")\n\n// Parquet files can also be registered as tables and then used in SQL statements.\nparquetFile.registerTempTable(\"parquetFile\")\nval platforms = sqlContext.sql(\"SELECT name FROM parquetFile WHERE visits > 0\")\nplatforms.map(t => \"Name: \" + t(0)).collect().foreach(println)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Name: Foursquare\nName: Twitter\nName: Facebook\nName: Instagram\nName: Flickr\nName: Tumblr\nparquetFile: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\nplatforms: org.apache.spark.sql.DataFrame = [name: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457929744633E12,"submitTime":1.457929654042E12,"finishTime":1.457929767152E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"47c0e1f5-b40f-44aa-849c-b01cb2aed268"},{"version":"CommandV1","origId":46797,"guid":"38f19ae9-4a62-4ad3-bd0f-82b4e413ee58","subtype":"command","commandType":"auto","position":4.0,"command":"%md\n### Partition Discovery\nTable partitioning is a common optimization approach used in systems like Hive. In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory. The Parquet data source is now able to discover and infer partitioning information automatically. For example, we can store all our previously used population data (from the programming guide example!) into a partitioned table using the following directory structure, with two extra columns, `gender` and `country` as partitioning columns:\n```\n    path\n    ??? to\n        ??? table\n            ??? gender=male\n            ?   ??? ...\n            ?   ?\n            ?   ??? country=US\n            ?   ?   ??? data.parquet\n            ?   ??? country=CN\n            ?   ?   ??? data.parquet\n            ?   ??? ...\n            ??? gender=female\n                ??? ...\n                ?\n                ??? country=US\n                ?   ??? data.parquet\n                ??? country=CN\n                ?   ??? data.parquet\n                ??? ...\n```\nBy passing `path/to/table` to either `SQLContext.read.parquet` or `SQLContext.read.load`, Spark SQL will automatically extract the partitioning information from the paths. Now the schema of the returned DataFrame becomes:\n```\n    root\n    |-- name: string (nullable = true)\n    |-- age: long (nullable = true)\n    |-- gender: string (nullable = true)\n    |-- country: string (nullable = true)\n```\nNotice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by `spark.sql.sources.partitionColumnTypeInference.enabled`, which is default to `true`. When type inference is disabled, string type will be used for the partitioning columns.\n\nStarting from Spark 1.6.0, partition discovery only finds partitions under the given paths by default. For the above example, if users pass `path/to/table/gender=male` to either `SQLContext.read.parquet` or `SQLContext.read.load`, `gender` will not be considered as a partitioning column. If users need to specify the base path that partition discovery should start with, they can set `basePath` in the data source options. For example, when `path/to/table/gender=male` is the path of the data and users set `basePath` to `path/to/table/`, `gender` will be a partitioning column.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e4799b23-3299-4b0c-af4b-72331806afbf"},{"version":"CommandV1","origId":46798,"guid":"e4314595-2a82-4975-9989-fd08c166e3ce","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n### Schema Merging\nLike ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.\n\nSince schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by:\n1.  setting data source option `mergeSchema` to `true` when reading Parquet files (as shown in the examples below), or\n2.  setting the global SQL option `spark.sql.parquet.mergeSchema` to `true`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"56ab6ccc-4b83-4998-a832-1e280d6b057e"},{"version":"CommandV1","origId":49234,"guid":"959ca06b-dbe6-4230-b8b1-bc349a75eefd","subtype":"command","commandType":"auto","position":5.5,"command":"// Create a simple DataFrame, stored into a partition directory\nval df1 = sc.parallelize(1 to 5).map(i => (i, i * 2)).toDF(\"single\", \"double\")\ndf1.write.mode(\"overwrite\").parquet(\"/tmp/data/test_table/key=1\")\n\n// Create another DataFrame in a new partition directory, adding a new column and dropping an existing column\nval df2 = sc.parallelize(6 to 10).map(i => (i, i * 3)).toDF(\"single\", \"triple\")\ndf2.write.mode(\"overwrite\").parquet(\"/tmp/data/test_table/key=2\")\n\n// Read the partitioned table\nval df3 = sqlContext.read.option(\"mergeSchema\", \"true\").parquet(\"/tmp/data/test_table\")\ndf3.printSchema()\n\n// The final schema consists of all 3 columns in the Parquet files together\n// with the partitioning column appeared in the partition directory paths.\n// root\n//  |-- single: integer (nullable = true)\n//  |-- double: integer (nullable = true)\n//  |-- triple: integer (nullable = true)\n//  |-- key: integer (nullable = true))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- single: integer (nullable = true)\n |-- double: integer (nullable = true)\n |-- triple: integer (nullable = true)\n |-- key: integer (nullable = true)\n\ndf1: org.apache.spark.sql.DataFrame = [single: int, double: int]\ndf2: org.apache.spark.sql.DataFrame = [single: int, triple: int]\ndf3: org.apache.spark.sql.DataFrame = [single: int, double: int, triple: int, key: int]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under dbfs:/data/test_table.","error":"<div class=\"ansiout\">\tat scala.Predef$.assert(Predef.scala:179)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:512)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$12.apply(ParquetRelation.scala:421)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$12.apply(ParquetRelation.scala:421)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:421)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:202)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:202)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:636)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:635)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.&lt;init&gt;(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:441)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:316)</div>","startTime":1.457930314773E12,"submitTime":1.457930224111E12,"finishTime":1.457930324364E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4893e4e9-c717-48f6-9768-0e477e3ed289"},{"version":"CommandV1","origId":46799,"guid":"c5f7b494-45dd-4b52-8ef5-8946cd507078","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n### Hive metastore Parquet table conversion\nWhen reading from and writing to Hive metastore Parquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for better performance. This behavior is controlled by the `spark.sql.hive.convertMetastoreParquet` configuration, and is turned on by default.\n\n#### Hive/Parquet Schema Reconciliation\nThere are two key differences between Hive and Parquet from the perspective of table schema processing.\n1.  Hive is case insensitive, while Parquet is not\n2.  Hive considers all columns nullable, while nullability in Parquet is significant\n\nDue to this reason, we must reconcile Hive metastore schema with Parquet schema when converting a Hive metastore Parquet table to a Spark SQL Parquet table. The reconciliation rules are:\n1.  Fields that have the same name in both schema must have the same data type regardless of nullability. The reconciled field should have the data type of the Parquet side, so that nullability is respected.\n2.  The reconciled schema contains exactly those fields defined in Hive metastore schema.\n  -   Any fields that only appear in the Parquet schema are dropped in the reconciled schema.\n  -   Any fileds that only appear in the Hive metastore schema are added as nullable field in the reconciled schema.\n\n#### Metadata Refreshing\nSpark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"d93f6ea1-c90a-40a0-a4a6-7c8a9ecc46d9"},{"version":"CommandV1","origId":49235,"guid":"1be47908-73f5-44a4-b2f1-b754b32612df","subtype":"command","commandType":"auto","position":6.5,"command":"// sqlContext should be a HiveContext to refresh table\nsqlContext.refreshTable(\"simple_range\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457930437593E12,"submitTime":1.457930346982E12,"finishTime":1.457930437658E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"6cce5d33-c38d-4f00-95c2-327d1c7deb0f"},{"version":"CommandV1","origId":49236,"guid":"70957cb0-da4b-472d-ae82-9f75628d29dd","subtype":"command","commandType":"auto","position":6.75,"command":"%sql\n-- Or you can use SQL to refresh table\nREFRESH TABLE simple_range;","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"startTime":1.457930440731E12,"submitTime":1.457930350105E12,"finishTime":1.457930440939E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"2deeee27-ba7f-4355-a32f-d873ab4eb85a"},{"version":"CommandV1","origId":46800,"guid":"5639a0a0-0b8e-4879-bb70-668b95ca802d","subtype":"command","commandType":"auto","position":7.0,"command":"%md\n### Configuration\n\nConfiguration of Parquet can be done using the `setConf` method on\n`SQLContext` or by running `SET key=value` commands using SQL.\n\n| Property Name | Default | Meaning |\n| --- | --- | --- | --- | \n| `spark.sql.parquet.binaryAsString` | false | Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.\n| `spark.sql.parquet.int96AsTimestamp` | true | Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems. |\n| `spark.sql.parquet.cacheMetadata` | true | Turns on caching of Parquet schema metadata. Can speed up querying of static data. |\n| `spark.sql.parquet.compression.codec` | gzip | Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo. |\n| `spark.sql.parquet.filterPushdown` | true | Enables Parquet filter push-down optimization when set to true. |\n| `spark.sql.hive.convertMetastoreParquet` | true | When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support. |\n| `spark.sql.parquet.output.committer.class` | `org.apache.parquet.hadoop.ParquetOutputCommitter` | The output committer class used by Parquet. The specified class needs to be a subclass of `org.apache.hadoop.mapreduce.OutputCommitter`. Typically, it's also a subclass of `org.apache.parquet.hadoop.ParquetOutputCommitter`. Spark SQL comes with a builtin `org.apache.spark.sql.parquet.DirectParquetOutputCommitter`, which can be more efficient then the default Parquet output committer when writing data to S3. |\n| `spark.sql.parquet.mergeSchema` | `false` | When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available. | \n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"1bc93688-c158-41ec-9740-ea9c9bc81d45"},{"version":"CommandV1","origId":46801,"guid":"b603bef4-26cc-481f-b29b-01fa329e3efc","subtype":"command","commandType":"auto","position":8.0,"command":"%md\n## JSON Datasets\nSpark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using `SQLContext.read.json()` on either an RDD of String, or a JSON file.\n\nNote that the file that is offered as *a json file* is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"f1c494a3-a206-4cb8-be11-015a344d8135"},{"version":"CommandV1","origId":49237,"guid":"e37a3d02-f450-4f7f-8531-9932c139eb35","subtype":"command","commandType":"auto","position":8.5,"command":"// A JSON dataset is pointed to by path.\n// The path can be either a single text file or a directory storing text files.\nval path = \"/tmp/platforms.json\"\nval platforms = sqlContext.read.json(path)\n\n// The inferred schema can be visualized using the printSchema() method.\nplatforms.printSchema()\n// root\n//  |-- name: string (nullable = true)\n//  |-- status: boolean (nullable = true)\n//  |-- visits: double (nullable = true)\n\n// Register this DataFrame as a table.\nplatforms.registerTempTable(\"platforms\")\n\n// SQL statements can be run by using the sql methods provided by sqlContext.\nval facebook = sqlContext.sql(\"SELECT name FROM platforms WHERE name like 'Face%k'\")\nfacebook.show()\n\n// Alternatively, a DataFrame can be created for a JSON dataset represented by\n// an RDD[String] storing one JSON object per string.\nval rdd = sc.parallelize(\"\"\"{\"name\":\"IWyn\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\" :: Nil)\nval anotherPlatforms = sqlContext.read.json(rdd)\nanotherPlatforms.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- name: string (nullable = true)\n |-- status: boolean (nullable = true)\n |-- visits: double (nullable = true)\n\n+--------+\n|    name|\n+--------+\n|Facebook|\n+--------+\n\n+---------------+----+\n|        address|name|\n+---------------+----+\n|[Columbus,Ohio]|IWyn|\n+---------------+----+\n\npath: String = /tmp/platforms.json\nplatforms: org.apache.spark.sql.DataFrame = [name: string, status: boolean, visits: double]\nfacebook: org.apache.spark.sql.DataFrame = [name: string]\nrdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[67122] at parallelize at &lt;console&gt;:57\nanotherPlatforms: org.apache.spark.sql.DataFrame = [address: struct&lt;city:string,state:string&gt;, name: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"startTime":1.457930455369E12,"submitTime":1.457930364751E12,"finishTime":1.457930471632E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"r.sainudiin@math.canterbury.ac.nz","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"919d144a-1b5c-4693-85e4-5614298fc89a"},{"version":"CommandV1","origId":46803,"guid":"86a25827-5a32-4765-8d58-8d7bec348677","subtype":"command","commandType":"auto","position":9.0,"command":"%md\n## Hive Tables\nSpark SQL also supports reading and writing data stored in [Apache Hive](http://hive.apache.org/). However, since Hive has a large number of dependencies, it is not included in the default Spark assembly. Hive support is enabled by adding the `-Phive` and `-Phive-thriftserver` flags to Spark?s build. This command builds a new assembly jar that includes Hive. Note that this Hive assembly jar must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.\n\nConfiguration of Hive is done by placing your `hive-site.xml`, `core-site.xml` (for security configuration), `hdfs-site.xml` (for HDFS configuration) file in `conf/`. Please note when running the query on a YARN cluster (`cluster` mode), the `datanucleus` jars under the `lib_managed/jars` directory and `hive-site.xml` under `conf/` directory need to be available on the driver and all executors launched by the YARN cluster. The convenient way to do this is adding them through the `--jars` option and `--file` option of the `spark-submit` command.\n\nWhen working with Hive one must construct a `HiveContext`, which inherits from `SQLContext`, and adds support for finding tables in the MetaStore and writing queries using HiveQL. Users who do not have an existing Hive deployment can still create a `HiveContext`. When not configured by the hive-site.xml, the context automatically creates `metastore_db` in the current directory and creates `warehouse` directory indicated by HiveConf, which defaults to `/user/hive/warehouse`. Note that you may need to grant write privilege on `/user/hive/warehouse` to the user who starts the spark application.\n\n```scala\n// sc is an existing SparkContext.\nval sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\n\nsqlContext.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\nsqlContext.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")\n\n// Queries are expressed in HiveQL\nsqlContext.sql(\"FROM src SELECT key, value\").collect().foreach(println)\n```\n\n### Interacting with Different Versions of Hive Metastore\nOne of the most important pieces of Spark SQL?s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against Hive 1.2.1 and use those classes for internal execution (serdes, UDFs, UDAFs, etc).\n\nThe following options can be used to configure the version of Hive that is used to retrieve metadata:\n\n| Property Name | Default | Meaning |\n| --- | --- | --- |\n| `spark.sql.hive.metastore.version` | `1.2.1` | Version of the Hive metastore. Available options are `0.12.0` through `1.2.1`. |\n| `spark.sql.hive.metastore.jars` | `builtin` | Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options: `builtin`, `maven`, a classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with you application. |\n| `spark.sql.hive.metastore.sharedPrefixes` | `com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc` | A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j. |\n| `spark.sql.hive.metastore.barrierPrefixes` | `(empty)` | A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. `org.apache.spark.*`). |\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.457429239608E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"ivan.sadikov@gmail.com","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"67cae12d-d550-43bf-92dd-24e22300193b"},{"version":"CommandV1","origId":46804,"guid":"0c63d746-828a-478e-9c23-ce901ec29eaf","subtype":"command","commandType":"auto","position":10.0,"command":"%md\n## JDBC To Other Databases\nSpark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using [JdbcRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD). This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL).\n\nTo get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:\n```scala\nSPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell\n```\n\nTables from the remote database can be loaded as a DataFrame or Spark SQL Temporary table using the Data Sources API. The following options are supported:\n\n| Property Name | Meaning |\n| --- | --- | --- |\n| `url` | The JDBC URL to connect to. |\n| `dbtable` | The JDBC table that should be read. Note that anything that is valid in a `FROM` clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. |\n| `driver` | The class name of the JDBC driver needed to connect to this URL. This class will be loaded on the master and workers before running an JDBC commands to allow the driver to register itself with the JDBC subsystem.\n| `partitionColumn, lowerBound, upperBound, numPartitions` | These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. `partitionColumn` must be a numeric column from the table in question. Notice that `lowerBound` and `upperBound` are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.\n| `fetchSize` | The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).\n\n```scala\n// Example of using JDBC datasource\nval jdbcDF = sqlContext.read.format(\"jdbc\").options(Map(\"url\" -> \"jdbc:postgresql:dbserver\", \"dbtable\" -> \"schema.tablename\")).load()\n```\n\n```sql\n-- Or using JDBC datasource in SQL\nCREATE TEMPORARY TABLE jdbcTable\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url \"jdbc:postgresql:dbserver\",\n  dbtable \"schema.tablename\"\n)\n```","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"76253ac1-2e75-416d-93b8-6babac95c6c6"},{"version":"CommandV1","origId":49238,"guid":"95ce33c0-ec83-454c-911e-7feb44d706f4","subtype":"command","commandType":"auto","position":11.0,"command":"%md\n### Troubleshooting\n- The JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java?s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute\\_classpath.sh on all worker nodes to include your driver JARs.\n- Some databases, such as H2, convert all names to upper case. You?ll need to use upper case to refer to those names in Spark SQL.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"0bd17735-a697-4c24-b6f5-a9696ebdcc0b"},{"version":"CommandV1","origId":60662,"guid":"f162e858-0966-460a-8096-043791875e12","subtype":"command","commandType":"auto","position":12.0,"command":"%md\n\n# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### prepared by [Raazesh Sainudiin](https://nz.linkedin.com/in/raazesh-sainudiin-45955845) and [Sivanand Sivaram](https://www.linkedin.com/in/sivanand)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"26fad20a-bb74-4053-aacc-085be8004412"}],"dashboards":[],"guid":"63dc43e2-ecf0-4ee1-b1d3-3c4f80e41366","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201602081754420800-0c2673ac858e227cad536fdb45d140aeded238db/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
