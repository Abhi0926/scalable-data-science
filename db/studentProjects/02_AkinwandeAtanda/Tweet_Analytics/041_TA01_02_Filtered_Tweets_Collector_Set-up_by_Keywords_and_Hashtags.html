<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>041_TA01_02_Filtered_Tweets_Collector_Set-up_by_Keywords_and_Hashtags - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/img/favicon.ico"/>
<script>window.settings = {"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html","displayName":"Databricks Guide","icon":"question"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html","displayName":"Application Examples","icon":"code"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/courses/index.html","displayName":"Training","icon":"graduation-cap"}],"dbcForumURL":"http://forums.databricks.com/","nodeInfo":{"node_types":[{"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":3.0,"node_type_id":"class-node","description":"Class Node","container_memory_mb":6000,"memory_mb":6144,"num_cores":0.88}],"default_node_type_id":"class-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2a","isDefault":true},{"id":"us-west-2c","isDefault":false},{"id":"us-west-2b","isDefault":false}],"enablePublishNotebooks":false,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":false,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-U29e64bf9af-S2368283920-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-1.4-jenkins-ip-10-30-10-144-U29e64bf9af-S9c254ab12a-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-U29e64bf9af-S2368283920-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-U29e64bf9af-S2368283920-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-1.6.1-hadoop2-jenkins-ip-10-30-10-144-U29e64bf9af-S2368283920-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-1.5-jenkins-ip-10-30-10-144-U29e64bf9af-S9ca52d000d-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-1.3-jenkins-ip-10-30-10-144-U29e64bf9af-Sa2ee4664b2-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (apache/branch-2.0 preview)","packageLabel":"spark-image-1e373018dd5d67adc815234c23e8d1e9eca5394f90f8f2a00df56e2c182e0ad7","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-1.6.0-jenkins-ip-10-30-10-144-U29e64bf9af-Sf90f83597b-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-1.6.1-hadoop2-jenkins-ip-10-30-10-144-U29e64bf9af-S2368283920-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"enableClusterAutoScaling":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.22.1","accountsLimit":-1,"enableNotebookGitBranching":true,"local":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":false,"enableStaticNotebooks":true,"enableCssTransitions":true,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"defaultSparkVersion":{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-U29e64bf9af-S2368283920-2016-06-23-00:19:57.707639","upgradable":true,"deprecated":false,"customerVisible":true},"enableMountAclsConfig":false,"enableClusterAclsByTier":true,"disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"useFixedStaticNotebookVersionForDevelopment":false,"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"29e64bf9afe1117763a990704253c3678448e6c5","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"showDevTierBetaVersion":false,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/A%20Gentle%20Introduction%20to%20Apache%20Spark%20on%20Databricks.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/Quick%20Start%20DataFrames.html","displayName":"Quick Start DataFrames","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/GSW%20Passing%20Analysis%20(new).html","displayName":"GSW Passing Analysis (new)","icon":"img/home/Python_icon.svg"}],"upgradeURL":"","notebookLoadingBackground":"#fff","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":false,"useFramedStaticNotebooks":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":111546,"name":"041_TA01_02_Filtered_Tweets_Collector_Set-up_by_Keywords_and_Hashtags","language":"scala","commands":[{"version":"CommandV1","origId":111548,"guid":"3cddd862-4f08-4358-b48e-9e03d35897cf","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n\n# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### Course Project by [Akinwande Atanda](https://nz.linkedin.com/in/akinwande-atanda)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"aa953397-a25e-452b-8301-164e8b57517d"},{"version":"CommandV1","origId":111549,"guid":"e7e5cd75-824e-4607-a964-8bfde41ff8c3","subtype":"command","commandType":"auto","position":1.125,"command":"%md\nThe [html source url](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/studentProjects/02_AkinwandeAtanda/Tweet_Analytics/041_TA01_02_Filtered_Tweets_Collector_Set-up_by_Keywords_and_Hashtags.html) of this databricks notebook and its recorded Uji ![Image of Uji, Dogen's Time-Being](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/UjiTimeBeingDogen.png \"uji\"):\n\n[![sds/uji/studentProjects/02_AkinwandeAtanda/Tweet_Analytics/041_TA01_02_Filtered_Tweets_Collector_Set-up_by_Keywords_and_Hashtags](http://img.youtube.com/vi/zJirlHAV6YU/0.jpg)](https://www.youtube.com/v/zJirlHAV6YU?rel=0&autoplay=1&modestbranding=1&start=0&end=1611)\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"97c0ed0d-7213-4273-a47f-12be6fbad64e"},{"version":"CommandV1","origId":111550,"guid":"c8fc4c7d-9617-47e5-8206-b25a315fd40b","subtype":"command","commandType":"auto","position":1.25,"command":"%md\n#Tweet Analytics\n\n[Presentation contents](https://github.com/aaa121/Spark-Tweet-Streaming-Presentation-May-2016).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"5f01828b-59de-4199-a00f-0f6b8b0badc4"},{"version":"CommandV1","origId":111551,"guid":"86250375-6240-4f5c-b6eb-0bfe4e9abca0","subtype":"command","commandType":"auto","position":1.5,"command":"%md\n\n## Filtered Generic Twitter Collector by Keywords and Hashtags\n\n\nRemeber that the use of twitter itself comes with various strings attached. \n- **Read:** [Twitter Rules](https://twitter.com/rules)\n\n\nCrucially, the use of the content from twitter by you (as done in this worksheet) comes with some strings.\n- **Read:** [Developer Agreement & Policy Twitter Developer Agreement](https://dev.twitter.com/overview/terms/agreement-and-policy)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"74f5abf9-3c34-4ed9-8504-61ce7a94b947"},{"version":"CommandV1","origId":111552,"guid":"4090d1d8-a78a-4ed1-9768-6f262e490309","subtype":"command","commandType":"auto","position":1.75,"command":"import org.apache.spark._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter.TwitterUtils\n\nimport twitter4j.auth.OAuthAuthorization\nimport twitter4j.conf.ConfigurationBuilder\n\nimport com.google.gson.Gson","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter.TwitterUtils\nimport twitter4j.auth.OAuthAuthorization\nimport twitter4j.conf.ConfigurationBuilder\nimport com.google.gson.Gson\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:33: error: object twitter is not a member of package org.apache.spark.streaming\n       import org.apache.spark.streaming.twitter.TwitterUtils\n                                         ^\n&lt;console&gt;:35: error: not found: value twitter4j\n       import twitter4j.auth.OAuthAuthorization\n              ^\n&lt;console&gt;:36: error: not found: value twitter4j\n       import twitter4j.conf.ConfigurationBuilder\n              ^\n</div>","error":null,"workflows":[],"startTime":1.467866202777E12,"submitTime":1.467866232139E12,"finishTime":1.467866204142E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"f4106e91-5022-4870-a25f-b9a2aacb3b71"},{"version":"CommandV1","origId":111553,"guid":"46b37b8d-7691-4148-808a-8d1a8e03d6c6","subtype":"command","commandType":"auto","position":1.875,"command":"%md\n\n### Step 1: Enter your Twitter API Credentials.\n* Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them.\n* Run this cell for the input cells to appear.\n* Enter your credentials.\n* Run the cell again to pick up your defaults.\n\nThe cell-below is hidden to not expose the Twitter API Credentials: `consumerKey`, `consumerSecret`, `accessToken` and `accessTokenSecret`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"8bf47714-2fc1-489a-bd80-020531430905"},{"version":"CommandV1","origId":111554,"guid":"7426a898-1251-4413-874b-63d177765ba8","subtype":"command","commandType":"auto","position":1.9375,"command":"System.setProperty(\"twitter4j.oauth.consumerKey\", getArgument(\"1. Consumer Key (API Key)\", \"\"))\nSystem.setProperty(\"twitter4j.oauth.consumerSecret\", getArgument(\"2. Consumer Secret (API Secret)\", \"\"))\nSystem.setProperty(\"twitter4j.oauth.accessToken\", getArgument(\"3. Access Token\", \"\"))\nSystem.setProperty(\"twitter4j.oauth.accessTokenSecret\", getArgument(\"4. Access Token Secret\", \"\"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">&lt;console&gt;:43: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n              System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, getArgument(&quot;1. Consumer Key (API Key)&quot;, &quot;&quot;))\n                                                                ^\n&lt;console&gt;:44: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n              System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, getArgument(&quot;2. Consumer Secret (API Secret)&quot;, &quot;&quot;))\n                                                                   ^\n&lt;console&gt;:45: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n              System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, getArgument(&quot;3. Access Token&quot;, &quot;&quot;))\n                                                                ^\n&lt;console&gt;:48: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n              System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, getArgument(&quot;4. Access Token Secret&quot;, &quot;&quot;))\n                                                                      ^\nres0: String = null\n</div>","arguments":{"3. Access Token":"","2. Consumer Secret (API Secret)":"","1. Consumer Key (API Key)":"","4. Access Token Secret":""},"addedWidgets":{"3. Access Token":{"widgetType":"text","name":"3. Access Token","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}},"2. Consumer Secret (API Secret)":{"widgetType":"text","name":"2. Consumer Secret (API Secret)","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}},"1. Consumer Key (API Key)":{"widgetType":"text","name":"1. Consumer Key (API Key)","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}},"4. Access Token Secret":{"widgetType":"text","name":"4. Access Token Secret","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.467866208462E12,"submitTime":1.467866237832E12,"finishTime":1.467866208776E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"beb85b85-1449-4abc-8c4b-e19fb913b669"},{"version":"CommandV1","origId":111555,"guid":"9a8197fc-6213-44ba-8f35-ff0c6f7220ee","subtype":"command","commandType":"auto","position":1.96875,"command":"%md\nIf you see warnings then ignore for now:\n[https://forums.databricks.com/questions/6941/change-in-getargument-for-notebook-input.html](https://forums.databricks.com/questions/6941/change-in-getargument-for-notebook-input.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e8a02822-0b7d-40a1-897f-b3d91b9aec1d"},{"version":"CommandV1","origId":111556,"guid":"9d457789-b283-4e1e-b46c-b26df8269eea","subtype":"command","commandType":"auto","position":1.97265625,"command":"%md\n\n### Step 2: Configure where to output each filtered Batches of Tweet Stream and how often to compute them.\n* Run this cell for the input cells to appear.\n* Enter your credentials.\n* Run the cell again to pick up your defaults.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"5735705c-1ec9-4961-aa37-6f859eb778c1"},{"version":"CommandV1","origId":111557,"guid":"f9652d20-0376-40dc-90e0-1cf946fb09ed","subtype":"command","commandType":"auto","position":1.9765625,"command":"val outputDirectory = getArgument(\"1. Output Directory\", \"twitterNew3\")\nval batchInterval = getArgument(\"2. Interval for each DStream in Minutes \", \"1\").toInt\nval timeoutJobLength = getArgument(\"3. Max Time to fetch all batches of Dstream\", \"100\").toInt * 1000","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">&lt;console&gt;:43: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val outputDirectory = getArgument(&quot;1. Output Directory&quot;, &quot;twitterNew3&quot;)\n                             ^\n&lt;console&gt;:44: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val batchInterval = getArgument(&quot;2. Interval for each DStream in Minutes &quot;, &quot;1&quot;).toInt\n                           ^\n&lt;console&gt;:45: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val timeoutJobLength = getArgument(&quot;3. Max Time to fetch all batches of Dstream&quot;, &quot;100&quot;).toInt * 1000\n                              ^\noutputDirectory: String = /twitterNew3\nbatchInterval: Int = 10\ntimeoutJobLength: Int = 100000\n</div>","arguments":{"3. Max Time to fetch all batches of Dstream":"100","1. Output Directory":"/twitterNew3","2. Interval for each DStream in Minutes ":"10"},"addedWidgets":{"3. Max Time to fetch all batches of Dstream":{"widgetType":"text","name":"3. Max Time to fetch all batches of Dstream","defaultValue":"100","label":"","options":{"widgetType":"text","validationRegex":null}},"1. Output Directory":{"widgetType":"text","name":"1. Output Directory","defaultValue":"twitterNew3","label":"","options":{"widgetType":"text","validationRegex":null}},"2. Interval for each DStream in Minutes ":{"widgetType":"text","name":"2. Interval for each DStream in Minutes ","defaultValue":"1","label":"","options":{"widgetType":"text","validationRegex":null}}},"removedWidgets":[]},"errorSummary":"java.lang.NumberFormatException: For input string: \"\"","error":"<div class=\"ansiout\">\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:592)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:31)</div>","workflows":[],"startTime":1.467866216178E12,"submitTime":1.467866245544E12,"finishTime":1.46786621644E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"35961df9-c42e-41b9-9c57-461c15901f64"},{"version":"CommandV1","origId":111558,"guid":"d092ee5e-7557-4d6f-95d3-a33f4cb2b951","subtype":"command","commandType":"auto","position":1.98046875,"command":"// Replace with your AWS S3 credentials\n// NOTE: Set the access to this notebook appropriately to protect the security of your keys.\n// Or you can delete this cell after you run the mount command below once successfully.\n\nval AccessKey = getArgument(\"1. ACCESS_KEY\", \"REPLACE_WITH_YOUR_ACCESS_KEY\")\nval SecretKey = getArgument(\"2. SECRET_KEY\", \"REPLACE_WITH_YOUR_SECRET_KEY\")\nval EncodedSecretKey = SecretKey.replace(\"/\", \"%2F\")\nval AwsBucketName = getArgument(\"3. S3_BUCKET\", \"REPLACE_WITH_YOUR_S3_BUCKET\")\nval MountName = getArgument(\"4. MNT_NAME\", \"REPLACE_WITH_YOUR_MOUNT_NAME\")\nval s3Filename = \"tweetDump\"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">&lt;console&gt;:47: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val AccessKey = getArgument(&quot;1. ACCESS_KEY&quot;, &quot;REPLACE_WITH_YOUR_ACCESS_KEY&quot;)\n                       ^\n&lt;console&gt;:48: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val SecretKey = getArgument(&quot;2. SECRET_KEY&quot;, &quot;REPLACE_WITH_YOUR_SECRET_KEY&quot;)\n                       ^\n&lt;console&gt;:50: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val AwsBucketName = getArgument(&quot;3. S3_BUCKET&quot;, &quot;REPLACE_WITH_YOUR_S3_BUCKET&quot;)\n                           ^\n&lt;console&gt;:51: warning: method getArgument in trait WidgetsUtils is deprecated: Use dbutils.widgets.text() or dbutils.widgets.dropdown() to create a widget and dbutils.widgets.get() to get its bound value.\n       val MountName = getArgument(&quot;4. MNT_NAME&quot;, &quot;REPLACE_WITH_YOUR_MOUNT_NAME&quot;)\n                       ^\nAccessKey: String = &quot;&quot;\nSecretKey: String = &quot;&quot;\nEncodedSecretKey: String = &quot;&quot;\nAwsBucketName: String = tweet-bank\nMountName: String = s3Data\ns3Filename: String = tweetDump\n</div>","arguments":{"2. SECRET_KEY":"","4. MNT_NAME":"s3Data","3. S3_BUCKET":"tweet-bank","1. ACCESS_KEY":""},"addedWidgets":{"2. SECRET_KEY":{"widgetType":"text","name":"2. SECRET_KEY","defaultValue":"REPLACE_WITH_YOUR_SECRET_KEY","label":"","options":{"widgetType":"text","validationRegex":null}},"4. MNT_NAME":{"widgetType":"text","name":"4. MNT_NAME","defaultValue":"REPLACE_WITH_YOUR_MOUNT_NAME","label":"","options":{"widgetType":"text","validationRegex":null}},"3. S3_BUCKET":{"widgetType":"text","name":"3. S3_BUCKET","defaultValue":"REPLACE_WITH_YOUR_S3_BUCKET","label":"","options":{"widgetType":"text","validationRegex":null}},"1. ACCESS_KEY":{"widgetType":"text","name":"1. ACCESS_KEY","defaultValue":"REPLACE_WITH_YOUR_ACCESS_KEY","label":"","options":{"widgetType":"text","validationRegex":null}}},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.46786622233E12,"submitTime":1.467866251688E12,"finishTime":1.467866222486E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"dfc09f1f-5be5-4fed-9e29-211a66dd3b58"},{"version":"CommandV1","origId":111559,"guid":"f9805315-771a-4e73-9263-b038aef3db30","subtype":"command","commandType":"auto","position":1.982421875,"command":"dbutils.fs.unmount(s\"/mnt/$MountName\") // finally unmount when done","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">/mnt/s3Data has been unmounted.\nres1: Boolean = true\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: Directory not mounted: /mnt/s3Data; nested exception is: ","error":"<div class=\"ansiout\">\tjava.lang.IllegalArgumentException: Directory not mounted: /mnt/s3Data\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:73)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:42)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.unmount(DBUtilsCore.scala:341)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.unmount(DbfsUtilsImpl.scala:86)\nCaused by: java.lang.IllegalArgumentException: Directory not mounted: /mnt/s3Data\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.deleteMount(MetadataManager.scala:148)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:56)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:58)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:57)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:57)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:207)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:188)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:42)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:37)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:57)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:57)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$10.apply(JettyServer.scala:263)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:263)\n\tat com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:199)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:144)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:135)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:135)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:120)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:115)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:74)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:153)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:74)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:134)\n\tat com.databricks.rpc.JettyServer$RequestManager.doGet(JettyServer.scala:89)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:816)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:517)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:306)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:242)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:245)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:75)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:213)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:147)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)\n\tat java.lang.Thread.run(Thread.java:745)</div>","workflows":[],"startTime":1.463453234994E12,"submitTime":1.46345317546E12,"finishTime":1.463453236245E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"b5e55f75-9710-41d5-8c16-c932ace3b81b"},{"version":"CommandV1","origId":111560,"guid":"9ca03a31-1d0d-427c-a284-707aebb71c36","subtype":"command","commandType":"auto","position":1.9833984375,"command":"dbutils.fs.mount(s\"s3a://$AccessKey:$EncodedSecretKey@$AwsBucketName\", s\"/mnt/$MountName\") // mount if unmounted","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3Data; nested exception is: ","error":"<div class=\"ansiout\">\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3Data\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:73)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:42)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:299)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.mount(DbfsUtilsImpl.scala:77)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3Data\n\tat scala.Predef$.require(Predef.scala:233)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:121)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:38)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:58)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:57)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:57)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:214)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:195)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:42)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:37)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:57)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:57)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$10.apply(JettyServer.scala:262)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:262)\n\tat com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:199)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:144)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:135)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:135)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:121)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:116)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:74)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:154)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:74)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:134)\n\tat com.databricks.rpc.JettyServer$RequestManager.doGet(JettyServer.scala:89)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:816)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:517)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:306)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:242)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:245)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:75)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:213)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:147)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)\n\tat java.lang.Thread.run(Thread.java:745)</div>","workflows":[],"startTime":1.466822169601E12,"submitTime":1.466821951846E12,"finishTime":1.466822169685E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"8f6618af-2bb0-4f66-8243-5d05c182ad1a"},{"version":"CommandV1","origId":111561,"guid":"d1b40245-8771-4ab2-ab98-329363edea73","subtype":"command","commandType":"auto","position":1.9835205078125,"command":"%md **A directory can be created to save the Tweet Stream**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"b0ba9430-c4ef-4352-960f-2b6ce9cebb31"},{"version":"CommandV1","origId":111562,"guid":"0d1e0b3d-ef5f-4f5d-a862-fc07343d6b7a","subtype":"command","commandType":"auto","position":1.98358154296875,"command":"dbutils.fs.mkdirs(s\"/mnt/$MountName/twitterNew3/\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res1: Boolean = true\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.464839922341E12,"submitTime":1.464839889459E12,"finishTime":1.464839922591E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e87506b9-a573-433a-b3e3-507a910bc9a6"},{"version":"CommandV1","origId":111563,"guid":"17cc2f7b-beaa-4dae-81cc-8b10c5d448bf","subtype":"command","commandType":"auto","position":1.983642578125,"command":"//dbutils.fs.rm(\"/mnt/$MountName/NAME_OF_DIRECTORY/\",recurse=true) //Remove the directory if previously created and no longer required for further use. ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res32: Boolean = true\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.461919988404E12,"submitTime":1.461919839457E12,"finishTime":1.461919989219E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a121d6dc-9547-48d9-ba47-6963af0f00dd"},{"version":"CommandV1","origId":111564,"guid":"38d04376-babf-40a6-8b3c-97bcc5526f23","subtype":"command","commandType":"auto","position":1.98388671875,"command":"display(dbutils.fs.ls(s\"/mnt/s3Data/twitterNew3\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\""},{"name":"name","type":"\"string\""},{"name":"size","type":"\"long\""}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15, ip-10-93-250-95.ap-southeast-2.compute.internal): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1223)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:222)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1220)\n\t... 11 more\n\nDriver stacktrace:","error":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15, ip-10-93-250-95.ap-southeast-2.compute.internal): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1223)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:222)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1220)\n\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1863)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212)\n\tat org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n\tat org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374)\n\tat org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:80)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:42)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$2.apply(ScalaDriverLocal.scala:196)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$2.apply(ScalaDriverLocal.scala:188)\n\tat scala.Option.map(Option.scala:145)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:188)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:169)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:169)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:121)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:116)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:31)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:154)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:31)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:168)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:483)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:483)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:480)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:212)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1223)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:68)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:222)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1220)\n\t... 11 more\n","workflows":[],"startTime":1.464839927684E12,"submitTime":1.46483989478E12,"finishTime":1.464839932929E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"715e5e8d-49a7-4ce4-8b0e-5a110aa8c7ff"},{"version":"CommandV1","origId":111565,"guid":"2bee6b1d-9e4c-4a85-8ab5-4ad8d885951c","subtype":"command","commandType":"auto","position":1.984375,"command":"%md\n\n### Step 3: Run the Twitter Streaming job.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"8738e247-993f-4d64-ac01-98d6990c8325"},{"version":"CommandV1","origId":111566,"guid":"6d4b843e-bbb8-47c4-988b-4088c9a10a28","subtype":"command","commandType":"auto","position":1.99609375,"command":"%md \nCreate the function to set-up the Streaming Context and the streaming job.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"d761c81b-3904-445e-ab0b-64509e0c5c2b"},{"version":"CommandV1","origId":111567,"guid":"c5659d7a-7330-445b-b5e6-702f6bbaaa8e","subtype":"command","commandType":"auto","position":1.99658203125,"command":"%md\n###Keyword(s) and Hashtag(s) Tracking Set-up\n* Create a list of keyword(s) or hashtag(s) to track from Twitter\n* Twitter4j.Status returns ONLY tweets that contain any of the keyword(s) or hashtag(s) either in lower or upper case\n* For example: We created a list of keywords and hashtags to track tweets about the US presumptive Republican Presidential Candidate, Donald J. Trump\n* Tips: Search for popular hashtags on a specific topic on RiteTag.com\n[![](http://www.wonderoftech.com/wp-content/uploads/2014/07/RiteTag-Logo.jpg)](https://ritetag.com/hashtag-search/Donald%20trump?green=0)","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"5b104106-d273-4dbe-ae2e-95de5c63c0c7"},{"version":"CommandV1","origId":111568,"guid":"81bd9492-7c04-4c98-b188-fabcc29389ec","subtype":"command","commandType":"auto","position":1.996826171875,"command":"%md\n*** Read the Infographic on how to use twitter to increase the number of followers. This is relevant to those interested in marketing via social media ***\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"63aa645d-cf6b-4c0a-9867-1d578702fa7a"},{"version":"CommandV1","origId":111569,"guid":"4615b2a7-1387-4e48-866f-7f3b15748447","subtype":"command","commandType":"auto","position":1.9969482421875,"command":"//This allows easy embedding of publicly available information into any other notebook\n//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(\"URL\").\n//Example usage:\n// displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA\",250))\ndef frameIt( u:String, h:Int ) : String = {\n      \"\"\"<iframe \n src=\"\"\"\"+ u+\"\"\"\"\n width=\"95%\" height=\"\"\"\" + h + \"\"\"\"\n sandbox>\n  <p>\n    <a href=\"http://spark.apache.org/docs/latest/index.html\">\n      Fallback link for browsers that, unlikely, don't support frames\n    </a>\n  </p>\n</iframe>\"\"\"\n   }\ndisplayHTML(frameIt(\"http://www.wonderoftech.com/best-twitter-tips-followers/\", 600))","commandVersion":0,"state":"finished","results":{"type":"htmlSandbox","data":"<iframe \n src=\"http://www.wonderoftech.com/best-twitter-tips-followers/\"\n width=\"95%\" height=\"600\"\n sandbox>\n  <p>\n    <a href=\"http://spark.apache.org/docs/latest/index.html\">\n      Fallback link for browsers that, unlikely, don't support frames\n    </a>\n  </p>\n</iframe>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:32: error: not found: value frameIt\n              displayHTML(frameIt(&quot;http://www.wonderoftech.com/best-twitter-tips-followers/&quot;, 600))\n                          ^\n</div>","error":null,"workflows":[],"startTime":1.466822274258E12,"submitTime":1.466822056489E12,"finishTime":1.466822274376E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":true,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"98131357-6ea8-410a-9d23-8566ea756a17"},{"version":"CommandV1","origId":111570,"guid":"a1e6790b-d94f-4c6b-a424-25b952adf781","subtype":"command","commandType":"auto","position":1.9970703125,"command":" // the Library has already been attached to this cluster (show live how to do this from scratch?)\n\nvar newContextCreated = false\nvar numTweetsCollected = 0L // track number of tweets collected\n//val conf = new SparkConf().setAppName(\"TrackedTweetCollector\").setMaster(\"local\")\n// This is the function that creates the SteamingContext and sets up the Spark Streaming job.\ndef creatingFunc(): StreamingContext = {\n  // Create a Spark Streaming Context.\n  val ssc = new StreamingContext(sc, Minutes(batchInterval))\n  // Create a Twitter Stream for the input source. \n  val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))\n  \n  val track = List(\"Trump2016\", \"#MakeAmericaGreatAgain\", \"Donald Trump\",\"#lovetrumpshate\")\n  //List(?Hillary Clinton?, ?#neverhillary?, ?#hillaryclinton?, ?#demthrones?)\n  val twitterStream = TwitterUtils.createStream(ssc, auth, track)\n  val twitterStreamJson = twitterStream.map(x => { val gson = new Gson();\n                                                 val xJson = gson.toJson(x)\n                                                 xJson\n                                               }) \n \nval partitionsEachInterval = 1 // This tells the number of partitions in each RDD of tweets in the DStream.\n\ntwitterStreamJson.foreachRDD((rdd, time) => { // for each filtered RDD in the DStream\n      val count = rdd.count()\n      if (count > 0) {\n        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired\n        //outputRDD.saveAsTextFile(s\"${outputDirectory}/tweets_\" + time.milliseconds.toString) // save as textfile\n        outputRDD.saveAsTextFile(s\"/mnt/$MountName/${outputDirectory}\" + \"/tweets_\" + time.milliseconds.toString+\".txt\") // save as textfile in s3\n        numTweetsCollected += count // update with the latest count\n      }\n  })\n  \n  newContextCreated = true\n  ssc\n}\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">newContextCreated: Boolean = false\nnumTweetsCollected: Long = 0\ncreatingFunc: ()org.apache.spark.streaming.StreamingContext\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:78: error: not found: value master\n       val conf = new SparkConf().setAppName(&quot;TrackedTweetCollector&quot;).setMaster(master)\n                                                                                ^\n</div>","error":null,"workflows":[],"startTime":1.464842888576E12,"submitTime":1.4648428555E12,"finishTime":1.464842888776E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"831500bf-35b2-4212-9144-e54b36bbce95"},{"version":"CommandV1","origId":111571,"guid":"5ec2cc29-70d0-4900-8639-6da72763c181","subtype":"command","commandType":"auto","position":1.998046875,"command":"%md \nCreate the StreamingContext using getActiveOrCreate, as required when starting a streaming job in Databricks.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"45811743-91c4-44e4-9039-674098c9e028"},{"version":"CommandV1","origId":111572,"guid":"3d3a46fd-e8a4-4481-99f1-0ce2bc763c30","subtype":"command","commandType":"auto","position":1.99853515625,"command":"val ssc = StreamingContext.getActiveOrCreate(creatingFunc)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@396537dd\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: In Databricks, developers should utilize the shared SparkContext instead of creating one using the constructor. In Scala and Python notebooks, the shared context can be accessed as sc. When running a job, you can access the shared context by calling SparkContext.getOrCreate().","error":"<div class=\"ansiout\">\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2244)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext$.setActiveContext(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:2202)\n\tat org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:883)\n\tat org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:84)\n\tat Notebook.creatingFunc(&lt;console&gt;:57)\n\tat Notebook$$anonfun$1.apply(&lt;console&gt;:57)\n\tat Notebook$$anonfun$1.apply(&lt;console&gt;:57)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.streaming.StreamingContext$.getActiveOrCreate(StreamingContext.scala:819)</div>","workflows":[],"startTime":1.464842894487E12,"submitTime":1.464842861403E12,"finishTime":1.464842894624E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"018e1db9-e3f6-46d1-884a-b286877c973b"},{"version":"CommandV1","origId":111573,"guid":"ceb95810-e426-4386-bb02-a7f9c258a249","subtype":"command","commandType":"auto","position":1.998779296875,"command":"%md\n\nStart the Spark Streaming Context and return when the Streaming job exits or return with the specified timeout.  ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"db8eb2be-67f7-4fac-a65e-882b37b08e6c"},{"version":"CommandV1","origId":111574,"guid":"ec99be48-93f6-4f80-bd51-678e7a5fbd72","subtype":"command","commandType":"auto","position":1.9989013671875,"command":"ssc.start()","commandVersion":0,"state":"error","results":null,"errorSummary":"java.lang.IllegalArgumentException: There is already an RpcEndpoint called ReceiverTracker","error":"<div class=\"ansiout\">\tat org.apache.spark.rpc.netty.Dispatcher.registerRpcEndpoint(Dispatcher.scala:65)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.setupEndpoint(NettyRpcEnv.scala:136)\n\tat org.apache.spark.streaming.scheduler.ReceiverTracker.start(ReceiverTracker.scala:155)\n\tat org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:86)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:619)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:613)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:613)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:613)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:607)</div>","workflows":[],"startTime":1.464842864272E12,"submitTime":1.464842864272E12,"finishTime":1.464842864498E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"fd88fb9d-1788-41b0-9951-6f0e7494de53"},{"version":"CommandV1","origId":111575,"guid":"124455ad-586d-40bd-a5a5-0411599f3821","subtype":"command","commandType":"auto","position":1.99896240234375,"command":"%md\n\nStop Streaming and/or Terminate the Spark Streaming Context (=true) and return when the Streaming job exits or return with the specified timeout.  ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"3bf18a7e-2351-4e33-8459-f088d908bc1f"},{"version":"CommandV1","origId":111576,"guid":"cb462bff-3e03-433c-89f6-956404cea0b0","subtype":"command","commandType":"auto","position":1.9990234375,"command":"ssc.start()\nssc.awaitTerminationOrTimeout(timeoutJobLength)\n//ssc.stop(stopSparkContext = false)","commandVersion":0,"state":"error","results":null,"errorSummary":"java.lang.IllegalStateException: StreamingContext has already been stopped","error":"<div class=\"ansiout\">\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:641)</div>","workflows":[],"startTime":1.464842882549E12,"submitTime":1.464842882549E12,"finishTime":1.46484288282E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"aa658479-3d39-454c-a1fa-76aad7047885"},{"version":"CommandV1","origId":111577,"guid":"2a29834d-8678-4488-9630-33b1aa8d4ccc","subtype":"command","commandType":"auto","position":1.999267578125,"command":"%md\nCheck out the Clusters 'Streaming` UI as the job is running.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"240e877e-f149-4b5d-902c-b0a8ce5f0d60"},{"version":"CommandV1","origId":111578,"guid":"e6a31e4b-d828-4e46-bf11-2f2f3f5e7ec0","subtype":"command","commandType":"auto","position":1.99951171875,"command":"%md\n\nStop any active Streaming Contexts, but don't stop the spark contexts they are attached to.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"982fc11c-a250-4dcb-96b3-07f7244c8156"},{"version":"CommandV1","origId":111579,"guid":"2fdc8c39-a57c-4467-aa88-d266613e7089","subtype":"command","commandType":"auto","position":1.999755859375,"command":"%md\n\n### Step 4: View the Results.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"ba655694-91d4-48eb-be92-59980cca20a3"},{"version":"CommandV1","origId":111580,"guid":"03ee6e03-31ca-4d61-92d2-7ed880ce8402","subtype":"command","commandType":"auto","position":1.99981689453125,"command":"display(dbutils.fs.ls(\"/mnt/s3Data/twitterNew2/\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/mnt/s3Data/twitterNew2/tweets_1463704200000.txt/","tweets_1463704200000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463704500000.txt/","tweets_1463704500000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463704800000.txt/","tweets_1463704800000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463705100000.txt/","tweets_1463705100000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463705400000.txt/","tweets_1463705400000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463705700000.txt/","tweets_1463705700000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463706000000.txt/","tweets_1463706000000.txt/",0.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\""},{"name":"name","type":"\"string\""},{"name":"size","type":"\"long\""}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463706305243E12,"submitTime":1.463706301728E12,"finishTime":1.463706305613E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4f90a141-5dd4-4de2-8b6b-2d768a145fcb"},{"version":"CommandV1","origId":111581,"guid":"e2f92fc6-7c6b-4ca8-95f3-7b9e96eef7f1","subtype":"command","commandType":"auto","position":1.9998321533203125,"command":"%md \nRead each RDD in the DStream as Text File\n\n  * Get the file name from the above `display` and edit the input string to `textFile` below.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"56600921-2984-4161-8d3b-1956b4eae1d5"},{"version":"CommandV1","origId":111582,"guid":"3a6bd0d4-a551-4477-87d4-e967c6186712","subtype":"command","commandType":"auto","position":1.999847412109375,"command":"val rdd1 = sc.textFile(s\"/mnt/s3Data/twitterNew2/tweets_1463704200000.txt/\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">rdd1: org.apache.spark.rdd.RDD[String] = /mnt/s3Data/twitterNew2/tweets_1463704200000.txt/ MapPartitionsRDD[23] at textFile at &lt;console&gt;:46\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463704344403E12,"submitTime":1.463704340969E12,"finishTime":1.463704344522E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e3dd2c87-2266-4246-92c1-82c33d6a7a71"},{"version":"CommandV1","origId":111583,"guid":"1d1bb72b-d5fb-4c86-a450-3eb8c7e7f243","subtype":"command","commandType":"auto","position":1.9998626708984375,"command":"rdd1.count","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res7: Long = 6161\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /mnt/s3Data/twitterNew/tweets_146192040000","error":"<div class=\"ansiout\">\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)</div>","workflows":[],"startTime":1.463704347278E12,"submitTime":1.463704343834E12,"finishTime":1.463704349444E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"b270a085-cb89-427c-99e4-7df25f7a5b9b"},{"version":"CommandV1","origId":111584,"guid":"9b29ac74-7e31-46b9-b665-2232512c6422","subtype":"command","commandType":"auto","position":1.9998703002929688,"command":"rdd1.take(1)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res8: Array[String] = Array({&quot;createdAt&quot;:&quot;May 20, 2016 12:27:12 AM&quot;,&quot;id&quot;:733453989484732416,&quot;text&quot;:&quot;????????????(/ _ ; )\\n??????????&quot;,&quot;source&quot;:&quot;\\u003ca href\\u003d\\&quot;http://twitter.com/download/iphone\\&quot; rel\\u003d\\&quot;nofollow\\&quot;\\u003eTwitter for iPhone\\u003c/a\\u003e&quot;,&quot;isTruncated&quot;:false,&quot;inReplyToStatusId&quot;:-1,&quot;inReplyToUserId&quot;:-1,&quot;isFavorited&quot;:false,&quot;isRetweeted&quot;:false,&quot;favoriteCount&quot;:0,&quot;retweetCount&quot;:0,&quot;isPossiblySensitive&quot;:false,&quot;lang&quot;:&quot;ja&quot;,&quot;contributorsIDs&quot;:[],&quot;userMentionEntities&quot;:[],&quot;urlEntities&quot;:[],&quot;hashtagEntities&quot;:[],&quot;mediaEntities&quot;:[],&quot;extendedMediaEntities&quot;:[],&quot;symbolEntities&quot;:[],&quot;currentUserRetweetId&quot;:-1,&quot;user&quot;:{&quot;id&quot;:282430900,&quot;name&quot;:&quot;??&quot;,&quot;screenName&quot;:&quot;chika22c7&quot;,&quot;location&quot;:&quot;???(Ag_chika22c7)??????3?Z?&quot;,&quot;description&quot;:&quot;??????Kis-My-Ft2(?????)?B?z??????????????????????????????????? ?????????????2??????????? ??????? ???? ???????????J? ??????? ????????????????? ???ID:378792&quot;,&quot;descriptionURLEntities&quot;:[],&quot;isContributorsEnabled&quot;:false,&quot;profileImageUrl&quot;:&quot;http://pbs.twimg.com/profile_images/595666941462843392/-dKjjuMu_normal.jpg&quot;,&quot;profileImageUrlHttps&quot;:&quot;https://pbs.twimg.com/profile_images/595666941462843392/-dKjjuMu_normal.jpg&quot;,&quot;isDefaultProfileImage&quot;:false,&quot;isProtected&quot;:false,&quot;followersCount&quot;:377,&quot;profileBackgroundColor&quot;:&quot;C0DEED&quot;,&quot;profileTextColor&quot;:&quot;333333&quot;,&quot;profileLinkColor&quot;:&quot;0084B4&quot;,&quot;profileSidebarFillColor&quot;:&quot;DDEEF6&quot;,&quot;profileSidebarBorderColor&quot;:&quot;C0DEED&quot;,&quot;profileUseBackgroundImage&quot;:true,&quot;isDefaultProfile&quot;:true,&quot;showAllInlineMedia&quot;:false,&quot;friendsCount&quot;:735,&quot;createdAt&quot;:&quot;Apr 15, 2011 5:59:44 AM&quot;,&quot;favouritesCount&quot;:19482,&quot;utcOffset&quot;:32400,&quot;timeZone&quot;:&quot;Tokyo&quot;,&quot;profileBackgroundImageUrl&quot;:&quot;http://abs.twimg.com/images/themes/theme1/bg.png&quot;,&quot;profileBackgroundImageUrlHttps&quot;:&quot;https://abs.twimg.com/images/themes/theme1/bg.png&quot;,&quot;profileBannerImageUrl&quot;:&quot;https://pbs.twimg.com/profile_banners/282430900/1427615662&quot;,&quot;profileBackgroundTiled&quot;:false,&quot;lang&quot;:&quot;ja&quot;,&quot;statusesCount&quot;:68522,&quot;isGeoEnabled&quot;:true,&quot;isVerified&quot;:false,&quot;translator&quot;:false,&quot;listedCount&quot;:11,&quot;isFollowRequestSent&quot;:false},&quot;quotedStatusId&quot;:-1})\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:58: error: missing arguments for method top in class RDD;\nfollow this method with `_' if you want to treat it as a partially applied function\n              rdd1.top\n                   ^\n</div>","error":null,"workflows":[],"startTime":1.463704353048E12,"submitTime":1.463704349599E12,"finishTime":1.463704353452E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"1a56e3a9-0402-447b-8b29-f64e08519f08"},{"version":"CommandV1","origId":111585,"guid":"6d83ccf5-58f6-4721-8fd7-9f6e53567aad","subtype":"command","commandType":"auto","position":1.9998779296875,"command":"display(dbutils.fs.ls(s\"/mnt/$MountName/${outputDirectory}\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/mnt/s3Data/twitterNew2/tweets_1463704200000.txt/","tweets_1463704200000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463704500000.txt/","tweets_1463704500000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463704800000.txt/","tweets_1463704800000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463705100000.txt/","tweets_1463705100000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463705400000.txt/","tweets_1463705400000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463705700000.txt/","tweets_1463705700000.txt/",0.0],["dbfs:/mnt/s3Data/twitterNew2/tweets_1463706000000.txt/","tweets_1463706000000.txt/",0.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\""},{"name":"name","type":"\"string\""},{"name":"size","type":"\"long\""}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"java.io.FileNotFoundException: /mnt/$MountName/${outputDirectory}","error":"<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:65)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:42)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:189)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:61)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:29)</div>","workflows":[],"startTime":1.463706319285E12,"submitTime":1.463706315772E12,"finishTime":1.463706319534E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"3ffd9561-f5dc-4748-a31e-d35bf7576334"},{"version":"CommandV1","origId":111586,"guid":"ec68b981-c5c8-4aa7-a453-c3c99f662c7a","subtype":"command","commandType":"auto","position":1.99993896484375,"command":"%md\n### 5. Read all the RDD as a Whole Text File","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.461918743494E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"6ed7cd64-0367-43fc-82a9-80c5fb856451"},{"version":"CommandV1","origId":111587,"guid":"697c14eb-bdfa-4a4c-a7a2-b1c35aa36100","subtype":"command","commandType":"auto","position":1.9999542236328125,"command":"//val dStream = sc.wholeTextFiles(s\"/mnt/$MountName/${outputDirectory}\")\nval dStream = sc.textFile(s\"/mnt/s3Data/twitterNew2/*.txt/\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">dStream: org.apache.spark.rdd.RDD[String] = /mnt/s3Data/twitterNew2/*.txt/ MapPartitionsRDD[98] at textFile at &lt;console&gt;:47\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463706332432E12,"submitTime":1.463706328908E12,"finishTime":1.463706332503E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4c1262fd-9a9d-4ccd-b0c4-3b4d764c9330"},{"version":"CommandV1","origId":111588,"guid":"ee8de15e-cb93-473a-b9a8-bdc953436a98","subtype":"command","commandType":"auto","position":1.999969482421875,"command":"dStream.take(1)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res14: Array[String] = Array({&quot;createdAt&quot;:&quot;May 20, 2016 12:27:12 AM&quot;,&quot;id&quot;:733453989484732416,&quot;text&quot;:&quot;????????????(/ _ ; )\\n??????????&quot;,&quot;source&quot;:&quot;\\u003ca href\\u003d\\&quot;http://twitter.com/download/iphone\\&quot; rel\\u003d\\&quot;nofollow\\&quot;\\u003eTwitter for iPhone\\u003c/a\\u003e&quot;,&quot;isTruncated&quot;:false,&quot;inReplyToStatusId&quot;:-1,&quot;inReplyToUserId&quot;:-1,&quot;isFavorited&quot;:false,&quot;isRetweeted&quot;:false,&quot;favoriteCount&quot;:0,&quot;retweetCount&quot;:0,&quot;isPossiblySensitive&quot;:false,&quot;lang&quot;:&quot;ja&quot;,&quot;contributorsIDs&quot;:[],&quot;userMentionEntities&quot;:[],&quot;urlEntities&quot;:[],&quot;hashtagEntities&quot;:[],&quot;mediaEntities&quot;:[],&quot;extendedMediaEntities&quot;:[],&quot;symbolEntities&quot;:[],&quot;currentUserRetweetId&quot;:-1,&quot;user&quot;:{&quot;id&quot;:282430900,&quot;name&quot;:&quot;??&quot;,&quot;screenName&quot;:&quot;chika22c7&quot;,&quot;location&quot;:&quot;???(Ag_chika22c7)??????3?Z?&quot;,&quot;description&quot;:&quot;??????Kis-My-Ft2(?????)?B?z??????????????????????????????????? ?????????????2??????????? ??????? ???? ???????????J? ??????? ????????????????? ???ID:378792&quot;,&quot;descriptionURLEntities&quot;:[],&quot;isContributorsEnabled&quot;:false,&quot;profileImageUrl&quot;:&quot;http://pbs.twimg.com/profile_images/595666941462843392/-dKjjuMu_normal.jpg&quot;,&quot;profileImageUrlHttps&quot;:&quot;https://pbs.twimg.com/profile_images/595666941462843392/-dKjjuMu_normal.jpg&quot;,&quot;isDefaultProfileImage&quot;:false,&quot;isProtected&quot;:false,&quot;followersCount&quot;:377,&quot;profileBackgroundColor&quot;:&quot;C0DEED&quot;,&quot;profileTextColor&quot;:&quot;333333&quot;,&quot;profileLinkColor&quot;:&quot;0084B4&quot;,&quot;profileSidebarFillColor&quot;:&quot;DDEEF6&quot;,&quot;profileSidebarBorderColor&quot;:&quot;C0DEED&quot;,&quot;profileUseBackgroundImage&quot;:true,&quot;isDefaultProfile&quot;:true,&quot;showAllInlineMedia&quot;:false,&quot;friendsCount&quot;:735,&quot;createdAt&quot;:&quot;Apr 15, 2011 5:59:44 AM&quot;,&quot;favouritesCount&quot;:19482,&quot;utcOffset&quot;:32400,&quot;timeZone&quot;:&quot;Tokyo&quot;,&quot;profileBackgroundImageUrl&quot;:&quot;http://abs.twimg.com/images/themes/theme1/bg.png&quot;,&quot;profileBackgroundImageUrlHttps&quot;:&quot;https://abs.twimg.com/images/themes/theme1/bg.png&quot;,&quot;profileBannerImageUrl&quot;:&quot;https://pbs.twimg.com/profile_banners/282430900/1427615662&quot;,&quot;profileBackgroundTiled&quot;:false,&quot;lang&quot;:&quot;ja&quot;,&quot;statusesCount&quot;:68522,&quot;isGeoEnabled&quot;:true,&quot;isVerified&quot;:false,&quot;translator&quot;:false,&quot;listedCount&quot;:11,&quot;isFollowRequestSent&quot;:false},&quot;quotedStatusId&quot;:-1})\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.io.IOException: Not a file: dbfs:/mnt/s3Data/twitterNew2/tweets_1463704200000.txt","error":"<div class=\"ansiout\">\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:215)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1307)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)</div>","workflows":[],"startTime":1.463704526383E12,"submitTime":1.463704522915E12,"finishTime":1.463704527E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"4ed8b787-331f-48ae-bf02-745c1cc3ed00"},{"version":"CommandV1","origId":111589,"guid":"6eea3bc5-2d92-4283-a27d-a855e5477baa","subtype":"command","commandType":"auto","position":1.9999771118164062,"command":"dStream.count //This returns the number of events or tweets in all the RDD stream","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res31: Long = 75410\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.lang.ArrayIndexOutOfBoundsException: 0","error":"<div class=\"ansiout\">\tat org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$OneFileInfo.&lt;init&gt;(CombineFileInputFormat.java:506)\n\tat org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getMoreSplits(CombineFileInputFormat.java:285)\n\tat org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:245)\n\tat org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)</div>","workflows":[],"startTime":1.463706335214E12,"submitTime":1.463706331694E12,"finishTime":1.463706338029E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"9649e556-6d60-4dc0-8d99-066aa39b8e4e"},{"version":"CommandV1","origId":111590,"guid":"fffab225-78ad-44a8-b96a-8c2bcb1c895b","subtype":"command","commandType":"auto","position":1.9999799728393555,"command":"%md A better way of Merging the Files","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"2cdcdf80-afd2-4d3b-9771-b843389b01f2"},{"version":"CommandV1","origId":111591,"guid":"71739851-ad2e-43ac-97a2-b6acc1b5e6b7","subtype":"command","commandType":"auto","position":1.9999828338623047,"command":"val dStreamw = sc.wholeTextFiles(s\"/mnt/s3Data/twitterNew2/*.txt/\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">dStreamw: org.apache.spark.rdd.RDD[(String, String)] = /mnt/s3Data/twitterNew2/*.txt/ MapPartitionsRDD[46] at wholeTextFiles at &lt;console&gt;:46\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463704725781E12,"submitTime":1.463704722322E12,"finishTime":1.463704725877E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"2c927fb9-28df-468d-bdd1-06111170972c"},{"version":"CommandV1","origId":111592,"guid":"371e2d41-1e57-4dba-934d-9ad5392d070c","subtype":"command","commandType":"auto","position":1.9999885559082031,"command":"val dStreamTitle = dStreamw.map(rdd => rdd._1).collect","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">dStreamTitle: Array[String] = Array(dbfs:/mnt/s3Data/twitterNew2/tweets_1463704200000.txt/part-00000, dbfs:/mnt/s3Data/twitterNew2/tweets_1463704500000.txt/part-00000)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:48: error: value _1 is not a member of String\n       val dStreamTitle = dStream.map(rdd =&gt; rdd._1).collect\n                                                 ^\n</div>","error":null,"workflows":[],"startTime":1.463704836807E12,"submitTime":1.46370483334E12,"finishTime":1.463704838254E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"43078cdd-1b1a-4f00-9401-76df387412b1"},{"version":"CommandV1","origId":111593,"guid":"a663bcdf-3707-4bf7-84c8-c0ac27a47456","subtype":"command","commandType":"auto","position":1.9999942779541016,"command":"val dStreamContent = dStreamw.map(rdd => rdd._2)\ndStreamContent.cache","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">dStreamContent: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[90] at map at &lt;console&gt;:50\nres26: dStreamContent.type = MapPartitionsRDD[90] at map at &lt;console&gt;:50\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463706249956E12,"submitTime":1.463706246417E12,"finishTime":1.46370625003E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a5ed8ea5-5da7-4cec-9fd4-7ee3bbb2aa3f"},{"version":"CommandV1","origId":111594,"guid":"4d724216-79fd-4021-9314-3481a6c4d904","subtype":"command","commandType":"auto","position":1.9999971389770508,"command":"dStreamContent.take(1)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"b4738c12-c8a6-461d-9791-f9d2c3d51f7b"},{"version":"CommandV1","origId":111595,"guid":"1cd602c0-027d-4dcc-a80c-a68f2d0b427d","subtype":"command","commandType":"auto","position":2.999997138977051,"command":"%md\n\n# [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)\n\n\n### Course Project by [Akinwande Atanda](https://nz.linkedin.com/in/akinwande-atanda)\n\n*supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)\nand \n[![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"ec64880e-0352-494a-96f7-d9d74b5993a1"}],"dashboards":[],"guid":"4e129535-c0c0-443b-88f1-f844f4d6d3d3","globalVars":{},"iPythonMetadata":null,"inputWidgets":{"2. Recompute the top hashtags every N seconds":{"nuid":"8d42296a-aac0-4a29-93a3-f689053077ef","currentValue":"300","widgetInfo":{"widgetType":"text","name":"2. Recompute the top hashtags every N seconds","defaultValue":"1","label":"","options":{"widgetType":"text","validationRegex":null}}},"4. MNT_NAME":{"nuid":"418ba380-05b8-4d08-a57a-d4233777b087","currentValue":"s3Data","widgetInfo":{"widgetType":"text","name":"4. MNT_NAME","defaultValue":"REPLACE_WITH_YOUR_MOUNT_NAME","label":"","options":{"widgetType":"text","validationRegex":null}}},"2. Interval for each DStream in Minutes ":{"nuid":"795b5ec9-58bb-400d-bcba-58880e754641","currentValue":"10","widgetInfo":{"widgetType":"text","name":"2. Interval for each DStream in Minutes ","defaultValue":"1","label":"","options":{"widgetType":"text","validationRegex":null}}},"2. SECRET_KEY":{"nuid":"7b97b1ef-a2f6-4139-aedc-4a5cc624c545","currentValue":"","widgetInfo":{"widgetType":"text","name":"2. SECRET_KEY","defaultValue":"REPLACE_WITH_YOUR_SECRET_KEY","label":"","options":{"widgetType":"text","validationRegex":null}}},"3. Compute the top hashtags for the last N seconds":{"nuid":"a5710cf4-70ac-4289-8789-3708ce681f58","currentValue":"5","widgetInfo":{"widgetType":"text","name":"3. Compute the top hashtags for the last N seconds","defaultValue":"5","label":"","options":{"widgetType":"text","validationRegex":null}}},"3. S3_BUCKET":{"nuid":"abaf84f9-6175-4645-85ad-0ca6234cfa9e","currentValue":"tweet-bank","widgetInfo":{"widgetType":"text","name":"3. S3_BUCKET","defaultValue":"REPLACE_WITH_YOUR_S3_BUCKET","label":"","options":{"widgetType":"text","validationRegex":null}}},"1. Output Directory":{"nuid":"10841baf-c975-45a3-82f1-f0ffb7958911","currentValue":"/twitterNew3","widgetInfo":{"widgetType":"text","name":"1. Output Directory","defaultValue":"twitterNew3","label":"","options":{"widgetType":"text","validationRegex":null}}},"1. ACCESS_KEY":{"nuid":"ec4a6092-245e-4fd5-9f35-76fee21a056c","currentValue":"","widgetInfo":{"widgetType":"text","name":"1. ACCESS_KEY","defaultValue":"REPLACE_WITH_YOUR_ACCESS_KEY","label":"","options":{"widgetType":"text","validationRegex":null}}},"1. Consumer Key (API Key)":{"nuid":"5ce483c4-1cbf-4a9f-af63-784698728459","currentValue":"","widgetInfo":{"widgetType":"text","name":"1. Consumer Key (API Key)","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"2. Consumer Secret (API Secret)":{"nuid":"f14b6067-6212-4682-b4ea-e8f7f0d0e54c","currentValue":"","widgetInfo":{"widgetType":"text","name":"2. Consumer Secret (API Secret)","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"3. Max Time to fetch all batches of Dstream":{"nuid":"eb30a104-0f04-4b0f-b119-fa5be6c35087","currentValue":"100","widgetInfo":{"widgetType":"text","name":"3. Max Time to fetch all batches of Dstream","defaultValue":"100","label":"","options":{"widgetType":"text","validationRegex":null}}},"4. Wait this many seconds before stopping the streaming job":{"nuid":"3ab48e6d-21e9-4a2b-b7b4-89957199bb29","currentValue":"604800","widgetInfo":{"widgetType":"text","name":"4. Wait this many seconds before stopping the streaming job","defaultValue":"100","label":"","options":{"widgetType":"text","validationRegex":null}}},"4. Access Token Secret":{"nuid":"3e222b90-7e4b-4053-9ac2-2c329da0835a","currentValue":"","widgetInfo":{"widgetType":"text","name":"4. Access Token Secret","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"3. Access Token":{"nuid":"1d7ed522-4f04-4125-ab3b-378d48520033","currentValue":"","widgetInfo":{"widgetType":"text","name":"3. Access Token","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201606222113370000-29e64bf9afe1117763a990704253c3678448e6c5/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
