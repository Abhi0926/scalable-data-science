<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>004_RDDsTransformationsActions - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":true,"defaultInteractivePricePerDBU":0.4,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":true,"enableJobsAclsV2InUI":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"enableAttachExistingCluster":true,"resetJobListOnConnect":true,"serverlessDefaultSparkVersion":"latest-stable-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"class-node","description":"Class Node","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"r3.xlarge","provider":"AWS","local_disk_size_gb":80,"compute_units":13.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":31232,"num_cores":4,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":31232,"is_hidden":false,"category":"Memory Optimized","num_cores":4.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":62464,"is_hidden":false,"category":"Memory Optimized","num_cores":8.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"r3.4xlarge","provider":"AWS","local_disk_size_gb":320,"compute_units":52.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":124928,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":124928,"is_hidden":false,"category":"Memory Optimized","num_cores":16.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"r3.8xlarge","provider":"AWS","local_disk_size_gb":320,"compute_units":104.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":249856,"num_cores":32,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":249856,"is_hidden":false,"category":"Memory Optimized","num_cores":32.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":8079,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge","support_cluster_tags":true,"container_memory_mb":10099,"node_instance_type":{"instance_type_id":"c3.2xlarge","provider":"AWS","local_disk_size_gb":80,"compute_units":28.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":15360,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":15360,"is_hidden":false,"category":"Compute Optimized","num_cores":8.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":19998,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge","support_cluster_tags":true,"container_memory_mb":24998,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":55.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":30720,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":30720,"is_hidden":false,"category":"Compute Optimized","num_cores":16.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":43837,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge","support_cluster_tags":true,"container_memory_mb":54796,"node_instance_type":{"instance_type_id":"c3.8xlarge","provider":"AWS","local_disk_size_gb":320,"compute_units":108.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":61440,"num_cores":32,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":61440,"is_hidden":false,"category":"Compute Optimized","num_cores":32.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"i2.xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":14.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":31232,"num_cores":4,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":31232,"is_hidden":false,"category":"Storage Optimized","num_cores":4.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"i2.2xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":27.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":62464,"is_hidden":false,"category":"Storage Optimized","num_cores":8.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"i2.4xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":53.0,"number_of_ips":4,"local_disks":4,"reserved_compute_units":3.64,"gpus":0,"memory_mb":124928,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":124928,"is_hidden":false,"category":"Storage Optimized","num_cores":16.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"i2.8xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":104.0,"number_of_ips":4,"local_disks":8,"reserved_compute_units":3.64,"gpus":0,"memory_mb":249856,"num_cores":32,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":249856,"is_hidden":false,"category":"Storage Optimized","num_cores":32.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"support_ssh":false,"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":28000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":30720,"is_hidden":true,"category":"Memory Optimized","num_cores":4.0,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":true},{"support_ssh":false,"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":12128,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":55.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":30720,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":15360,"is_hidden":true,"category":"Compute Optimized","num_cores":8.0,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":true}],"default_node_type_id":"class-node"},"sqlAclsDisabledMap":{"spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"serverlessClusterProductName":"Serverless Pool","enableTableHandler":false,"maxEbsVolumesPerInstance":10,"isAdmin":false,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"sqlAclsEnabledMap":{"spark.databricks.acl.enabled":"true","spark.databricks.acl.sqlOnly":"true"},"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"enableManageAccountTab":true,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2a","isDefault":true},{"id":"us-west-2c","isDefault":false},{"id":"us-west-2b","isDefault":false}],"enableCustomSpotPricingUIByTier":true,"serverlessClustersEnabled":true,"enableFindAndReplace":true,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"allowRunOnPendingClusters":true,"applications":false,"enableAzureToolbar":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":true,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"perClusterAutoterminationEnabled":true,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1, deprecated)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.10","displayName":"Spark 2.1.1-db6 (Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-d613235f93e0f29838beb2079a958c02a192ed67a502192bc67a8a5f2fb37f35","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.10","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-7c30ea8be65631e0d0f16e863a60b6034949cb7bdc6491d0f3c19cab5ea0df01","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-experimental-scala2.10","displayName":"Latest experimental (3.3 snapshot, Scala 2.10)","packageLabel":"spark-image-14af3eaf04761a9884d1fa568c90873dc090fbc5fd9d0a6a86ae72f483f8ebc4","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.11","displayName":"Latest RC (3.3 snapshot, Scala 2.11)","packageLabel":"spark-image-567d1fdc02065af09428afbb5c0beed84dab7531a684581aab8039ed0ad866a3","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.11","displayName":"Latest stable (3.1, Scala 2.11)","packageLabel":"spark-image-0e50da7a73d3bc76fcca8ac8a67c1e036c5a30deb5663adf6d0f332cc8ec2c90","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.10","displayName":"Latest RC (3.3 snapshot, Scala 2.10)","packageLabel":"spark-image-14af3eaf04761a9884d1fa568c90873dc090fbc5fd9d0a6a86ae72f483f8ebc4","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.10","displayName":"Latest stable (3.1, Scala 2.10)","packageLabel":"spark-image-0c03d5f78139022c37032b5f3ffac5b5a4445d9cab8733e333f16a67a47262f9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1, deprecated)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.1.x-scala2.11","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-0e50da7a73d3bc76fcca8ac8a67c1e036c5a30deb5663adf6d0f332cc8ec2c90","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.1.x-scala2.10","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-0c03d5f78139022c37032b5f3ffac5b5a4445d9cab8733e333f16a67a47262f9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1, deprecated)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.11","displayName":"Spark 2.1.1-db6 (Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-experimental-scala2.11","displayName":"Latest experimental (3.3 snapshot, Scala 2.11)","packageLabel":"spark-image-567d1fdc02065af09428afbb5c0beed84dab7531a684581aab8039ed0ad866a3","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-6976d15922bb3a45312b79dad792053bb523f1bd12c9c75f7838af84eb3e5e0b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.0.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":false}],"enableClearStateAndRunAll":true,"enableRestrictedClusterCreation":false,"enableFeedback":false,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":6,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":12,"i3.16xlarge":24,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"Standard_L4s":1.5,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":3,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1.5,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.52.607","accountsLimit":-1,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"removePasswordInAccountSettings":false,"preferStartTerminatedCluster":false,"enableNonPollingTableCall":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":true,"enableCreateClusterOnAttach":true,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":2047,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"defaultAutoterminationMin":120,"useDevTierHomePage":false,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","cloud":"AWS","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"enableElasticDisk":true,"logfiles":"logfiles/","enableRelativeNotebookLinks":true,"enableMultiSelect":true,"enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableStructuredDataAcls":false,"serverlessClustersByDefault":false,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"9e1bc816e6e883b788345cd4eeb6c6e593c9f178","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","serverlessAttachEbsVolumesByDefault":false,"enableTokensConfig":false,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableTokens":false,"enableMiniClusters":false,"enableNewJobList":true,"enableNewTableUI":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":false,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":true,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":true,"enableEBSVolumesUIByTier":true,"singleSignOnComingSoon":false,"removeSubCommandCodeWhenExport":true,"upgradeURL":"","maxAutoterminationMinutes":10000,"autoterminateClustersByDefault":false,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":false,"useOnDemandClustersByDefault":false,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":181650,"name":"004_RDDsTransformationsActions","language":"scala","commands":[{"version":"CommandV1","origId":181651,"guid":"ecb3b35d-a693-468c-a895-93ab8e1c3edc","subtype":"command","commandType":"auto","position":0.25,"command":"%md\n\n# [SDS-2.2, Scalable Data Science](https://lamastex.github.io/scalable-data-science/sds/2/2/)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866559,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"25ed0d9d-8541-41f7-b1b6-72c3e8955107"},{"version":"CommandV1","origId":181653,"guid":"584f7c04-5d16-43a9-9acd-f552c0840a9a","subtype":"command","commandType":"auto","position":0.5,"command":"%md\n# **Introduction to Spark**\n## Spark Essentials: RDDs, Transformations and Actions\n\n* This introductory notebook describes how to get started running Spark (Scala) code in Notebooks.\n* Working with Spark's Resilient Distributed Datasets (RDDs)\n  * creating RDDs\n  * performing basic transformations on RDDs\n  * performing basic actions on RDDs\n\n**RECOLLECT** from `001_WhySpark` notebook and AJ's videos (did you watch the ones marked *Watch Now*? if NOT we should watch NOW!!! This was last night's \"Viewing Home-Work\") that *Spark does fault-tolerant, distributed, in-memory computing*","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866587,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":null,"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fb9b36ed-5f37-4cb9-85c2-47a1a02b5bbf"},{"version":"CommandV1","origId":181654,"guid":"aaa6cbeb-a4c9-4b77-8917-ed670de30fd5","subtype":"command","commandType":"auto","position":0.875,"command":"%md\n# Spark Cluster Overview:\n## Driver Program, Cluster Manager and Worker Nodes\n\nThe *driver* does the following:\n1. connects to a *cluster manager* to allocate resources across applications\n* acquire *executors* on cluster nodes\n  * executor processs run compute tasks and cache data in memory or disk on a *worker node*\n* sends *application* (user program built on Spark) to the executors\n* sends *tasks* for the executors to run\n  * task is a unit of work that will be sent to one executor\n  \n![](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n\nSee [http://spark.apache.org/docs/latest/cluster-overview.html](http://spark.apache.org/docs/latest/cluster-overview.html) for an overview of the spark cluster.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866620,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"61cd22d7-2386-4950-8702-4d4e03906fa3"},{"version":"CommandV1","origId":181656,"guid":"56557fb5-a5a9-4dd4-9029-16667b4f79e0","subtype":"command","commandType":"auto","position":1.25,"command":"%md\n## The Abstraction of Resilient Distributed Dataset (RDD)\n\n#### RDD is a fault-tolerant collection of elements that can be operated on in parallel\n\n#### Two types of Operations are possible on an RDD\n* Transformations\n* Actions\n\n**(watch now 2:26)**:\n\n[![RDD in Spark by Anthony Joseph in BerkeleyX/CS100.1x](http://img.youtube.com/vi/3nreQ1N7Jvk/0.jpg)](https://www.youtube.com/watch?v=3nreQ1N7Jvk?rel=0&autoplay=1&modestbranding=1&start=1&end=146)\n\n\n***\n\n## Transformations\n**(watch now 1:18)**:\n\n[![Spark Transformations by Anthony Joseph in BerkeleyX/CS100.1x](http://img.youtube.com/vi/360UHWy052k/0.jpg)](https://www.youtube.com/watch?v=360UHWy052k?rel=0&autoplay=1&modestbranding=1)\n\n***\n\n\n## Actions\n**(watch now 0:48)**:\n\n[![Spark Actions by Anthony Joseph in BerkeleyX/CS100.1x](http://img.youtube.com/vi/F2G4Wbc5ZWQ/0.jpg)](https://www.youtube.com/watch?v=F2G4Wbc5ZWQ?rel=0&autoplay=1&modestbranding=1&start=1&end=48)\n\n***\n\n**Key Points**\n* Resilient distributed datasets (RDDs) are the primary abstraction in Spark.\n* RDDs are immutable once created:\n    * can transform it.\n    * can perform actions on it.\n    * but cannot change an RDD once you construct it.\n* Spark tracks each RDD's lineage information or recipe to enable its efficient recomputation if a machine fails.\n* RDDs enable operations on collections of elements in parallel.\n* We can construct RDDs by:\n    * parallelizing Scala collections such as lists or arrays\n    * by transforming an existing RDD,\n    * from files in distributed file systems such as (HDFS, S3, etc.).\n* We can specify the number of partitions for an RDD\n* The more partitions in an RDD, the more opportunities for parallelism\n* There are **two types of operations** you can perform on an RDD:\n    * **transformations** (are lazily evaluated) \n      * map\n      * flatMap\n      * filter\n      * distinct\n      * ...\n    * **actions** (actual evaluation happens)\n      * count\n      * reduce\n      * take\n      * collect\n      * takeOrdered\n      * ...\n* Spark transformations enable us to create new RDDs from an existing RDD.\n* RDD transformations are lazy evaluations (results are not computed right away)\n* Spark remembers the set of transformations that are applied to a base data set (this is the lineage graph of RDD) \n* The allows Spark to automatically recover RDDs from failures and slow workers.\n* The lineage graph is a recipe for creating a result and it can be optimized before execution.\n* A transformed RDD is executed only when an action runs on it.\n* You can also persist, or cache, RDDs in memory or on disk (this speeds up iterative ML algorithms that transforms the initial RDD iteratively).\n* Here is a great reference URL for working with Spark.\n    * [The latest Spark programming guide](http://spark.apache.org/docs/latest/programming-guide.html).\n    ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866807,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"44e1bd54-e6dd-45aa-bcec-d035356e39eb"},{"version":"CommandV1","origId":181658,"guid":"dd638fa7-c804-496e-9246-dc1a65237f3c","subtype":"command","commandType":"auto","position":1.390625,"command":"%md\n## Let us get our hands dirty in Spark implementing these ideas!","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866873,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"851edea0-e228-4822-8e79-df224c512636"},{"version":"CommandV1","origId":181659,"guid":"ed2bae84-adaf-48f4-8f52-a842340bbfa9","subtype":"command","commandType":"auto","position":1.4375,"command":"%md\n\n#### DO NOW \n\nIn your databricks community edition:\n\n1. In your `WorkSpace' create a Folder named 'scalable-data-science'\n2. 'Import' the databricks archive file at the following URL:\n    * [https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc](https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc)\n3. This should open a structure of directories in with path: `/Workspace/scalable-data-science/xtraResources/'\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866912,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"42c8d077-d350-4209-9091-69bdfd37853e"},{"version":"CommandV1","origId":190584,"guid":"54c59589-5c1f-4e53-8bac-cbc53484599d","subtype":"command","commandType":"auto","position":1.53125,"command":"\n      %md\n### Let us look at the legend and overview of the visual RDD Api by doing the following first:\n\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-1.png)","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"28880e3e-ba11-4db2-a426-cc1c989f447d"},{"version":"CommandV1","origId":181660,"guid":"69776de9-b98a-4ddc-a6ca-54e306c7ee87","subtype":"command","commandType":"auto","position":1.625,"command":"%md\n### Running **Spark**\nThe variable **sc** allows you to access a Spark Context to run your Spark programs.\nRecall ``SparkContext`` is in the Driver Program.\n\n![](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n\n**NOTE: Do not create the *sc* variable - it is already initialized for you. **","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866949,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"markdown","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":null,"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7fe76b6a-558d-438b-84aa-3f5ad14bb20d"},{"version":"CommandV1","origId":181661,"guid":"87b9a289-0407-42a9-8b3e-99c87909f4d2","subtype":"command","commandType":"auto","position":1.8125,"command":"%md\n### We will do the following next:\n1. Create an RDD using `sc.parallelize`\n* Perform the `collect` action on the RDD and find the number of partitions it is made of using `getNumPartitions` action\n* Perform the ``take`` action on the RDD\n* Transform the RDD by ``map`` to make another RDD\n* Transform the RDD by ``filter`` to make another RDD\n* Perform the ``reduce`` action on the RDD\n* Transform the RDD by ``flatMap`` to make another RDD\n* Create a Pair RDD\n* Perform some transformations on a Pair RDD\n* Where in the cluster is your computation running?\n* Shipping Closures, Broadcast Variables and Accumulator Variables\n* Spark Essentials: Summary\n* HOMEWORK","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369866987,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1052e9f0-d44e-46b1-be5a-da9cc6eba50b"},{"version":"CommandV1","origId":181662,"guid":"62e619e6-a02b-4365-a729-26b9f607613b","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n### 1. Create an RDD using `sc.parallelize`\n\nFirst, let us create an RDD of three elements (of integer type ``Int``) from a Scala ``Seq`` (or ``List`` or ``Array``) with two partitions by using the ``parallelize`` method of the available Spark Context ``sc`` as follows:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867026,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e1737683-d03a-483c-a3f5-5280ab1d1944"},{"version":"CommandV1","origId":181663,"guid":"dd340a15-9820-440d-be03-e8b9df768d5f","subtype":"command","commandType":"auto","position":2.125,"command":"val x = sc.parallelize(Array(1, 2, 3), 2)    // <Ctrl+Enter> to evaluate this cell (using 2 partitions)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[181331] at parallelize at &lt;console&gt;:34\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1504163781265,"submitTime":1504163790284,"finishTime":1504163781745,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"17955eee-988d-4ae4-9159-cfb91ab77f1c"},{"version":"CommandV1","origId":181664,"guid":"eb0fa6be-e7cb-44ca-a9c2-13121a62686e","subtype":"command","commandType":"auto","position":2.25,"command":"x.  // place the cursor after 'x.' and hit Tab to see the methods available for the RDD x we created","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0aae707c-e893-43d9-be45-cb8d07c44e1a"},{"version":"CommandV1","origId":181665,"guid":"17847599-2e74-4eb9-8e3f-ca0ba7c2a563","subtype":"command","commandType":"auto","position":2.28125,"command":"%md\n### 2. Perform the `collect` action on the RDD and find the number of partitions it is made of using `getNumPartitions` action\n\nNo action has been taken by ``sc.parallelize`` above.  To see what is \"cooked\" by the recipe for RDD ``x`` we need to take an action.  \n\nThe simplest is the ``collect`` action which returns all of the elements of the RDD as an ``Array`` to the driver program and displays it.\n\n*So you have to make sure that all of that data will fit in the driver program if you call ``collect`` action!*","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867089,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e51a6f69-b10b-4cb2-8129-4a1be61e5f14"},{"version":"CommandV1","origId":181666,"guid":"06ee1b6a-b2d7-4ffc-906e-1422701b23bc","subtype":"command","commandType":"auto","position":2.3125,"command":"%md\n#### Let us look at the [collect action in detail](/#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/collect) and return here to try out the example codes.\n\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-90.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867132,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8c33e95a-5cee-4822-8972-fc764f63bdfc"},{"version":"CommandV1","origId":181667,"guid":"5196ba11-5f69-412c-97f5-3df24db95efc","subtype":"command","commandType":"auto","position":2.34375,"command":"%md\nLet us perform a `collect` action on RDD `x` as follows: ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e0a3a687-cc92-40d5-9fb5-ec6c0d0d86b7"},{"version":"CommandV1","origId":181668,"guid":"fd823bcf-745f-4507-b208-8eafa2c58553","subtype":"command","commandType":"auto","position":2.359375,"command":"x.collect()    // <Ctrl+Enter> to collect (action) elements of rdd; should be (1, 2, 3)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res0: Array[Int] = Array(1, 2, 3)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978135294,"submitTime":1456978111592,"finishTime":1456978135411,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"52b088a3-e334-4d43-becc-7d7c71fc175b"},{"version":"CommandV1","origId":181669,"guid":"ca1315e0-26bd-419f-8048-99c38011a03f","subtype":"command","commandType":"auto","position":2.375,"command":"%md\n*CAUTION:* ``collect`` can crash the driver when called upon an RDD with massively many elements.  \nSo, it is better to use other diplaying actions like ``take`` or ``takeOrdered`` as follows:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867183,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7ca905de-b775-46c7-8cc2-21cf3f195d8e"},{"version":"CommandV1","origId":181670,"guid":"58e9e55c-f192-4288-9d9d-f57b64648812","subtype":"command","commandType":"auto","position":2.3857421875,"command":"%md\n#### Let us look at the [getNumPartitions action in detail](/#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/getNumPartitions) and return here to try out the example codes.\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-88.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"da3128c5-4524-4cc5-85af-e96b9272abae"},{"version":"CommandV1","origId":181671,"guid":"16078e7a-57aa-4bfe-8a3d-ecd69244e44e","subtype":"command","commandType":"auto","position":2.38671875,"command":"// <Ctrl+Enter> to evaluate this cell and find the number of partitions in RDD x\nx.getNumPartitions ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res1: Int = 2\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:42: error: Int does not take parameters\n              x.getNumPartitions()\n                                ^\n</div>","error":null,"workflows":[],"startTime":1456978141464,"submitTime":1456978117265,"finishTime":1456978141546,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9751bff4-985f-425b-bd47-673335349146"},{"version":"CommandV1","origId":181672,"guid":"911a52b1-1a80-4782-96d0-363bbfb25571","subtype":"command","commandType":"auto","position":2.390625,"command":"%md\nWe can see which elements of the RDD are in which parition by calling `glom()` before `collect()`. \n\n`glom()` flattens elements of the same partition into an `Array`. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"365ebcee-e2e4-4b65-a02b-d0243500623c"},{"version":"CommandV1","origId":181673,"guid":"8b5a9ae0-ffa4-4202-aa6e-5187497e2393","subtype":"command","commandType":"auto","position":2.3984375,"command":"x.glom().collect() // glom() flattens elements on the same partition","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res2: Array[Array[Int]] = Array(Array(1), Array(2, 3))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978161890,"submitTime":1456978138216,"finishTime":1456978161978,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b245ddb3-26f5-4b8a-8d38-80d6a4c85e76"},{"version":"CommandV1","origId":181674,"guid":"21416f1e-59ba-455b-94a7-1ff6ec2bc282","subtype":"command","commandType":"auto","position":2.40625,"command":"%md\nThus from the output above, `Array[Array[Int]] = Array(Array(1), Array(2, 3))`, we know that `1` is in one partition while `2` and `3` are in another partition.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c940fe14-d434-4bd0-ae80-4ec95f64de48"},{"version":"CommandV1","origId":181675,"guid":"d15bf824-acc1-47f0-b977-1a9445dbfa93","subtype":"command","commandType":"auto","position":2.421875,"command":"%md\n##### You Try!\nCrate an RDD `x` with three elements, 1,2,3, and this time do not specifiy the number of partitions.  Then the default number of partitions will be used.\nFind out what this is for the cluster you are attached to. \n\nThe default number of partitions for an RDD depends on the cluster this notebook is attached to among others - see [programming-guide](http://spark.apache.org/docs/latest/programming-guide.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b9283b04-6256-4d37-9584-f698ae6f3d0d"},{"version":"CommandV1","origId":181676,"guid":"5cc69d0e-8b34-4d3c-8984-c24b8fafd290","subtype":"command","commandType":"auto","position":2.427734375,"command":"val x = sc.parallelize(Seq(1, 2, 3))    // <Shift+Enter> to evaluate this cell (using default number of partitions)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">Unclosed block</div>","error":null,"workflows":[],"startTime":1457043380201,"submitTime":1457043334101,"finishTime":1457043381764,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f72a0fa1-d810-41dd-b747-9025cebe020f"},{"version":"CommandV1","origId":181677,"guid":"81771679-1ed9-4273-9a94-2a753c2e7fa8","subtype":"command","commandType":"auto","position":2.4296875,"command":"x.getNumPartitions // <Shift+Enter> to evaluate this cell","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1457043387980,"submitTime":1457043341860,"finishTime":1457043388024,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"db95c084-8c4a-496d-b31f-50127bc7c1de"},{"version":"CommandV1","origId":181678,"guid":"6c23874d-5360-4aef-be79-deb6d5836f03","subtype":"command","commandType":"auto","position":2.43359375,"command":"x.glom().collect() // <Ctrl+Enter> to evaluate this cell","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1457043401746,"submitTime":1457043355645,"finishTime":1457043401789,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"97385778-2b71-44df-b996-b2cedbadebc3"},{"version":"CommandV1","origId":181679,"guid":"551b4674-bf59-4465-a5e5-ba20a2f6f210","subtype":"command","commandType":"auto","position":2.4375,"command":"%md\n### 3. Perform the `take` action on the RDD\n\nThe ``.take(n)`` action returns an array with the first ``n`` elements of the RDD.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867223,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ef02b6c7-3306-48e3-8de6-42063a6b2492"},{"version":"CommandV1","origId":181680,"guid":"27132770-aee1-4e8f-aa7b-421e0d0201df","subtype":"command","commandType":"auto","position":2.46875,"command":"x.take(2) // Ctrl+Enter to take two elements from the RDD x","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res5: Array[Int] = Array(1, 2)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978574509,"submitTime":1456978550803,"finishTime":1456978574632,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"514c0a77-8d0f-4daf-a135-49293d4a3f98"},{"version":"CommandV1","origId":181681,"guid":"1e25ae02-031d-4f95-8467-f988defd3bcf","subtype":"command","commandType":"auto","position":2.4754638671875,"command":"%md\n##### You Try!\nFill in the parenthes `( )` below in order to `take` just one element from RDD `x`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4f41ffcd-ba33-43f4-830c-3f393ededd7f"},{"version":"CommandV1","origId":181682,"guid":"25f1c1eb-63cb-43c7-b180-be2733eaa8a9","subtype":"command","commandType":"auto","position":2.482177734375,"command":"//x.take(  ) // uncomment by removing '//' before x in the cell and fill in the parenthesis to take just one element from RDD x and Cntrl+Enter","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1457043423410,"submitTime":1457043377301,"finishTime":1457043423453,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c1614fae-a61b-49ba-ade3-5f6cd5024b48"},{"version":"CommandV1","origId":181683,"guid":"d1a20933-8755-48ea-9c7f-af55d161859c","subtype":"command","commandType":"auto","position":2.49560546875,"command":"%md\n***\n\n### 4. Transform the RDD by ``map`` to make another RDD\n\nThe ``map`` transformation returns a new RDD that's formed by passing each element of the source RDD through a function (closure). The closure is automatically passed on to the workers for evaluation (when an action is called later). ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867281,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6a4c41f7-ba92-4fc9-97d3-ae4fd4518a94"},{"version":"CommandV1","origId":181684,"guid":"470f3b2a-8942-4a27-be09-65cfa3730eaa","subtype":"command","commandType":"auto","position":2.4971160888671875,"command":"%md\n#### Let us look at the [map transformation in detail](/#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/map) and return here to try out the example codes.\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-18.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867322,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b8cfc765-05dd-4540-8276-e1d292f104b2"},{"version":"CommandV1","origId":181685,"guid":"9b8b3660-2296-45b6-9e08-e243dd354360","subtype":"command","commandType":"auto","position":2.4978370666503906,"command":"// Shift+Enter to make RDD x and RDD y that is mapped from x\nval x = sc.parallelize(Array(\"b\", \"a\", \"c\")) // make RDD x: [b, a, c]\nval y = x.map(z => (z,1))                    // map x into RDD y: [(b, 1), (a, 1), (c, 1)]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[734] at parallelize at &lt;console&gt;:37\ny: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[735] at map at &lt;console&gt;:38\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978839332,"submitTime":1456978815577,"finishTime":1456978839716,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"aa5f77e9-fe83-4e0a-8c87-c454eba650de"},{"version":"CommandV1","origId":181686,"guid":"a9382207-0a7b-46c7-ac73-7e8fb0698df3","subtype":"command","commandType":"auto","position":2.498377799987793,"command":"// Cntrl+Enter to collect and print the two RDDs\nprintln(x.collect().mkString(\", \"))\nprintln(y.collect().mkString(\", \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">b, a, c\n(b,1), (a,1), (c,1)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978875692,"submitTime":1456978851700,"finishTime":1456978875865,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ea192225-1a29-430c-829b-a139056dc9e2"},{"version":"CommandV1","origId":181687,"guid":"d3a30c7c-2042-4ff7-827f-0fb88d5a5ecb","subtype":"command","commandType":"auto","position":2.748046875,"command":"%md\n***\n\n### 5. Transform the RDD by ``filter`` to make another RDD\n\nThe ``filter`` transformation returns a new RDD that's formed by selecting those elements of the source RDD on which the function returns ``true``.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867386,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3d6c791c-4495-4ff7-9cf0-ef138f4aa8e7"},{"version":"CommandV1","origId":181688,"guid":"d9d3cf0f-b427-43fc-95bd-3ea2cf964ba8","subtype":"command","commandType":"auto","position":2.748291015625,"command":"%md\n#### Let us look at the [filter transformation in detail](/#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/filter) and return here to try out the example codes.\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-24.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867426,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6536d958-75ee-4826-a881-86c613abbcb0"},{"version":"CommandV1","origId":181689,"guid":"dc0d357b-1cdd-403e-919a-0763c6b29528","subtype":"command","commandType":"auto","position":2.74835205078125,"command":"//Shift+Enter to make RDD x and filter it by (n => n%2 == 1) to make RDD y\nval x = sc.parallelize(Array(1,2,3))\n// the closure (n => n%2 == 1) in the filter will \n// return True if element n in RDD x has remainder 1 when divided by 2 (i.e., if n is odd)\nval y = x.filter(n => n%2 == 1) ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[768] at parallelize at &lt;console&gt;:37\ny: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[769] at filter at &lt;console&gt;:40\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978967413,"submitTime":1456978943688,"finishTime":1456978967555,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5e9e0566-ca72-4bc5-8768-0055c2f887f0"},{"version":"CommandV1","origId":181690,"guid":"9355dfbf-26cf-41a2-b6aa-faf1bf219903","subtype":"command","commandType":"auto","position":2.748382568359375,"command":"// Cntrl+Enter to collect and print the two RDDs\nprintln(x.collect().mkString(\", \"))\nprintln(y.collect().mkString(\", \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1, 2, 3\n1, 3\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456978975435,"submitTime":1456978951717,"finishTime":1456978975582,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"09568fe7-70b3-4187-85d5-b1090b73db58"},{"version":"CommandV1","origId":181691,"guid":"2a3e8f58-54e5-407d-884f-342d82ec4937","subtype":"command","commandType":"auto","position":2.75,"command":"%md\n***\n### 6. Perform the ``reduce`` action on the RDD\n\nReduce aggregates a data set element using a function (closure). \nThis function takes two arguments and returns one and can often be seen as a binary operator. \nThis operator has to be commutative and associative so that it can be computed correctly in parallel (where we have little control over the order of the operations!).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867500,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"39dbd525-5cd9-41d4-b7f7-23e6614f0643"},{"version":"CommandV1","origId":181692,"guid":"6a063a6f-103d-4278-958c-edb07bd6621e","subtype":"command","commandType":"auto","position":2.765625,"command":"%md\n### Let us look at the [reduce action in detail](/#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/reduce) and return here to try out the example codes.\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-94.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867539,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b2261f35-448c-472f-8ad7-39c52b5be335"},{"version":"CommandV1","origId":181693,"guid":"e9ecab19-f1e1-4099-bb84-6488467a4a9f","subtype":"command","commandType":"auto","position":2.7734375,"command":"//Shift+Enter to make RDD x of inteegrs 1,2,3,4 and reduce it to sum\nval x = sc.parallelize(Array(1,2,3,4))\nval y = x.reduce((a,b) => a+b)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[808] at parallelize at &lt;console&gt;:37\ny: Int = 10\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979104661,"submitTime":1456979080930,"finishTime":1456979104824,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bb7df362-19b8-45c8-9d54-744623ef944f"},{"version":"CommandV1","origId":181694,"guid":"3bec60e1-c0f5-48b7-a904-1ccaec41438f","subtype":"command","commandType":"auto","position":2.77734375,"command":"//Cntrl+Enter to collect and print RDD x and the Int y, sum of x\nprintln(x.collect.mkString(\", \"))\nprintln(y)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1, 2, 3, 4\n10\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979114231,"submitTime":1456979090459,"finishTime":1456979114327,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a2a1b18d-d0e2-42d3-b0c7-a56739768505"},{"version":"CommandV1","origId":181695,"guid":"27f6ed13-cd9a-47ad-81a1-d33061801fe2","subtype":"command","commandType":"auto","position":2.994140625,"command":"%md\n### 7. Transform an RDD by ``flatMap`` to make another RDD\n\n``flatMap`` is similar to ``map`` but each element from input RDD can be mapped to zero or more output elements. \nTherefore your function should return a sequential collection such as an ``Array`` rather than a single element as shown below.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867607,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b10dcfd8-4806-477e-b268-51c3bdf31bb6"},{"version":"CommandV1","origId":181696,"guid":"284715e0-e617-4549-9212-d804f2d513a7","subtype":"command","commandType":"auto","position":2.9951171875,"command":"%md\n### Let us look at the [flatMap transformation in detail](/#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/flatMap) and return here to try out the example codes.\n\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-31.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867699,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"92d5a0ad-b58a-4eb4-9c5f-4bf8335af8ce"},{"version":"CommandV1","origId":181697,"guid":"956edc5a-c559-4096-8674-b1dbcc57633f","subtype":"command","commandType":"auto","position":2.99560546875,"command":"//Shift+Enter to make RDD x and flatMap it into RDD by closure (n => Array(n, n*100, 42))\nval x = sc.parallelize(Array(1,2,3))\nval y = x.flatMap(n => Array(n, n*100, 42))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[844] at parallelize at &lt;console&gt;:37\ny: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[845] at flatMap at &lt;console&gt;:38\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979235253,"submitTime":1456979211370,"finishTime":1456979235376,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"929c7cbd-c640-42cc-9be5-8668505e11a4"},{"version":"CommandV1","origId":181698,"guid":"280fecdd-3485-4be3-bf6e-178f813a1d1f","subtype":"command","commandType":"auto","position":2.995849609375,"command":"//Cntrl+Enter to collect and print RDDs x and y\nprintln(x.collect().mkString(\", \"))\nprintln(y.collect().mkString(\", \"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1, 2, 3\n1, 100, 42, 2, 200, 42, 3, 300, 42\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979240991,"submitTime":1456979216787,"finishTime":1456979241165,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"892c5ffd-0f14-4813-9aea-876933c555ae"},{"version":"CommandV1","origId":181699,"guid":"7ef0a7d2-dd3a-4e47-916a-9339327d41ef","subtype":"command","commandType":"auto","position":2.999755859375,"command":"%md\n### 8. Create a Pair RDD\n\nLet's next work with RDD of ``(key,value)`` pairs called a *Pair RDD* or *Key-Value RDD*.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867782,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c5e96697-360a-49b6-bae7-a2c4df8cd3fd"},{"version":"CommandV1","origId":181700,"guid":"8ee48af0-d7a5-4ebe-ba78-b274014e6ecc","subtype":"command","commandType":"auto","position":3.0,"command":"// Cntrl+Enter to make RDD words and display it by collect\nval words = sc.parallelize(Array(\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\", \"a\", \"b\", \"b\"))\nwords.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[859] at parallelize at &lt;console&gt;:35\nres13: Array[String] = Array(a, b, a, a, b, b, a, a, a, b, b)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":"<div class=\"ansiout\">&lt;console&gt;:12: error: value reduceByKey is not a member of org.apache.spark.rdd.RDD[(String, Int)]\n       val wordcounts = words.map(s =&gt; (s, 1)).reduceByKey(_ + _).collect()\n                                               ^\n</div>","workflows":[],"startTime":1456979296284,"submitTime":1456979272551,"finishTime":1456979296410,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":null,"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"72fa4658-f513-4b98-bc36-45377c409e6d"},{"version":"CommandV1","origId":181701,"guid":"6e7767e5-e22d-4cb8-9865-61014b613c52","subtype":"command","commandType":"auto","position":3.46875,"command":"%md\nLet's make a Pair RDD called `wordCountPairRDD` that is made of (key,value) pairs with key=word and value=1 in order to encode each occurrence of each word in the RDD `words`, as follows:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3de0c059-b4e5-4653-ad84-944bc365d24f"},{"version":"CommandV1","origId":181702,"guid":"0d67da23-6a2d-43a1-9f01-4057ab8785d8","subtype":"command","commandType":"auto","position":3.9375,"command":"// Cntrl+Enter to make and collect Pair RDD wordCountPairRDD\nval wordCountPairRDD = words.map(s => (s, 1))\nwordCountPairRDD.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[867] at map at &lt;console&gt;:37\nres14: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979327798,"submitTime":1456979304070,"finishTime":1456979327906,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"55414096-0963-4b6c-8d27-b1d79f40a463"},{"version":"CommandV1","origId":181703,"guid":"3127e985-18d3-4c51-9e2f-a1ee6f88ba26","subtype":"command","commandType":"auto","position":4.171875,"command":"%md\n### 9. Perform some transformations on a Pair RDD\n\nLet's next work with RDD of ``(key,value)`` pairs called a *Pair RDD* or *Key-Value RDD*.\n\nNow some of the Key-Value transformations that we could perform include the following.\n* **`reduceByKey` transformation**\n  * which takes an RDD and returns a new RDD of key-value pairs, such that:\n    * the values for each key are aggregated using the given reduced function\n    * and the reduce function has to be of the type that takes two values and returns one value.\n* **`sortByKey` transformation**\n  * this returns a new RDD of key-value pairs that's sorted by keys in ascending order\n* **`groupByKey` transformation**\n  * this returns a new RDD consisting of key and iterable-valued pairs.\n\nLet's see some concrete examples next.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0e9287e6-fce6-4f1b-95b3-633815816967"},{"version":"CommandV1","origId":181704,"guid":"2038f2da-a8b2-4340-92b2-3611561df9b4","subtype":"command","commandType":"auto","position":4.376953125,"command":"%md\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-44.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867817,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9a2fbd18-87b7-492c-8498-4892811c2ca5"},{"version":"CommandV1","origId":181705,"guid":"f66cde9f-47e0-41b1-af2a-d453823c23f2","subtype":"command","commandType":"auto","position":4.40625,"command":"// Cntrl+Enter to reduceByKey and collect wordcounts RDD\n//val wordcounts = wordCountPairRDD.reduceByKey( _ + _ )\nval wordcounts = wordCountPairRDD.reduceByKey( (v1,v2) => v1+v2 )\nwordcounts.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[908] at reduceByKey at &lt;console&gt;:42\nres16: Array[(String, Int)] = Array((a,6), (b,5))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979500836,"submitTime":1456979477097,"finishTime":1456979501045,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"33fafe8e-039e-4437-afaa-f19368dc8a38"},{"version":"CommandV1","origId":181706,"guid":"4a8b7ddd-8626-40f4-8750-4a51486cdbc1","subtype":"command","commandType":"auto","position":4.875,"command":"%md\nNow, let us do just the crucial steps and avoid collecting intermediate RDDs (something we should avoid for large datasets anyways, as they may not fit in the driver program).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fa6d164e-45f1-4864-83a5-55a2a29b560c"},{"version":"CommandV1","origId":181707,"guid":"c0d35dab-0f2b-476c-8e9d-3e031f3c3efb","subtype":"command","commandType":"auto","position":6.75,"command":"//Cntrl+Enter to make words RDD and do the word count in two lines\nval words = sc.parallelize(Array(\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\", \"a\", \"b\", \"b\"))\nval wordcounts = words.map(s => (s, 1)).reduceByKey(_ + _).collect() ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[914] at parallelize at &lt;console&gt;:37\nwordcounts: Array[(String, Int)] = Array((a,6), (b,5))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979529013,"submitTime":1456979505291,"finishTime":1456979529248,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cadeb587-7c91-4b04-b2e8-77ec147dc0b0"},{"version":"CommandV1","origId":181708,"guid":"d4433c87-86b5-4298-994c-87639a5fa385","subtype":"command","commandType":"auto","position":8.625,"command":"%md\n##### You Try!\nYou try evaluating `sortByKey()` which will make a new RDD that consists of the elements of the original pair RDD that are sorted by Keys.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"99ac2055-9092-4254-8aaf-72ad2ba2856c"},{"version":"CommandV1","origId":181709,"guid":"8ff6acbf-b8f8-4381-b882-87193710cd59","subtype":"command","commandType":"auto","position":9.5625,"command":"// Shift+Enter and comprehend code\nval words = sc.parallelize(Array(\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\", \"a\", \"b\", \"b\"))\nval wordCountPairRDD = words.map(s => (s, 1))\nval wordCountPairRDDSortedByKey = wordCountPairRDD.sortByKey()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:40: error: value sortByKey is not a member of Array[(String, Int)]\n              wordcounts.sortByKey()\n                         ^\n</div>","error":null,"workflows":[],"startTime":1457043481760,"submitTime":1457043435645,"finishTime":1457043481805,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"26dca38e-25a9-48d5-9df2-bb3e2c5656ee"},{"version":"CommandV1","origId":181710,"guid":"60f60718-7417-49d2-8bb6-fb525a40768e","subtype":"command","commandType":"auto","position":9.796875,"command":"wordCountPairRDD.collect() // Shift+Enter and comprehend code","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1457043498319,"submitTime":1457043452214,"finishTime":1457043498359,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"590e6f43-8e99-4b16-920a-7248f541faca"},{"version":"CommandV1","origId":181711,"guid":"859c28a3-1ee7-4f1d-8574-4a857623b81f","subtype":"command","commandType":"auto","position":10.03125,"command":"wordCountPairRDDSortedByKey.collect() // Cntrl+Enter and comprehend code","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1457043501967,"submitTime":1457043455854,"finishTime":1457043502009,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3724d8e8-6cc4-438b-b01c-f8ab19afb0ac"},{"version":"CommandV1","origId":181712,"guid":"ec3b3084-87b1-479b-aa6d-250eb484f8c6","subtype":"command","commandType":"auto","position":10.08984375,"command":"%md\n\nThe next key value transformation we will see is `groupByKey`\n\nWhen we apply the `groupByKey` transformation to `wordCountPairRDD` we end up with a new RDD that contains two elements.\nThe first element is the tuple `b` and an iterable `CompactBuffer(1,1,1,1,1)` obtained by grouping the value `1` for each of the five key value pairs `(b,1)`.\nSimilarly the second element is the key `a` and an iterable `CompactBuffer(1,1,1,1,1,1)` obtained by grouping the value `1` for each of the six key value pairs `(a,1)`.\n\n*CAUTION*: `groupByKey` can cause a large amount of data movement across the network.\nIt also can create very large iterables at a worker.\nImagine you have an RDD where you have 1 billion pairs that have the key `a`.\nAll of the values will have to fit in a single worker if you use group by key.\nSo instead of a group by key, consider using reduced by key.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c81438da-16ef-47dd-b32e-5a49f84f6415"},{"version":"CommandV1","origId":181713,"guid":"6a6ef579-4214-4ff7-b8b6-96cf6d7ab31d","subtype":"command","commandType":"auto","position":10.1484375,"command":"%md\n![](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-45.png)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"89367975-7cc7-46c8-be7c-2cef5577ead5"},{"version":"CommandV1","origId":181714,"guid":"6a1523ae-6a4f-4a31-9c16-6c15ef5778a6","subtype":"command","commandType":"auto","position":10.265625,"command":"val wordCountPairRDDGroupByKey = wordCountPairRDD.groupByKey() // <Shift+Enter> CAUTION: this transformation can be very wide!","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">wordCountPairRDDGroupByKey: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[984] at groupByKey at &lt;console&gt;:38\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979738257,"submitTime":1456979714513,"finishTime":1456979738346,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f979fb92-a790-493e-aeb7-834839d76a16"},{"version":"CommandV1","origId":181715,"guid":"75d78706-5a65-42d0-8f01-f4cc559e3ae2","subtype":"command","commandType":"auto","position":10.3828125,"command":"wordCountPairRDDGroupByKey.collect()  // Cntrl+Enter","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res19: Array[(String, Iterable[Int])] = Array((a,CompactBuffer(1, 1, 1, 1, 1, 1)), (b,CompactBuffer(1, 1, 1, 1, 1)))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456979740674,"submitTime":1456979716945,"finishTime":1456979740943,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ad290d60-a2e6-422b-90fc-2b8377a95ae8"},{"version":"CommandV1","origId":181716,"guid":"c1278a3a-307a-49fb-89c1-909220a6b2ee","subtype":"command","commandType":"auto","position":10.44140625,"command":"%md\n### 10. Where in the cluster is your computation running?\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"63ba7ec8-d52f-4d92-8e47-220916f6c2ac"},{"version":"CommandV1","origId":181717,"guid":"560ec68f-5982-4239-ba3c-7456b07634e9","subtype":"command","commandType":"auto","position":10.5,"command":"val list = 1 to 10\nvar sum = 0\nlist.map(x => sum = sum + x)\nprint(sum)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">55list: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum: Int = 55\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1504680747057,"submitTime":1504680764828,"finishTime":1504680747407,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9e604314-b215-409a-803e-d554035e7186"},{"version":"CommandV1","origId":181718,"guid":"a0b95e25-7c39-48a4-b56d-3c654af1058c","subtype":"command","commandType":"auto","position":14.25,"command":"val rdd = sc.parallelize(1 to 10)\nvar sum = 0","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[358776] at parallelize at &lt;console&gt;:34\nsum: Int = 0\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:44: error: value collect is not a member of Unit\n       rdd1.collect\n            ^\n</div>","error":null,"workflows":[],"startTime":1504680696390,"submitTime":1504680714166,"finishTime":1504680696636,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d58fbb1e-f228-4795-976e-cec185b24597"},{"version":"CommandV1","origId":191882,"guid":"8b40ac2f-8800-47af-94ca-84e220b9f97b","subtype":"command","commandType":"auto","position":15.1875,"command":"val rdd1 = rdd.map(x => sum = sum + x)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">rdd1: org.apache.spark.rdd.RDD[Unit] = MapPartitionsRDD[358778] at map at &lt;console&gt;:38\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1504680775359,"submitTime":1504680793134,"finishTime":1504680775552,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"07b18838-c9c7-4e77-8a5d-42f152d4b905"},{"version":"CommandV1","origId":191883,"guid":"609d41f1-b721-43c3-a524-49ab9bd7f03d","subtype":"command","commandType":"auto","position":15.65625,"command":"rdd1.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res24: Array[Unit] = Array((), (), (), (), (), (), (), (), (), ())\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1504680813256,"submitTime":1504680830983,"finishTime":1504680813502,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"04425b30-b0b2-440f-a82b-c8625f8f010b"},{"version":"CommandV1","origId":191884,"guid":"f620f73c-61f5-460e-8f94-997cd4def717","subtype":"command","commandType":"auto","position":15.890625,"command":"val rdd1 = rdd.map(x => {var sum = 0;\n                         sum = sum + x\n                         sum}\n                  )","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">rdd1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[358779] at map at &lt;console&gt;:38\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:39: error: reassignment to val\n                                sum = sum + x\n                                    ^\n</div>","error":null,"workflows":[],"startTime":1504680906806,"submitTime":1504680924590,"finishTime":1504680906989,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"618723b1-c612-4f58-a3df-e0e353aed661"},{"version":"CommandV1","origId":191885,"guid":"8dc56a7c-97c3-4748-a815-de62dc84abcc","subtype":"command","commandType":"auto","position":16.0078125,"command":"rdd1.collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res25: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1504680930990,"submitTime":1504680948756,"finishTime":1504680931174,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e6d1853e-e6c6-421d-a43d-1b31d40b1bdd"},{"version":"CommandV1","origId":181719,"guid":"d41a4e98-d6fa-4386-9248-2f25368a7a7d","subtype":"command","commandType":"auto","position":16.125,"command":"%md\n### 11. Shipping Closures, Broadcast Variables and Accumulator Variables\n\n#### Closures, Broadcast and Accumulator Variables\n**(watch now 2:06)**:\n\n[![Closures, Broadcast and Accumulators by Anthony Joseph in BerkeleyX/CS100.1x](http://img.youtube.com/vi/I9Zcr4R35Ao/0.jpg)](https://www.youtube.com/watch?v=I9Zcr4R35Ao?rel=0&autoplay=1&modestbranding=1)\n\n\nWe will use these variables in the sequel.\n\n#### SUMMARY\nSpark automatically creates closures \n  * for functions that run on RDDs at workers,\n  * and for any global variables that are used by those workers\n  * one closure per worker is sent with every task\n  * and there's no communication between workers\n  * closures are one way from the driver to the worker\n  * any changes that you make to the global variables at the workers \n    * are not sent to the driver or\n    * are not sent to other workers.\n  \n    \n The problem we have is that these closures\n   * are automatically created are sent or re-sent with every job\n   * with a large global variable it gets inefficient to send/resend lots of data to each worker\n   * we cannot communicate that back to the driver\n  \n  \n To do this, Spark provides shared variables in two different types.\n  * **broadcast variables**\n    * lets us to efficiently send large read-only values to all of the workers\n    * these are saved at the workers for use in one or more Spark operations.    \n  * **accumulator variables**\n    * These allow us to aggregate values from workers back to the driver.\n    * only the driver can access the value of the accumulator \n    * for the tasks, the accumulators are basically write-only\n    \n ***\n \n ### 12. Spark Essentials: Summary\n **(watch now: 0:29)**\n \n[![Spark Essentials Summary by Anthony Joseph in BerkeleyX/CS100.1x](http://img.youtube.com/vi/F50Vty9Ia8Y/0.jpg)](https://www.youtube.com/watch?v=F50Vty9Ia8Y?rel=0&autoplay=1&modestbranding=1)\n\n*NOTE:* In databricks cluster, we (the course coordinator/administrators) set the number of workers for you.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c25f5c96-5ff5-49dd-ab3a-3884c8eb5f2a"},{"version":"CommandV1","origId":181720,"guid":"3e939519-2463-4212-b5d0-05490594dfd1","subtype":"command","commandType":"auto","position":17.0625,"command":"%md\n### 13. HOMEWORK \nSee the notebook in this folder named `005_RDDsTransformationsActionsHOMEWORK`. \nThis notebook will give you more examples of the operations above as well as others we will be using later, including:\n* Perform the ``takeOrdered`` action on the RDD\n* Transform the RDD by ``distinct`` to make another RDD and\n* Doing a bunch of transformations to our RDD and performing an action in a single cell.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867880,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"90968368-8574-4c91-ad55-6ec07ac79b30"},{"version":"CommandV1","origId":181721,"guid":"bead8544-e6da-4f32-8764-a90eb63325c9","subtype":"command","commandType":"auto","position":18.0,"command":"%md\n***\n***\n### **Importing Standard Scala and Java libraries**\n* For other libraries that are not available by default, you can upload other libraries to the Workspace.\n* Refer to the **[Libraries](https://docs.databricks.com/user-guide/libraries.html)** guide for more details.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1456369867921,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"856edb0d-6597-4cd8-a2d7-ed3f441c4b1e"},{"version":"CommandV1","origId":181722,"guid":"2a249c71-d387-43e8-91c5-967093e3340e","subtype":"command","commandType":"auto","position":18.5,"command":"import scala.math._\nval x = min(1, 10)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import scala.math._\nx: Int = 1\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456369886870,"submitTime":1456369867975,"finishTime":1456369886938,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e1b4f945-6c24-4ddb-9358-9badf0006cc2"},{"version":"CommandV1","origId":181723,"guid":"13ece452-781f-4103-b156-12be98731427","subtype":"command","commandType":"auto","position":18.75,"command":"import java.util.HashMap\nval map = new HashMap[String, Int]()\nmap.put(\"a\", 1)\nmap.put(\"b\", 2)\nmap.put(\"c\", 3)\nmap.put(\"d\", 4)\nmap.put(\"e\", 5)\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.util.HashMap\nmap: java.util.HashMap[String,Int] = {a=1, b=2, c=3, d=4, e=5}\nres9: Int = 0\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1456369886943,"submitTime":1456369867998,"finishTime":1456369887167,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ba5cbc31-7d4f-4447-b536-05b04070e887"}],"dashboards":[],"guid":"84199bdc-dd1f-4e82-bc28-3c1a674d1e65","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
