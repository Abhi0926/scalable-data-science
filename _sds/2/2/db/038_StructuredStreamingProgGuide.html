<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>038_StructuredStreamingProgGuide - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/source_code_pro.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":true,"defaultInteractivePricePerDBU":0.4,"enableClusterMetricsUI":true,"useReactTableCreateView":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableReservoirTableUI":false,"enableClearStateFeature":true,"enableJobsAclsV2InUI":true,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"enableAttachExistingCluster":true,"resetJobListOnConnect":true,"serverlessDefaultSparkVersion":"latest-stable-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"class-node","description":"Class Node","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"r3.xlarge","provider":"AWS","local_disk_size_gb":80,"compute_units":13.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":31232,"num_cores":4,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":31232,"is_hidden":false,"category":"Memory Optimized","num_cores":4.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":62464,"is_hidden":false,"category":"Memory Optimized","num_cores":8.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"r3.4xlarge","provider":"AWS","local_disk_size_gb":320,"compute_units":52.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":124928,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":124928,"is_hidden":false,"category":"Memory Optimized","num_cores":16.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"r3.8xlarge","provider":"AWS","local_disk_size_gb":320,"compute_units":104.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":249856,"num_cores":32,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":249856,"is_hidden":false,"category":"Memory Optimized","num_cores":32.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":8079,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge","support_cluster_tags":true,"container_memory_mb":10099,"node_instance_type":{"instance_type_id":"c3.2xlarge","provider":"AWS","local_disk_size_gb":80,"compute_units":28.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":15360,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":15360,"is_hidden":false,"category":"Compute Optimized","num_cores":8.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":19998,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge","support_cluster_tags":true,"container_memory_mb":24998,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":55.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":30720,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":30720,"is_hidden":false,"category":"Compute Optimized","num_cores":16.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":43837,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge","support_cluster_tags":true,"container_memory_mb":54796,"node_instance_type":{"instance_type_id":"c3.8xlarge","provider":"AWS","local_disk_size_gb":320,"compute_units":108.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":61440,"num_cores":32,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":61440,"is_hidden":false,"category":"Compute Optimized","num_cores":32.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"i2.xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":14.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":31232,"num_cores":4,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":31232,"is_hidden":false,"category":"Storage Optimized","num_cores":4.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"i2.2xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":27.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":62464,"is_hidden":false,"category":"Storage Optimized","num_cores":8.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"i2.4xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":53.0,"number_of_ips":4,"local_disks":4,"reserved_compute_units":3.64,"gpus":0,"memory_mb":124928,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":124928,"is_hidden":false,"category":"Storage Optimized","num_cores":16.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"display_order":1,"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"i2.8xlarge","provider":"AWS","local_disk_size_gb":800,"compute_units":104.0,"number_of_ips":4,"local_disks":8,"reserved_compute_units":3.64,"gpus":0,"memory_mb":249856,"num_cores":32,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":249856,"is_hidden":false,"category":"Storage Optimized","num_cores":32.0,"support_port_forwarding":true,"support_ebs_volumes":true,"is_deprecated":false},{"support_ssh":false,"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":28000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":4,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":30720,"is_hidden":true,"category":"Memory Optimized","num_cores":4.0,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":true},{"support_ssh":false,"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":12128,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":55.0,"number_of_ips":4,"local_disks":2,"reserved_compute_units":3.64,"gpus":0,"memory_mb":30720,"num_cores":16,"local_disk_type":"AHCI","max_attachable_disks":10,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":15360,"is_hidden":true,"category":"Compute Optimized","num_cores":8.0,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":true}],"default_node_type_id":"class-node"},"sqlAclsDisabledMap":{"spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"serverlessClusterProductName":"Serverless Pool","showS3TableImportOption":true,"maxEbsVolumesPerInstance":10,"isAdmin":false,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"sqlAclsEnabledMap":{"spark.databricks.acl.enabled":"true","spark.databricks.acl.sqlOnly":"true"},"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2a","isDefault":true},{"id":"us-west-2c","isDefault":false},{"id":"us-west-2b","isDefault":false}],"enableCustomSpotPricingUIByTier":true,"serverlessClustersEnabled":true,"enableFindAndReplace":true,"disallowUrlImportExceptFromDocs":false,"defaultStandardClusterModel":{"cluster_name":"","node_type_id":"class-node","spark_version":"3.3.x-scala2.11","num_workers":8,"autoscale":null,"aws_attributes":{"availability":"SPOT_WITH_FALLBACK","instance_profile_arn":null,"first_on_demand":5.0,"spot_bid_price_percent":100,"zone_id":"us-west-2a"},"autotermination_minutes":120,"default_tags":{"Vendor":"Databricks","Creator":"r.sainudiin@math.canterbury.ac.nz","ClusterName":null,"ClusterId":"<Generated after creation>"}},"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableBitbucketCloud":true,"createTableInNotebookS3Link":{"url":"https://docs.databricks.com/_static/notebooks/data-import/s3.html","displayName":"S3","workspaceFileName":"S3 Example"},"sanitizeHtmlResult":false,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"allowRunOnPendingClusters":true,"applications":false,"useAutoscalingByDefault":false,"enableAzureToolbar":false,"fileStoreBase":"FileStore","enableEmailInAzure":false,"enableRLibraries":true,"enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":true,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"checkBeforeAddingAadUser":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"createTableInNotebookDBFSLink":{"url":"https://docs.databricks.com/_static/notebooks/data-import/dbfs.html","displayName":"DBFS","workspaceFileName":"DBFS Example"},"perClusterAutoterminationEnabled":true,"enableNotebookCommandNumbers":true,"allowStyleInSanitizedHtml":false,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"3.3.x-gpu-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-280a8d41cd338f5b48d43eb87622c542c6e6584c430f6d3afe8f3401b9607cb9","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.3.x-scala2.10","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-dd410c68e21c3c563ad6128d35705b605d70530124d55aff1dd12d7e15adfa20","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1, deprecated)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.2.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.1-db6-scala2.10","displayName":"Spark 2.1.1-db6 (Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-d613235f93e0f29838beb2079a958c02a192ed67a502192bc67a8a5f2fb37f35","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"latest-stable-gpu-scala2.11","displayName":"Latest stable (3.4, GPU, Scala 2.11)","packageLabel":"spark-image-40f6dfb2a2502164930a89baac5ceba51d0af3142c7a13d8e9ead8c0cf100efe","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.4.x-scala2.11","displayName":"3.4 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-ecd6c5107c6be684ff96a08dfcc195a1eb89d5a3a3048cdb40fbf52fc1d75ddc","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.2.x-scala2.10","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-557788bea0eea16bbf7a8ba13ace07e64dd7fc86270bd5cea086097fe886431f","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"latest-experimental-scala2.10","displayName":"Latest experimental (3.4 snapshot, Scala 2.10)","packageLabel":"spark-image-a03f42d1c365945a4971bcc14bd8e5e8a35f2e63536ac976a1bbda5eff26cc0e","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-rc-scala2.11","displayName":"Latest RC (3.5, Scala 2.11)","packageLabel":"spark-image-2eb3fab6c8e2fc069042655577a5908c21ecf8a2e320d84a91db1bbeda0cd9aa","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-stable-scala2.11","displayName":"Latest stable (3.4, Scala 2.11)","packageLabel":"spark-image-ecd6c5107c6be684ff96a08dfcc195a1eb89d5a3a3048cdb40fbf52fc1d75ddc","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"next-major-version-scala2.11","displayName":"Next major version (4.0 snapshot, Scala 2.11)","packageLabel":"spark-image-fc43b079519476309b541b56ae62e32b921b0516255717acd288b01afb81e614","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-rc-scala2.10","displayName":"Latest RC (3.5, Scala 2.10)","packageLabel":"spark-image-d43417be4e158a82997ad307ae9958b7334d4f3805d53f279b3dbd9f5639b0e5","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-stable-scala2.10","displayName":"Latest stable (3.4, Scala 2.10)","packageLabel":"spark-image-8e4c9eb80690e8553eadff617739e4125264e7fc9ce3bad494f7d19627557a6c","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.4.x-gpu-scala2.11","displayName":"3.4 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-40f6dfb2a2502164930a89baac5ceba51d0af3142c7a13d8e9ead8c0cf100efe","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1, deprecated)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"latest-experimental-gpu-scala2.11","displayName":"Latest experimental (3.4 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-cf686fcc077800f7b56cb47aa5c0590e3dd7d06b34d3fa9c5c5db2a558ae49b0","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.2.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.1.x-scala2.11","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-241fa8b78ee6343242b1756b18076270894385ff40a81172a6fb5eadf66155d3","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"3.1.x-scala2.10","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-7efac6b9a8f2da59cb4f6d0caac46cfcb3f1ebf64c8073498c42d0360f846714","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.3.x-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-73a161da0570b3f51c8eb238602af2f5561789ea80b25c69a48691fc84e2d974","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"next-major-version-gpu-scala2.11","displayName":"Next major version (4.0 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-546f9fbee419ad335394ca82fe332220e618437676903d38e5c06b28207261fa","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1, deprecated)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.1.1-db6-scala2.11","displayName":"Spark 2.1.1-db6 (Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"latest-experimental-scala2.11","displayName":"Latest experimental (3.4 snapshot, Scala 2.11)","packageLabel":"spark-image-3d803db21db15e7fc8a5b7016b9a5d9fcbe7575646470943e2d7312f98ea705a","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-5537926238bc55cb6cd76ee0f0789511349abead3781c4780721a845f34b5d4e","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"3.0.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-rc-gpu-scala2.11","displayName":"Latest RC (3.5 GPU, Scala 2.11)","packageLabel":"spark-image-46098148db8969467d6a011cb450e67f9b69d0f39f5450c7b36af4999f52fee2","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.4.x-scala2.10","displayName":"3.4 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-8e4c9eb80690e8553eadff617739e4125264e7fc9ce3bad494f7d19627557a6c","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]}],"enablePresentationMode":false,"enableClearStateAndRunAll":true,"enableRestrictedClusterCreation":false,"enableFeedback":false,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"jobsUnreachableThresholdMillis":60000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"createTableInNotebookImportedFileLink":{"url":"https://docs.databricks.com/_static/notebooks/data-import/imported-file.html","displayName":"Imported File","workspaceFileName":"Imported File Example"},"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","showDbuPricing":true,"databricksDocsBaseHostname":"docs.databricks.com","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":4,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"Standard_DS15_v2":8,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"Standard_DS14_v2":4,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":8,"i3.16xlarge":16,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"Standard_L4s":1.5,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"tableFilesBaseFolder":"/tables","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"homePageWelcomeMessage":"Welcome to ","metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.58.797","accountsLimit":-1,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"useAADLogin":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"showReleaseNote":true,"displayDefaultContainerMemoryGB":6,"broadenedEditPermission":false,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"removePasswordInAccountSettings":false,"preferStartTerminatedCluster":false,"enableUserInviteWorkflow":true,"createTableConnectorOptionLinks":[{"url":"https://docs.databricks.com/_static/notebooks/redshift.html","displayName":"Amazon Redshift","workspaceFileName":"Amazon Redshift Example"},{"url":"https://docs.databricks.com/_static/notebooks/structured-streaming-kinesis.html","displayName":"Amazon Kinesis","workspaceFileName":"Amazon Kinesis Example"},{"url":"https://docs.databricks.com/_static/notebooks/cassandra.html","displayName":"Cassandra","workspaceFileName":"Cassandra Example"},{"url":"https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html","displayName":"Kafka","workspaceFileName":"Kafka Example"},{"url":"https://docs.databricks.com/_static/notebooks/redis.html","displayName":"Redis","workspaceFileName":"Redis Example"},{"url":"https://docs.databricks.com/_static/notebooks/elasticsearch.html","displayName":"Elasticsearch","workspaceFileName":"Elasticsearch Example"}],"enableStaticNotebooks":true,"sandboxForUrlSandboxFrame":"allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms","enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterEdit":true,"enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":true,"enableCreateClusterOnAttach":false,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"defaultMinWorkers":2,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"defaultMaxWorkers":8,"enableWorkspaceAclsConfig":true,"serverlessRunPythonAsLowPrivilegeUser":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":true,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"3.3.x-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-73a161da0570b3f51c8eb238602af2f5561789ea80b25c69a48691fc84e2d974","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"defaultAutoterminationMin":120,"useDevTierHomePage":false,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":true,"defaultTagKeys":{"CLUSTER_NAME":"ClusterName","VENDOR":"Vendor","CLUSTER_TYPE":"ResourceClass","CREATOR":"Creator","CLUSTER_ID":"ClusterId"},"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","azurePortalLink":"https://portal.azure.com","cloud":"AWS","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","defaultServerlessClusterModel":{"cluster_name":"","node_type_id":"i3.2xlarge","spark_version":"latest-stable-scala2.11","num_workers":null,"enable_jdbc_auto_start":true,"custom_tags":{"ResourceClass":"Serverless"},"autoscale":{"min_workers":2,"max_workers":20},"spark_conf":{"spark.databricks.cluster.profile":"serverless","spark.databricks.repl.allowedLanguages":"sql,python","spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"aws_attributes":{"ebs_volume_count":null,"availability":"SPOT_WITH_FALLBACK","first_on_demand":1,"ebs_volume_type":null,"spot_bid_price_percent":100,"zone_id":"us-west-2a","ebs_volume_size":null},"autotermination_minutes":0,"enable_elastic_disk":true,"default_tags":{"Vendor":"Databricks","Creator":"r.sainudiin@math.canterbury.ac.nz","ClusterName":null,"ClusterId":"<Generated after creation>"}},"enableOrgSwitcherUI":false,"bitbucketCloudBaseApiV2Url":"https://api.bitbucket.org/2.0","clustersLimit":-1,"enableJdbcImport":true,"enableElasticDisk":true,"logfiles":"logfiles/","enableRelativeNotebookLinks":true,"enableMultiSelect":true,"homePageLogo":"login/databricks_logoTM_rgb_TM.svg","enableWebappSharding":false,"enableNotebookParamsEdit":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"separateTableForJobClusters":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableStructuredDataAcls":false,"showVersion":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"94fde6d5b2d83b50ef111c76666a4bab6edee612","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","databricksDocsNotebookPathPrefix":"^https://docs\\.databricks\\.com/_static/notebooks/.+$","serverlessAttachEbsVolumesByDefault":false,"enableTokensConfig":false,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"allowDisplayHtmlByUrl":false,"enableTokens":true,"enableMiniClusters":false,"enableNewJobList":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"loginLogo":"/login/databricks_logoTM_rgb_TM.svg","useStandardTierUpgradeTooltips":false,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"checkAadUserInWorkspaceTenant":false,"dynamicSparkVersions":true,"useIframeForHtmlResult":false,"enableClusterTagsUIByTier":true,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":true,"maxImportFileVersion":5,"enableEBSVolumesUIByTier":true,"singleSignOnComingSoon":false,"removeSubCommandCodeWhenExport":true,"upgradeURL":"","maxAutoterminationMinutes":10000,"showResultsFromExternalSearchEngine":true,"autoterminateClustersByDefault":false,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":false,"useOnDemandClustersByDefault":false,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":204185,"name":"038_StructuredStreamingProgGuide","language":"scala","commands":[{"version":"CommandV1","origId":207880,"guid":"476a91b8-9daf-4f50-a6b2-275b48024f27","subtype":"command","commandType":"auto","position":0.5,"command":"%md\n\n# [SDS-2.2, Scalable Data Science](https://lamastex.github.io/scalable-data-science/sds/2/2/)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f8a50a94-b49b-4a36-8102-d43c90cfcd94"},{"version":"CommandV1","origId":204186,"guid":"6c076747-5117-42b4-b631-6694add00862","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n# Structured Streaming - A Programming Guide Walkthrough\n\n-   [Overview](https://spark.apache.org/docs/2.2.0/index.html)\n-   [Programming Guides****](structured-streaming-programming-guide.html#)\n   \n\n-   [Overview](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#overview)\n-   [Quick\n    Example](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#quick-example)\n-   [Programming\n    Model](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#programming-model)\n    -   [Basic\n        Concepts](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#basic-concepts)\n    -   [Handling Event-time and Late\n        Data](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#handling-event-time-and-late-data)\n    -   [Fault Tolerance\n        Semantics](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#fault-tolerance-semantics)\n-   [API using Datasets and\n    DataFrames](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)\n    -   [Creating streaming DataFrames and streaming\n        Datasets](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets)\n        -   [Input\n            Sources](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#input-sources)\n        -   [Schema inference and partition of streaming\n            DataFrames/Datasets](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#schema-inference-and-partition-of-streaming-dataframesdatasets)\n    -   [Operations on streaming\n        DataFrames/Datasets](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#operations-on-streaming-dataframesdatasets)\n        -   [Basic Operations - Selection, Projection,\n            Aggregation](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#basic-operations---selection-projection-aggregation)\n        -   [Window Operations on Event\n            Time](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#window-operations-on-event-time)\n        -   [Handling Late Data and\n            Watermarking](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)\n        -   [Join\n            Operations](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#join-operations)\n        -   [Streaming\n            Deduplication](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#streaming-deduplication)\n        -   [Arbitrary Stateful\n            Operations](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#arbitrary-stateful-operations)\n        -   [Unsupported\n            Operations](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#unsupported-operations)\n    -   [Starting Streaming\n        Queries](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#starting-streaming-queries)\n        -   [Output\n            Modes](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes)\n        -   [Output\n            Sinks](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-sinks)\n        -   [Using\n            Foreach](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#using-foreach)\n    -   [Managing Streaming\n        Queries](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#managing-streaming-queries)\n    -   [Monitoring Streaming\n        Queries](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#monitoring-streaming-queries)\n        -   [Interactive\n            APIs](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#interactive-apis)\n        -   [Asynchronous\n            API](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#asynchronous-api)\n    -   [Recovering from Failures with\n        Checkpointing](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing)\n-   [Where to go from\n    here](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#where-to-go-from-here)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2873d2d6-e9b3-49dc-bdce-2fb555a419f6"},{"version":"CommandV1","origId":204188,"guid":"206b9f2c-bb43-4d2a-be12-68854454dd21","subtype":"command","commandType":"auto","position":2.0,"command":"%md\nOverview\n========\n\nStructured Streaming is a scalable and fault-tolerant stream processing\nengine built on the Spark SQL engine. You can express your streaming\ncomputation the same way you would express a batch computation on static\ndata. The Spark SQL engine will take care of running it incrementally\nand continuously and updating the final result as streaming data\ncontinues to arrive. You can use the [Dataset/DataFrame\nAPI](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html) in\nScala, Java, Python or R to express streaming aggregations, event-time\nwindows, stream-to-batch joins, etc. The computation is executed on the\nsame optimized Spark SQL engine. Finally, the system ensures end-to-end\nexactly-once fault-tolerance guarantees through checkpointing and Write\nAhead Logs. In short, *Structured Streaming provides fast, scalable,\nfault-tolerant, end-to-end exactly-once stream processing without the\nuser having to reason about streaming.*\n\nIn this guide, we are going to walk you through the programming model\nand the APIs. First, let’s start with a simple example - a streaming\nword count.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bb1267cc-19d3-4553-90cc-fd07f57b3122"},{"version":"CommandV1","origId":207888,"guid":"a709f25c-0d12-4fe8-bbac-b284e1182d2b","subtype":"command","commandType":"auto","position":2.5,"command":"%md\nProgramming Model\n=================\n\nThe key idea in Structured Streaming is to treat a live data stream as a\ntable that is being continuously appended. This leads to a new stream\nprocessing model that is very similar to a batch processing model. You\nwill express your streaming computation as standard batch-like query as\non a static table, and Spark runs it as an *incremental* query on the\n*unbounded* input table. Let’s understand this model in more detail.\n\nBasic Concepts\n--------------\n\nConsider the input data stream as the “Input Table”. Every data item\nthat is arriving on the stream is like a new row being appended to the\nInput Table.\n\n![Stream as a\nTable](https://spark.apache.org/docs/2.2.0/img/structured-streaming-stream-as-a-table.png \"Stream as a Table\")\n\nA query on the input will generate the “Result Table”. Every trigger\ninterval (say, every 1 second), new rows get appended to the Input\nTable, which eventually updates the Result Table. Whenever the result\ntable gets updated, we would want to write the changed result rows to an\nexternal sink.\n\n![Model](https://spark.apache.org/docs/2.2.0/img/structured-streaming-model.png)\n\nThe “Output” is defined as what gets written out to the external\nstorage. The output can be defined in a different mode:\n\n-   *Complete Mode* - The entire updated Result Table will be written to\n    the external storage. It is up to the storage connector to decide\n    how to handle writing of the entire table.\n\n-   *Append Mode* - Only the new rows appended in the Result Table since\n    the last trigger will be written to the external storage. This is\n    applicable only on the queries where existing rows in the Result\n    Table are not expected to change.\n\n-   *Update Mode* - Only the rows that were updated in the Result Table\n    since the last trigger will be written to the external storage\n    (available since Spark 2.1.1). Note that this is different from the\n    Complete Mode in that this mode only outputs the rows that have\n    changed since the last trigger. If the query doesn’t contain\n    aggregations, it will be equivalent to Append mode.\n\nNote that each mode is applicable on certain types of queries. This is\ndiscussed in detail later on *output-modes*.\nTo illustrate the use of this model, let’s understand the model in\ncontext of the **Quick Example** above. \n\nThe first `streamingLines` DataFrame is the input table, and the final\n`wordCounts` DataFrame is the result table. Note that the query on\n`streamingLines` DataFrame to generate `wordCounts` is *exactly the\nsame* as it would be a static DataFrame. However, when this query is\nstarted, Spark will continuously check for new data from the directory. \nIf there is new data, Spark will run an “incremental” query\nthat combines the previous running counts with the new data to compute\nupdated counts, as shown below.\n\n![Model](https://spark.apache.org/docs/2.2.0/img/structured-streaming-example-model.png)\n\nThis model is significantly different from many other stream processing\nengines. Many streaming systems require the user to maintain running\naggregations themselves, thus having to reason about fault-tolerance,\nand data consistency (at-least-once, or at-most-once, or exactly-once).\nIn this model, Spark is responsible for updating the Result Table when\nthere is new data, thus relieving the users from reasoning about it. As\nan example, let’s see how this model handles event-time based processing\nand late arriving data.\n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0fd3f862-3f25-498d-b031-0700711a9c70"},{"version":"CommandV1","origId":204189,"guid":"003c9157-f232-4b39-905c-72be00d550b9","subtype":"command","commandType":"auto","position":3.0,"command":"%md\nQuick Example\n=============\n\nLet’s say you want to maintain a running word count of text data\nreceived from a file writer that is writing files into a directory \n`datasets/streamingFiles` in the distributed file  system. Let’s see how you\ncan express this using Structured Streaming. \n\nLet’s walk through the\nexample step-by-step and understand how it works. \n\n**First** we need to start a file writing job in the companion notebook `037a_AnimalNamesStructStreamingFiles` and then return here.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"45ecdf95-a707-4b3b-a4ea-b36921bd455d"},{"version":"CommandV1","origId":204270,"guid":"e58da02a-13dc-4447-b10d-8ff22286b654","subtype":"command","commandType":"auto","position":3.25,"command":"display(dbutils.fs.ls(\"/datasets/streamingFiles\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/datasets/streamingFiles/00_00.log","00_00.log",35],["dbfs:/datasets/streamingFiles/00_02.log","00_02.log",35],["dbfs:/datasets/streamingFiles/00_04.log","00_04.log",35],["dbfs:/datasets/streamingFiles/00_06.log","00_06.log",35],["dbfs:/datasets/streamingFiles/00_08.log","00_08.log",35],["dbfs:/datasets/streamingFiles/00_10.log","00_10.log",35],["dbfs:/datasets/streamingFiles/00_12.log","00_12.log",35],["dbfs:/datasets/streamingFiles/00_14.log","00_14.log",35],["dbfs:/datasets/streamingFiles/00_16.log","00_16.log",35],["dbfs:/datasets/streamingFiles/00_18.log","00_18.log",35],["dbfs:/datasets/streamingFiles/00_20.log","00_20.log",35],["dbfs:/datasets/streamingFiles/00_22.log","00_22.log",35],["dbfs:/datasets/streamingFiles/00_24.log","00_24.log",35],["dbfs:/datasets/streamingFiles/00_26.log","00_26.log",35],["dbfs:/datasets/streamingFiles/00_28.log","00_28.log",35],["dbfs:/datasets/streamingFiles/00_30.log","00_30.log",35],["dbfs:/datasets/streamingFiles/00_32.log","00_32.log",35],["dbfs:/datasets/streamingFiles/00_34.log","00_34.log",35],["dbfs:/datasets/streamingFiles/00_37.log","00_37.log",35],["dbfs:/datasets/streamingFiles/00_39.log","00_39.log",35],["dbfs:/datasets/streamingFiles/00_41.log","00_41.log",35],["dbfs:/datasets/streamingFiles/00_43.log","00_43.log",35],["dbfs:/datasets/streamingFiles/00_45.log","00_45.log",35],["dbfs:/datasets/streamingFiles/00_47.log","00_47.log",35],["dbfs:/datasets/streamingFiles/00_49.log","00_49.log",35],["dbfs:/datasets/streamingFiles/00_51.log","00_51.log",35],["dbfs:/datasets/streamingFiles/00_53.log","00_53.log",35],["dbfs:/datasets/streamingFiles/00_55.log","00_55.log",35],["dbfs:/datasets/streamingFiles/00_57.log","00_57.log",35],["dbfs:/datasets/streamingFiles/00_59.log","00_59.log",35],["dbfs:/datasets/streamingFiles/01_01.log","01_01.log",35],["dbfs:/datasets/streamingFiles/01_03.log","01_03.log",35],["dbfs:/datasets/streamingFiles/01_05.log","01_05.log",35],["dbfs:/datasets/streamingFiles/01_07.log","01_07.log",35],["dbfs:/datasets/streamingFiles/01_09.log","01_09.log",35],["dbfs:/datasets/streamingFiles/01_11.log","01_11.log",35],["dbfs:/datasets/streamingFiles/01_13.log","01_13.log",35],["dbfs:/datasets/streamingFiles/01_15.log","01_15.log",35],["dbfs:/datasets/streamingFiles/01_17.log","01_17.log",35],["dbfs:/datasets/streamingFiles/01_19.log","01_19.log",35],["dbfs:/datasets/streamingFiles/01_21.log","01_21.log",35],["dbfs:/datasets/streamingFiles/01_23.log","01_23.log",35],["dbfs:/datasets/streamingFiles/01_25.log","01_25.log",35],["dbfs:/datasets/streamingFiles/01_27.log","01_27.log",35],["dbfs:/datasets/streamingFiles/01_29.log","01_29.log",35],["dbfs:/datasets/streamingFiles/01_31.log","01_31.log",35],["dbfs:/datasets/streamingFiles/01_33.log","01_33.log",35],["dbfs:/datasets/streamingFiles/01_35.log","01_35.log",35],["dbfs:/datasets/streamingFiles/01_37.log","01_37.log",35],["dbfs:/datasets/streamingFiles/01_39.log","01_39.log",35],["dbfs:/datasets/streamingFiles/01_41.log","01_41.log",35],["dbfs:/datasets/streamingFiles/01_43.log","01_43.log",35],["dbfs:/datasets/streamingFiles/01_45.log","01_45.log",35],["dbfs:/datasets/streamingFiles/01_47.log","01_47.log",35],["dbfs:/datasets/streamingFiles/01_49.log","01_49.log",35],["dbfs:/datasets/streamingFiles/01_51.log","01_51.log",35],["dbfs:/datasets/streamingFiles/01_53.log","01_53.log",35],["dbfs:/datasets/streamingFiles/01_55.log","01_55.log",35],["dbfs:/datasets/streamingFiles/01_57.log","01_57.log",35],["dbfs:/datasets/streamingFiles/01_59.log","01_59.log",35],["dbfs:/datasets/streamingFiles/02_01.log","02_01.log",35],["dbfs:/datasets/streamingFiles/02_03.log","02_03.log",35],["dbfs:/datasets/streamingFiles/02_05.log","02_05.log",35],["dbfs:/datasets/streamingFiles/02_07.log","02_07.log",35],["dbfs:/datasets/streamingFiles/02_09.log","02_09.log",35],["dbfs:/datasets/streamingFiles/02_11.log","02_11.log",35],["dbfs:/datasets/streamingFiles/02_13.log","02_13.log",35],["dbfs:/datasets/streamingFiles/02_15.log","02_15.log",35],["dbfs:/datasets/streamingFiles/02_17.log","02_17.log",35],["dbfs:/datasets/streamingFiles/02_19.log","02_19.log",35],["dbfs:/datasets/streamingFiles/02_21.log","02_21.log",35],["dbfs:/datasets/streamingFiles/02_23.log","02_23.log",35],["dbfs:/datasets/streamingFiles/02_25.log","02_25.log",35],["dbfs:/datasets/streamingFiles/02_27.log","02_27.log",35],["dbfs:/datasets/streamingFiles/02_29.log","02_29.log",35],["dbfs:/datasets/streamingFiles/02_31.log","02_31.log",35],["dbfs:/datasets/streamingFiles/02_33.log","02_33.log",35],["dbfs:/datasets/streamingFiles/02_35.log","02_35.log",35],["dbfs:/datasets/streamingFiles/02_37.log","02_37.log",35],["dbfs:/datasets/streamingFiles/02_39.log","02_39.log",35],["dbfs:/datasets/streamingFiles/02_41.log","02_41.log",35],["dbfs:/datasets/streamingFiles/02_43.log","02_43.log",35],["dbfs:/datasets/streamingFiles/02_45.log","02_45.log",35],["dbfs:/datasets/streamingFiles/02_47.log","02_47.log",35],["dbfs:/datasets/streamingFiles/02_49.log","02_49.log",35],["dbfs:/datasets/streamingFiles/02_51.log","02_51.log",35],["dbfs:/datasets/streamingFiles/02_53.log","02_53.log",35],["dbfs:/datasets/streamingFiles/02_55.log","02_55.log",35],["dbfs:/datasets/streamingFiles/02_57.log","02_57.log",35],["dbfs:/datasets/streamingFiles/02_59.log","02_59.log",35],["dbfs:/datasets/streamingFiles/03_01.log","03_01.log",35],["dbfs:/datasets/streamingFiles/03_03.log","03_03.log",35],["dbfs:/datasets/streamingFiles/03_06.log","03_06.log",35],["dbfs:/datasets/streamingFiles/03_08.log","03_08.log",35],["dbfs:/datasets/streamingFiles/03_10.log","03_10.log",35],["dbfs:/datasets/streamingFiles/03_12.log","03_12.log",35],["dbfs:/datasets/streamingFiles/03_14.log","03_14.log",35],["dbfs:/datasets/streamingFiles/03_16.log","03_16.log",35],["dbfs:/datasets/streamingFiles/03_18.log","03_18.log",35],["dbfs:/datasets/streamingFiles/03_20.log","03_20.log",35],["dbfs:/datasets/streamingFiles/03_22.log","03_22.log",35],["dbfs:/datasets/streamingFiles/03_24.log","03_24.log",35],["dbfs:/datasets/streamingFiles/03_26.log","03_26.log",35],["dbfs:/datasets/streamingFiles/03_28.log","03_28.log",35],["dbfs:/datasets/streamingFiles/03_30.log","03_30.log",35],["dbfs:/datasets/streamingFiles/03_32.log","03_32.log",35],["dbfs:/datasets/streamingFiles/03_34.log","03_34.log",35],["dbfs:/datasets/streamingFiles/03_36.log","03_36.log",35],["dbfs:/datasets/streamingFiles/03_38.log","03_38.log",35],["dbfs:/datasets/streamingFiles/03_40.log","03_40.log",35],["dbfs:/datasets/streamingFiles/03_42.log","03_42.log",35],["dbfs:/datasets/streamingFiles/03_44.log","03_44.log",35],["dbfs:/datasets/streamingFiles/03_46.log","03_46.log",35],["dbfs:/datasets/streamingFiles/03_48.log","03_48.log",35],["dbfs:/datasets/streamingFiles/03_50.log","03_50.log",35],["dbfs:/datasets/streamingFiles/03_52.log","03_52.log",35],["dbfs:/datasets/streamingFiles/03_54.log","03_54.log",35],["dbfs:/datasets/streamingFiles/03_56.log","03_56.log",35],["dbfs:/datasets/streamingFiles/03_58.log","03_58.log",35],["dbfs:/datasets/streamingFiles/04_00.log","04_00.log",35],["dbfs:/datasets/streamingFiles/04_02.log","04_02.log",35],["dbfs:/datasets/streamingFiles/04_04.log","04_04.log",35],["dbfs:/datasets/streamingFiles/04_06.log","04_06.log",35],["dbfs:/datasets/streamingFiles/04_08.log","04_08.log",35],["dbfs:/datasets/streamingFiles/04_10.log","04_10.log",35],["dbfs:/datasets/streamingFiles/04_12.log","04_12.log",35],["dbfs:/datasets/streamingFiles/04_14.log","04_14.log",35],["dbfs:/datasets/streamingFiles/04_16.log","04_16.log",35],["dbfs:/datasets/streamingFiles/04_18.log","04_18.log",35],["dbfs:/datasets/streamingFiles/04_20.log","04_20.log",35],["dbfs:/datasets/streamingFiles/04_22.log","04_22.log",35],["dbfs:/datasets/streamingFiles/04_24.log","04_24.log",35],["dbfs:/datasets/streamingFiles/04_26.log","04_26.log",35],["dbfs:/datasets/streamingFiles/04_28.log","04_28.log",35],["dbfs:/datasets/streamingFiles/04_30.log","04_30.log",35],["dbfs:/datasets/streamingFiles/04_32.log","04_32.log",35],["dbfs:/datasets/streamingFiles/04_34.log","04_34.log",35],["dbfs:/datasets/streamingFiles/04_36.log","04_36.log",35],["dbfs:/datasets/streamingFiles/04_38.log","04_38.log",35],["dbfs:/datasets/streamingFiles/04_40.log","04_40.log",35],["dbfs:/datasets/streamingFiles/04_42.log","04_42.log",35],["dbfs:/datasets/streamingFiles/04_44.log","04_44.log",35],["dbfs:/datasets/streamingFiles/04_46.log","04_46.log",35],["dbfs:/datasets/streamingFiles/04_48.log","04_48.log",35],["dbfs:/datasets/streamingFiles/04_50.log","04_50.log",35],["dbfs:/datasets/streamingFiles/04_52.log","04_52.log",35],["dbfs:/datasets/streamingFiles/04_54.log","04_54.log",35],["dbfs:/datasets/streamingFiles/04_56.log","04_56.log",35],["dbfs:/datasets/streamingFiles/04_58.log","04_58.log",35],["dbfs:/datasets/streamingFiles/05_00.log","05_00.log",35],["dbfs:/datasets/streamingFiles/05_02.log","05_02.log",35],["dbfs:/datasets/streamingFiles/05_04.log","05_04.log",35],["dbfs:/datasets/streamingFiles/05_06.log","05_06.log",35],["dbfs:/datasets/streamingFiles/05_08.log","05_08.log",35],["dbfs:/datasets/streamingFiles/05_10.log","05_10.log",35],["dbfs:/datasets/streamingFiles/05_12.log","05_12.log",35],["dbfs:/datasets/streamingFiles/05_14.log","05_14.log",35],["dbfs:/datasets/streamingFiles/05_16.log","05_16.log",35],["dbfs:/datasets/streamingFiles/05_18.log","05_18.log",35],["dbfs:/datasets/streamingFiles/05_20.log","05_20.log",35],["dbfs:/datasets/streamingFiles/05_22.log","05_22.log",35],["dbfs:/datasets/streamingFiles/05_24.log","05_24.log",35],["dbfs:/datasets/streamingFiles/05_26.log","05_26.log",35],["dbfs:/datasets/streamingFiles/05_28.log","05_28.log",35],["dbfs:/datasets/streamingFiles/05_30.log","05_30.log",35],["dbfs:/datasets/streamingFiles/05_32.log","05_32.log",35],["dbfs:/datasets/streamingFiles/05_34.log","05_34.log",35],["dbfs:/datasets/streamingFiles/05_36.log","05_36.log",35],["dbfs:/datasets/streamingFiles/05_38.log","05_38.log",35],["dbfs:/datasets/streamingFiles/05_40.log","05_40.log",35],["dbfs:/datasets/streamingFiles/05_42.log","05_42.log",35],["dbfs:/datasets/streamingFiles/05_44.log","05_44.log",35],["dbfs:/datasets/streamingFiles/05_46.log","05_46.log",35],["dbfs:/datasets/streamingFiles/05_48.log","05_48.log",35],["dbfs:/datasets/streamingFiles/05_50.log","05_50.log",35],["dbfs:/datasets/streamingFiles/05_52.log","05_52.log",35],["dbfs:/datasets/streamingFiles/05_54.log","05_54.log",35],["dbfs:/datasets/streamingFiles/05_56.log","05_56.log",35],["dbfs:/datasets/streamingFiles/05_58.log","05_58.log",35],["dbfs:/datasets/streamingFiles/06_00.log","06_00.log",35],["dbfs:/datasets/streamingFiles/06_02.log","06_02.log",35],["dbfs:/datasets/streamingFiles/06_04.log","06_04.log",35],["dbfs:/datasets/streamingFiles/06_06.log","06_06.log",35],["dbfs:/datasets/streamingFiles/06_08.log","06_08.log",35],["dbfs:/datasets/streamingFiles/06_10.log","06_10.log",35],["dbfs:/datasets/streamingFiles/06_12.log","06_12.log",35],["dbfs:/datasets/streamingFiles/06_15.log","06_15.log",35],["dbfs:/datasets/streamingFiles/06_17.log","06_17.log",35],["dbfs:/datasets/streamingFiles/06_19.log","06_19.log",35],["dbfs:/datasets/streamingFiles/06_21.log","06_21.log",35],["dbfs:/datasets/streamingFiles/06_23.log","06_23.log",35],["dbfs:/datasets/streamingFiles/06_25.log","06_25.log",35],["dbfs:/datasets/streamingFiles/06_27.log","06_27.log",35],["dbfs:/datasets/streamingFiles/06_29.log","06_29.log",35],["dbfs:/datasets/streamingFiles/06_31.log","06_31.log",35],["dbfs:/datasets/streamingFiles/06_33.log","06_33.log",35],["dbfs:/datasets/streamingFiles/06_35.log","06_35.log",35],["dbfs:/datasets/streamingFiles/06_37.log","06_37.log",35],["dbfs:/datasets/streamingFiles/06_39.log","06_39.log",35],["dbfs:/datasets/streamingFiles/06_41.log","06_41.log",35],["dbfs:/datasets/streamingFiles/06_43.log","06_43.log",35],["dbfs:/datasets/streamingFiles/06_45.log","06_45.log",35],["dbfs:/datasets/streamingFiles/06_47.log","06_47.log",35],["dbfs:/datasets/streamingFiles/06_49.log","06_49.log",35],["dbfs:/datasets/streamingFiles/06_51.log","06_51.log",35],["dbfs:/datasets/streamingFiles/06_53.log","06_53.log",35],["dbfs:/datasets/streamingFiles/06_55.log","06_55.log",35],["dbfs:/datasets/streamingFiles/06_57.log","06_57.log",35],["dbfs:/datasets/streamingFiles/06_59.log","06_59.log",35],["dbfs:/datasets/streamingFiles/07_01.log","07_01.log",35],["dbfs:/datasets/streamingFiles/07_03.log","07_03.log",35],["dbfs:/datasets/streamingFiles/07_05.log","07_05.log",35],["dbfs:/datasets/streamingFiles/07_07.log","07_07.log",35],["dbfs:/datasets/streamingFiles/07_09.log","07_09.log",35],["dbfs:/datasets/streamingFiles/07_11.log","07_11.log",35],["dbfs:/datasets/streamingFiles/07_13.log","07_13.log",35],["dbfs:/datasets/streamingFiles/07_15.log","07_15.log",35],["dbfs:/datasets/streamingFiles/07_17.log","07_17.log",35],["dbfs:/datasets/streamingFiles/07_19.log","07_19.log",35],["dbfs:/datasets/streamingFiles/07_21.log","07_21.log",35],["dbfs:/datasets/streamingFiles/07_23.log","07_23.log",35],["dbfs:/datasets/streamingFiles/07_25.log","07_25.log",35],["dbfs:/datasets/streamingFiles/07_27.log","07_27.log",35],["dbfs:/datasets/streamingFiles/07_29.log","07_29.log",35],["dbfs:/datasets/streamingFiles/07_31.log","07_31.log",35],["dbfs:/datasets/streamingFiles/07_33.log","07_33.log",35],["dbfs:/datasets/streamingFiles/07_35.log","07_35.log",35],["dbfs:/datasets/streamingFiles/07_37.log","07_37.log",35],["dbfs:/datasets/streamingFiles/07_39.log","07_39.log",35],["dbfs:/datasets/streamingFiles/07_41.log","07_41.log",35],["dbfs:/datasets/streamingFiles/07_43.log","07_43.log",35],["dbfs:/datasets/streamingFiles/07_45.log","07_45.log",35],["dbfs:/datasets/streamingFiles/07_47.log","07_47.log",35],["dbfs:/datasets/streamingFiles/07_49.log","07_49.log",35],["dbfs:/datasets/streamingFiles/07_51.log","07_51.log",35],["dbfs:/datasets/streamingFiles/07_53.log","07_53.log",35],["dbfs:/datasets/streamingFiles/07_55.log","07_55.log",35],["dbfs:/datasets/streamingFiles/07_57.log","07_57.log",35],["dbfs:/datasets/streamingFiles/07_59.log","07_59.log",35],["dbfs:/datasets/streamingFiles/08_01.log","08_01.log",35],["dbfs:/datasets/streamingFiles/08_03.log","08_03.log",35],["dbfs:/datasets/streamingFiles/08_05.log","08_05.log",35],["dbfs:/datasets/streamingFiles/08_07.log","08_07.log",35],["dbfs:/datasets/streamingFiles/08_09.log","08_09.log",35],["dbfs:/datasets/streamingFiles/08_11.log","08_11.log",35],["dbfs:/datasets/streamingFiles/08_13.log","08_13.log",35],["dbfs:/datasets/streamingFiles/08_15.log","08_15.log",35],["dbfs:/datasets/streamingFiles/08_17.log","08_17.log",35],["dbfs:/datasets/streamingFiles/08_19.log","08_19.log",35],["dbfs:/datasets/streamingFiles/08_21.log","08_21.log",35],["dbfs:/datasets/streamingFiles/08_23.log","08_23.log",35],["dbfs:/datasets/streamingFiles/08_25.log","08_25.log",35],["dbfs:/datasets/streamingFiles/08_27.log","08_27.log",35],["dbfs:/datasets/streamingFiles/08_29.log","08_29.log",35],["dbfs:/datasets/streamingFiles/08_31.log","08_31.log",35],["dbfs:/datasets/streamingFiles/08_33.log","08_33.log",35],["dbfs:/datasets/streamingFiles/08_36.log","08_36.log",35],["dbfs:/datasets/streamingFiles/08_38.log","08_38.log",35],["dbfs:/datasets/streamingFiles/08_40.log","08_40.log",35],["dbfs:/datasets/streamingFiles/08_42.log","08_42.log",35],["dbfs:/datasets/streamingFiles/08_44.log","08_44.log",35],["dbfs:/datasets/streamingFiles/08_46.log","08_46.log",35],["dbfs:/datasets/streamingFiles/08_48.log","08_48.log",35],["dbfs:/datasets/streamingFiles/08_50.log","08_50.log",35],["dbfs:/datasets/streamingFiles/08_52.log","08_52.log",35],["dbfs:/datasets/streamingFiles/08_54.log","08_54.log",35],["dbfs:/datasets/streamingFiles/08_56.log","08_56.log",35],["dbfs:/datasets/streamingFiles/08_58.log","08_58.log",35],["dbfs:/datasets/streamingFiles/09_00.log","09_00.log",35],["dbfs:/datasets/streamingFiles/09_02.log","09_02.log",35],["dbfs:/datasets/streamingFiles/09_04.log","09_04.log",35],["dbfs:/datasets/streamingFiles/09_06.log","09_06.log",35],["dbfs:/datasets/streamingFiles/09_08.log","09_08.log",35],["dbfs:/datasets/streamingFiles/09_10.log","09_10.log",35],["dbfs:/datasets/streamingFiles/09_12.log","09_12.log",35],["dbfs:/datasets/streamingFiles/09_14.log","09_14.log",35],["dbfs:/datasets/streamingFiles/09_16.log","09_16.log",35],["dbfs:/datasets/streamingFiles/09_18.log","09_18.log",35],["dbfs:/datasets/streamingFiles/09_20.log","09_20.log",35],["dbfs:/datasets/streamingFiles/09_22.log","09_22.log",35],["dbfs:/datasets/streamingFiles/09_24.log","09_24.log",35],["dbfs:/datasets/streamingFiles/09_26.log","09_26.log",35],["dbfs:/datasets/streamingFiles/09_28.log","09_28.log",35],["dbfs:/datasets/streamingFiles/09_30.log","09_30.log",35],["dbfs:/datasets/streamingFiles/09_32.log","09_32.log",35],["dbfs:/datasets/streamingFiles/09_34.log","09_34.log",35],["dbfs:/datasets/streamingFiles/09_36.log","09_36.log",35],["dbfs:/datasets/streamingFiles/09_38.log","09_38.log",35],["dbfs:/datasets/streamingFiles/09_40.log","09_40.log",35],["dbfs:/datasets/streamingFiles/09_42.log","09_42.log",35],["dbfs:/datasets/streamingFiles/09_44.log","09_44.log",35],["dbfs:/datasets/streamingFiles/09_46.log","09_46.log",35],["dbfs:/datasets/streamingFiles/09_48.log","09_48.log",35],["dbfs:/datasets/streamingFiles/09_50.log","09_50.log",35],["dbfs:/datasets/streamingFiles/09_52.log","09_52.log",35],["dbfs:/datasets/streamingFiles/09_54.log","09_54.log",35],["dbfs:/datasets/streamingFiles/09_56.log","09_56.log",35],["dbfs:/datasets/streamingFiles/09_58.log","09_58.log",35],["dbfs:/datasets/streamingFiles/10_00.log","10_00.log",35],["dbfs:/datasets/streamingFiles/10_02.log","10_02.log",35],["dbfs:/datasets/streamingFiles/10_04.log","10_04.log",35],["dbfs:/datasets/streamingFiles/10_06.log","10_06.log",35],["dbfs:/datasets/streamingFiles/10_08.log","10_08.log",35],["dbfs:/datasets/streamingFiles/10_10.log","10_10.log",35],["dbfs:/datasets/streamingFiles/10_12.log","10_12.log",35],["dbfs:/datasets/streamingFiles/10_14.log","10_14.log",35],["dbfs:/datasets/streamingFiles/10_16.log","10_16.log",35],["dbfs:/datasets/streamingFiles/10_18.log","10_18.log",35],["dbfs:/datasets/streamingFiles/10_20.log","10_20.log",35],["dbfs:/datasets/streamingFiles/10_22.log","10_22.log",35],["dbfs:/datasets/streamingFiles/10_24.log","10_24.log",35],["dbfs:/datasets/streamingFiles/10_26.log","10_26.log",35],["dbfs:/datasets/streamingFiles/10_28.log","10_28.log",35],["dbfs:/datasets/streamingFiles/10_30.log","10_30.log",35],["dbfs:/datasets/streamingFiles/10_32.log","10_32.log",35],["dbfs:/datasets/streamingFiles/10_34.log","10_34.log",35],["dbfs:/datasets/streamingFiles/10_36.log","10_36.log",35],["dbfs:/datasets/streamingFiles/10_38.log","10_38.log",35],["dbfs:/datasets/streamingFiles/10_40.log","10_40.log",35],["dbfs:/datasets/streamingFiles/10_42.log","10_42.log",35],["dbfs:/datasets/streamingFiles/10_44.log","10_44.log",35],["dbfs:/datasets/streamingFiles/10_46.log","10_46.log",35],["dbfs:/datasets/streamingFiles/10_48.log","10_48.log",35],["dbfs:/datasets/streamingFiles/10_50.log","10_50.log",35],["dbfs:/datasets/streamingFiles/10_52.log","10_52.log",35],["dbfs:/datasets/streamingFiles/10_54.log","10_54.log",35],["dbfs:/datasets/streamingFiles/10_56.log","10_56.log",35],["dbfs:/datasets/streamingFiles/10_58.log","10_58.log",35],["dbfs:/datasets/streamingFiles/11_00.log","11_00.log",35],["dbfs:/datasets/streamingFiles/11_02.log","11_02.log",35],["dbfs:/datasets/streamingFiles/11_04.log","11_04.log",35],["dbfs:/datasets/streamingFiles/11_06.log","11_06.log",35],["dbfs:/datasets/streamingFiles/11_09.log","11_09.log",35],["dbfs:/datasets/streamingFiles/11_11.log","11_11.log",35],["dbfs:/datasets/streamingFiles/11_13.log","11_13.log",35],["dbfs:/datasets/streamingFiles/11_15.log","11_15.log",35],["dbfs:/datasets/streamingFiles/11_17.log","11_17.log",35],["dbfs:/datasets/streamingFiles/11_19.log","11_19.log",35],["dbfs:/datasets/streamingFiles/11_21.log","11_21.log",35],["dbfs:/datasets/streamingFiles/11_23.log","11_23.log",35],["dbfs:/datasets/streamingFiles/11_25.log","11_25.log",35],["dbfs:/datasets/streamingFiles/11_27.log","11_27.log",35],["dbfs:/datasets/streamingFiles/11_29.log","11_29.log",35],["dbfs:/datasets/streamingFiles/11_31.log","11_31.log",35],["dbfs:/datasets/streamingFiles/11_33.log","11_33.log",35],["dbfs:/datasets/streamingFiles/11_35.log","11_35.log",35],["dbfs:/datasets/streamingFiles/11_37.log","11_37.log",35],["dbfs:/datasets/streamingFiles/11_39.log","11_39.log",35],["dbfs:/datasets/streamingFiles/11_41.log","11_41.log",35],["dbfs:/datasets/streamingFiles/11_43.log","11_43.log",35],["dbfs:/datasets/streamingFiles/11_45.log","11_45.log",35],["dbfs:/datasets/streamingFiles/11_47.log","11_47.log",35],["dbfs:/datasets/streamingFiles/11_49.log","11_49.log",35],["dbfs:/datasets/streamingFiles/11_51.log","11_51.log",35],["dbfs:/datasets/streamingFiles/11_53.log","11_53.log",35],["dbfs:/datasets/streamingFiles/11_55.log","11_55.log",35],["dbfs:/datasets/streamingFiles/11_57.log","11_57.log",35],["dbfs:/datasets/streamingFiles/11_59.log","11_59.log",35],["dbfs:/datasets/streamingFiles/12_01.log","12_01.log",35],["dbfs:/datasets/streamingFiles/12_03.log","12_03.log",35],["dbfs:/datasets/streamingFiles/12_05.log","12_05.log",35],["dbfs:/datasets/streamingFiles/12_07.log","12_07.log",35],["dbfs:/datasets/streamingFiles/12_09.log","12_09.log",35],["dbfs:/datasets/streamingFiles/12_11.log","12_11.log",35],["dbfs:/datasets/streamingFiles/12_13.log","12_13.log",35],["dbfs:/datasets/streamingFiles/12_15.log","12_15.log",35],["dbfs:/datasets/streamingFiles/12_17.log","12_17.log",35],["dbfs:/datasets/streamingFiles/12_19.log","12_19.log",35],["dbfs:/datasets/streamingFiles/12_21.log","12_21.log",35],["dbfs:/datasets/streamingFiles/12_23.log","12_23.log",35],["dbfs:/datasets/streamingFiles/12_25.log","12_25.log",35],["dbfs:/datasets/streamingFiles/12_27.log","12_27.log",35],["dbfs:/datasets/streamingFiles/12_29.log","12_29.log",35],["dbfs:/datasets/streamingFiles/12_31.log","12_31.log",35],["dbfs:/datasets/streamingFiles/12_33.log","12_33.log",35],["dbfs:/datasets/streamingFiles/12_35.log","12_35.log",35],["dbfs:/datasets/streamingFiles/12_37.log","12_37.log",35],["dbfs:/datasets/streamingFiles/12_39.log","12_39.log",35],["dbfs:/datasets/streamingFiles/12_41.log","12_41.log",35],["dbfs:/datasets/streamingFiles/12_43.log","12_43.log",35],["dbfs:/datasets/streamingFiles/12_45.log","12_45.log",35],["dbfs:/datasets/streamingFiles/12_47.log","12_47.log",35],["dbfs:/datasets/streamingFiles/12_49.log","12_49.log",35],["dbfs:/datasets/streamingFiles/12_51.log","12_51.log",35],["dbfs:/datasets/streamingFiles/12_53.log","12_53.log",35],["dbfs:/datasets/streamingFiles/12_55.log","12_55.log",35],["dbfs:/datasets/streamingFiles/12_57.log","12_57.log",35],["dbfs:/datasets/streamingFiles/12_59.log","12_59.log",35],["dbfs:/datasets/streamingFiles/13_01.log","13_01.log",35],["dbfs:/datasets/streamingFiles/13_03.log","13_03.log",35],["dbfs:/datasets/streamingFiles/13_05.log","13_05.log",35],["dbfs:/datasets/streamingFiles/13_07.log","13_07.log",35],["dbfs:/datasets/streamingFiles/13_09.log","13_09.log",35],["dbfs:/datasets/streamingFiles/13_11.log","13_11.log",35],["dbfs:/datasets/streamingFiles/13_13.log","13_13.log",35],["dbfs:/datasets/streamingFiles/13_15.log","13_15.log",35],["dbfs:/datasets/streamingFiles/13_17.log","13_17.log",35],["dbfs:/datasets/streamingFiles/13_19.log","13_19.log",35],["dbfs:/datasets/streamingFiles/13_21.log","13_21.log",35],["dbfs:/datasets/streamingFiles/13_23.log","13_23.log",35],["dbfs:/datasets/streamingFiles/13_25.log","13_25.log",35],["dbfs:/datasets/streamingFiles/13_27.log","13_27.log",35],["dbfs:/datasets/streamingFiles/13_29.log","13_29.log",35],["dbfs:/datasets/streamingFiles/13_31.log","13_31.log",35],["dbfs:/datasets/streamingFiles/13_33.log","13_33.log",35],["dbfs:/datasets/streamingFiles/13_35.log","13_35.log",35],["dbfs:/datasets/streamingFiles/13_37.log","13_37.log",35],["dbfs:/datasets/streamingFiles/13_39.log","13_39.log",35],["dbfs:/datasets/streamingFiles/13_41.log","13_41.log",35],["dbfs:/datasets/streamingFiles/13_43.log","13_43.log",35],["dbfs:/datasets/streamingFiles/13_45.log","13_45.log",35],["dbfs:/datasets/streamingFiles/13_47.log","13_47.log",35],["dbfs:/datasets/streamingFiles/13_49.log","13_49.log",35],["dbfs:/datasets/streamingFiles/13_51.log","13_51.log",35],["dbfs:/datasets/streamingFiles/13_53.log","13_53.log",35],["dbfs:/datasets/streamingFiles/13_55.log","13_55.log",35],["dbfs:/datasets/streamingFiles/13_57.log","13_57.log",35],["dbfs:/datasets/streamingFiles/13_59.log","13_59.log",35],["dbfs:/datasets/streamingFiles/14_01.log","14_01.log",35],["dbfs:/datasets/streamingFiles/14_03.log","14_03.log",35],["dbfs:/datasets/streamingFiles/14_06.log","14_06.log",35],["dbfs:/datasets/streamingFiles/14_08.log","14_08.log",35],["dbfs:/datasets/streamingFiles/14_10.log","14_10.log",35],["dbfs:/datasets/streamingFiles/14_12.log","14_12.log",35],["dbfs:/datasets/streamingFiles/14_14.log","14_14.log",35],["dbfs:/datasets/streamingFiles/14_16.log","14_16.log",35],["dbfs:/datasets/streamingFiles/14_18.log","14_18.log",35],["dbfs:/datasets/streamingFiles/14_20.log","14_20.log",35],["dbfs:/datasets/streamingFiles/14_22.log","14_22.log",35],["dbfs:/datasets/streamingFiles/14_24.log","14_24.log",35],["dbfs:/datasets/streamingFiles/14_26.log","14_26.log",35],["dbfs:/datasets/streamingFiles/14_28.log","14_28.log",35],["dbfs:/datasets/streamingFiles/14_30.log","14_30.log",35],["dbfs:/datasets/streamingFiles/14_32.log","14_32.log",35],["dbfs:/datasets/streamingFiles/14_34.log","14_34.log",35],["dbfs:/datasets/streamingFiles/14_36.log","14_36.log",35],["dbfs:/datasets/streamingFiles/14_38.log","14_38.log",35],["dbfs:/datasets/streamingFiles/14_40.log","14_40.log",35],["dbfs:/datasets/streamingFiles/14_42.log","14_42.log",35],["dbfs:/datasets/streamingFiles/14_44.log","14_44.log",35],["dbfs:/datasets/streamingFiles/14_46.log","14_46.log",35],["dbfs:/datasets/streamingFiles/14_48.log","14_48.log",35],["dbfs:/datasets/streamingFiles/14_50.log","14_50.log",35],["dbfs:/datasets/streamingFiles/14_52.log","14_52.log",35],["dbfs:/datasets/streamingFiles/14_54.log","14_54.log",35],["dbfs:/datasets/streamingFiles/14_56.log","14_56.log",35],["dbfs:/datasets/streamingFiles/14_58.log","14_58.log",35],["dbfs:/datasets/streamingFiles/15_00.log","15_00.log",35],["dbfs:/datasets/streamingFiles/15_02.log","15_02.log",35],["dbfs:/datasets/streamingFiles/15_04.log","15_04.log",35],["dbfs:/datasets/streamingFiles/15_06.log","15_06.log",35],["dbfs:/datasets/streamingFiles/15_08.log","15_08.log",35],["dbfs:/datasets/streamingFiles/15_10.log","15_10.log",35],["dbfs:/datasets/streamingFiles/15_12.log","15_12.log",35],["dbfs:/datasets/streamingFiles/15_14.log","15_14.log",35],["dbfs:/datasets/streamingFiles/15_16.log","15_16.log",35],["dbfs:/datasets/streamingFiles/15_18.log","15_18.log",35],["dbfs:/datasets/streamingFiles/15_20.log","15_20.log",35],["dbfs:/datasets/streamingFiles/15_22.log","15_22.log",35],["dbfs:/datasets/streamingFiles/15_24.log","15_24.log",35],["dbfs:/datasets/streamingFiles/15_26.log","15_26.log",35],["dbfs:/datasets/streamingFiles/15_28.log","15_28.log",35],["dbfs:/datasets/streamingFiles/15_30.log","15_30.log",35],["dbfs:/datasets/streamingFiles/15_32.log","15_32.log",35],["dbfs:/datasets/streamingFiles/15_34.log","15_34.log",35],["dbfs:/datasets/streamingFiles/15_36.log","15_36.log",35],["dbfs:/datasets/streamingFiles/15_38.log","15_38.log",35],["dbfs:/datasets/streamingFiles/15_40.log","15_40.log",35],["dbfs:/datasets/streamingFiles/15_42.log","15_42.log",35],["dbfs:/datasets/streamingFiles/15_44.log","15_44.log",35],["dbfs:/datasets/streamingFiles/15_46.log","15_46.log",35],["dbfs:/datasets/streamingFiles/15_48.log","15_48.log",35],["dbfs:/datasets/streamingFiles/15_50.log","15_50.log",35],["dbfs:/datasets/streamingFiles/15_52.log","15_52.log",35],["dbfs:/datasets/streamingFiles/15_54.log","15_54.log",35],["dbfs:/datasets/streamingFiles/15_56.log","15_56.log",35],["dbfs:/datasets/streamingFiles/15_58.log","15_58.log",35],["dbfs:/datasets/streamingFiles/16_00.log","16_00.log",35],["dbfs:/datasets/streamingFiles/16_02.log","16_02.log",35],["dbfs:/datasets/streamingFiles/16_04.log","16_04.log",35],["dbfs:/datasets/streamingFiles/16_06.log","16_06.log",35],["dbfs:/datasets/streamingFiles/16_08.log","16_08.log",35],["dbfs:/datasets/streamingFiles/16_10.log","16_10.log",35],["dbfs:/datasets/streamingFiles/16_12.log","16_12.log",35],["dbfs:/datasets/streamingFiles/16_14.log","16_14.log",35],["dbfs:/datasets/streamingFiles/16_16.log","16_16.log",35],["dbfs:/datasets/streamingFiles/16_18.log","16_18.log",35],["dbfs:/datasets/streamingFiles/16_20.log","16_20.log",35],["dbfs:/datasets/streamingFiles/16_22.log","16_22.log",35],["dbfs:/datasets/streamingFiles/16_24.log","16_24.log",35],["dbfs:/datasets/streamingFiles/16_26.log","16_26.log",35],["dbfs:/datasets/streamingFiles/16_28.log","16_28.log",35],["dbfs:/datasets/streamingFiles/16_30.log","16_30.log",35],["dbfs:/datasets/streamingFiles/16_32.log","16_32.log",35],["dbfs:/datasets/streamingFiles/16_34.log","16_34.log",35],["dbfs:/datasets/streamingFiles/16_36.log","16_36.log",35],["dbfs:/datasets/streamingFiles/16_38.log","16_38.log",35],["dbfs:/datasets/streamingFiles/16_40.log","16_40.log",35],["dbfs:/datasets/streamingFiles/16_42.log","16_42.log",35],["dbfs:/datasets/streamingFiles/16_44.log","16_44.log",35],["dbfs:/datasets/streamingFiles/16_46.log","16_46.log",35],["dbfs:/datasets/streamingFiles/16_48.log","16_48.log",35],["dbfs:/datasets/streamingFiles/16_50.log","16_50.log",35],["dbfs:/datasets/streamingFiles/16_52.log","16_52.log",35],["dbfs:/datasets/streamingFiles/16_54.log","16_54.log",35],["dbfs:/datasets/streamingFiles/16_56.log","16_56.log",35],["dbfs:/datasets/streamingFiles/16_58.log","16_58.log",35],["dbfs:/datasets/streamingFiles/17_00.log","17_00.log",35],["dbfs:/datasets/streamingFiles/17_02.log","17_02.log",35],["dbfs:/datasets/streamingFiles/17_04.log","17_04.log",35],["dbfs:/datasets/streamingFiles/17_06.log","17_06.log",35],["dbfs:/datasets/streamingFiles/17_08.log","17_08.log",35],["dbfs:/datasets/streamingFiles/17_10.log","17_10.log",35],["dbfs:/datasets/streamingFiles/17_12.log","17_12.log",35],["dbfs:/datasets/streamingFiles/17_14.log","17_14.log",35],["dbfs:/datasets/streamingFiles/17_17.log","17_17.log",35],["dbfs:/datasets/streamingFiles/17_19.log","17_19.log",35],["dbfs:/datasets/streamingFiles/17_21.log","17_21.log",35],["dbfs:/datasets/streamingFiles/17_23.log","17_23.log",35],["dbfs:/datasets/streamingFiles/17_25.log","17_25.log",35],["dbfs:/datasets/streamingFiles/17_27.log","17_27.log",35],["dbfs:/datasets/streamingFiles/17_29.log","17_29.log",35],["dbfs:/datasets/streamingFiles/17_31.log","17_31.log",35],["dbfs:/datasets/streamingFiles/17_33.log","17_33.log",35],["dbfs:/datasets/streamingFiles/17_35.log","17_35.log",35],["dbfs:/datasets/streamingFiles/17_37.log","17_37.log",35],["dbfs:/datasets/streamingFiles/17_39.log","17_39.log",35],["dbfs:/datasets/streamingFiles/17_41.log","17_41.log",35],["dbfs:/datasets/streamingFiles/17_43.log","17_43.log",35],["dbfs:/datasets/streamingFiles/17_45.log","17_45.log",35],["dbfs:/datasets/streamingFiles/17_47.log","17_47.log",35],["dbfs:/datasets/streamingFiles/17_49.log","17_49.log",35],["dbfs:/datasets/streamingFiles/17_51.log","17_51.log",35],["dbfs:/datasets/streamingFiles/17_53.log","17_53.log",35],["dbfs:/datasets/streamingFiles/17_55.log","17_55.log",35],["dbfs:/datasets/streamingFiles/17_57.log","17_57.log",35],["dbfs:/datasets/streamingFiles/17_59.log","17_59.log",35],["dbfs:/datasets/streamingFiles/18_01.log","18_01.log",35],["dbfs:/datasets/streamingFiles/18_03.log","18_03.log",35],["dbfs:/datasets/streamingFiles/18_05.log","18_05.log",35],["dbfs:/datasets/streamingFiles/18_07.log","18_07.log",35],["dbfs:/datasets/streamingFiles/18_09.log","18_09.log",35],["dbfs:/datasets/streamingFiles/18_11.log","18_11.log",35],["dbfs:/datasets/streamingFiles/18_13.log","18_13.log",35],["dbfs:/datasets/streamingFiles/18_15.log","18_15.log",35],["dbfs:/datasets/streamingFiles/18_17.log","18_17.log",35],["dbfs:/datasets/streamingFiles/18_19.log","18_19.log",35],["dbfs:/datasets/streamingFiles/18_21.log","18_21.log",35],["dbfs:/datasets/streamingFiles/18_23.log","18_23.log",35],["dbfs:/datasets/streamingFiles/18_25.log","18_25.log",35],["dbfs:/datasets/streamingFiles/18_27.log","18_27.log",35],["dbfs:/datasets/streamingFiles/18_29.log","18_29.log",35],["dbfs:/datasets/streamingFiles/18_31.log","18_31.log",35],["dbfs:/datasets/streamingFiles/18_33.log","18_33.log",35],["dbfs:/datasets/streamingFiles/18_35.log","18_35.log",35],["dbfs:/datasets/streamingFiles/18_37.log","18_37.log",35],["dbfs:/datasets/streamingFiles/18_39.log","18_39.log",35],["dbfs:/datasets/streamingFiles/18_41.log","18_41.log",35],["dbfs:/datasets/streamingFiles/18_43.log","18_43.log",35],["dbfs:/datasets/streamingFiles/18_45.log","18_45.log",35],["dbfs:/datasets/streamingFiles/18_47.log","18_47.log",35],["dbfs:/datasets/streamingFiles/18_49.log","18_49.log",35],["dbfs:/datasets/streamingFiles/18_51.log","18_51.log",35],["dbfs:/datasets/streamingFiles/18_53.log","18_53.log",35],["dbfs:/datasets/streamingFiles/18_55.log","18_55.log",35],["dbfs:/datasets/streamingFiles/18_57.log","18_57.log",35],["dbfs:/datasets/streamingFiles/18_59.log","18_59.log",35],["dbfs:/datasets/streamingFiles/19_01.log","19_01.log",35],["dbfs:/datasets/streamingFiles/19_03.log","19_03.log",35],["dbfs:/datasets/streamingFiles/19_05.log","19_05.log",35],["dbfs:/datasets/streamingFiles/19_07.log","19_07.log",35],["dbfs:/datasets/streamingFiles/19_09.log","19_09.log",35],["dbfs:/datasets/streamingFiles/19_11.log","19_11.log",35],["dbfs:/datasets/streamingFiles/19_13.log","19_13.log",35],["dbfs:/datasets/streamingFiles/19_15.log","19_15.log",35],["dbfs:/datasets/streamingFiles/19_17.log","19_17.log",35],["dbfs:/datasets/streamingFiles/19_19.log","19_19.log",35],["dbfs:/datasets/streamingFiles/19_21.log","19_21.log",35],["dbfs:/datasets/streamingFiles/19_23.log","19_23.log",35],["dbfs:/datasets/streamingFiles/19_25.log","19_25.log",35],["dbfs:/datasets/streamingFiles/19_27.log","19_27.log",35],["dbfs:/datasets/streamingFiles/19_29.log","19_29.log",35],["dbfs:/datasets/streamingFiles/19_31.log","19_31.log",35],["dbfs:/datasets/streamingFiles/19_33.log","19_33.log",35],["dbfs:/datasets/streamingFiles/19_35.log","19_35.log",35],["dbfs:/datasets/streamingFiles/19_37.log","19_37.log",35],["dbfs:/datasets/streamingFiles/19_39.log","19_39.log",35],["dbfs:/datasets/streamingFiles/19_41.log","19_41.log",35],["dbfs:/datasets/streamingFiles/19_43.log","19_43.log",35],["dbfs:/datasets/streamingFiles/19_45.log","19_45.log",35],["dbfs:/datasets/streamingFiles/19_47.log","19_47.log",35],["dbfs:/datasets/streamingFiles/19_49.log","19_49.log",35],["dbfs:/datasets/streamingFiles/19_51.log","19_51.log",35],["dbfs:/datasets/streamingFiles/19_53.log","19_53.log",35],["dbfs:/datasets/streamingFiles/19_55.log","19_55.log",35],["dbfs:/datasets/streamingFiles/19_57.log","19_57.log",35],["dbfs:/datasets/streamingFiles/19_59.log","19_59.log",35],["dbfs:/datasets/streamingFiles/20_01.log","20_01.log",35],["dbfs:/datasets/streamingFiles/20_03.log","20_03.log",35],["dbfs:/datasets/streamingFiles/20_05.log","20_05.log",35],["dbfs:/datasets/streamingFiles/20_08.log","20_08.log",35],["dbfs:/datasets/streamingFiles/20_10.log","20_10.log",35],["dbfs:/datasets/streamingFiles/20_12.log","20_12.log",35],["dbfs:/datasets/streamingFiles/20_14.log","20_14.log",35],["dbfs:/datasets/streamingFiles/20_16.log","20_16.log",35],["dbfs:/datasets/streamingFiles/20_18.log","20_18.log",35],["dbfs:/datasets/streamingFiles/20_20.log","20_20.log",35],["dbfs:/datasets/streamingFiles/20_22.log","20_22.log",35],["dbfs:/datasets/streamingFiles/20_24.log","20_24.log",35],["dbfs:/datasets/streamingFiles/20_26.log","20_26.log",35],["dbfs:/datasets/streamingFiles/20_28.log","20_28.log",35],["dbfs:/datasets/streamingFiles/20_30.log","20_30.log",35],["dbfs:/datasets/streamingFiles/20_32.log","20_32.log",35],["dbfs:/datasets/streamingFiles/20_34.log","20_34.log",35],["dbfs:/datasets/streamingFiles/20_36.log","20_36.log",35],["dbfs:/datasets/streamingFiles/20_38.log","20_38.log",35],["dbfs:/datasets/streamingFiles/20_40.log","20_40.log",35],["dbfs:/datasets/streamingFiles/20_42.log","20_42.log",35],["dbfs:/datasets/streamingFiles/20_44.log","20_44.log",35],["dbfs:/datasets/streamingFiles/20_46.log","20_46.log",35],["dbfs:/datasets/streamingFiles/20_48.log","20_48.log",35],["dbfs:/datasets/streamingFiles/20_50.log","20_50.log",35],["dbfs:/datasets/streamingFiles/20_52.log","20_52.log",35],["dbfs:/datasets/streamingFiles/20_54.log","20_54.log",35],["dbfs:/datasets/streamingFiles/20_56.log","20_56.log",35],["dbfs:/datasets/streamingFiles/20_58.log","20_58.log",35],["dbfs:/datasets/streamingFiles/21_00.log","21_00.log",35],["dbfs:/datasets/streamingFiles/21_02.log","21_02.log",35],["dbfs:/datasets/streamingFiles/21_04.log","21_04.log",35],["dbfs:/datasets/streamingFiles/21_06.log","21_06.log",35],["dbfs:/datasets/streamingFiles/21_08.log","21_08.log",35],["dbfs:/datasets/streamingFiles/21_10.log","21_10.log",35],["dbfs:/datasets/streamingFiles/21_12.log","21_12.log",35],["dbfs:/datasets/streamingFiles/21_14.log","21_14.log",35],["dbfs:/datasets/streamingFiles/21_16.log","21_16.log",35],["dbfs:/datasets/streamingFiles/21_18.log","21_18.log",35],["dbfs:/datasets/streamingFiles/21_20.log","21_20.log",35],["dbfs:/datasets/streamingFiles/21_22.log","21_22.log",35],["dbfs:/datasets/streamingFiles/21_24.log","21_24.log",35],["dbfs:/datasets/streamingFiles/21_26.log","21_26.log",35],["dbfs:/datasets/streamingFiles/21_28.log","21_28.log",35],["dbfs:/datasets/streamingFiles/21_30.log","21_30.log",35],["dbfs:/datasets/streamingFiles/21_32.log","21_32.log",35],["dbfs:/datasets/streamingFiles/21_34.log","21_34.log",35],["dbfs:/datasets/streamingFiles/21_36.log","21_36.log",35],["dbfs:/datasets/streamingFiles/21_38.log","21_38.log",35],["dbfs:/datasets/streamingFiles/21_40.log","21_40.log",35],["dbfs:/datasets/streamingFiles/21_42.log","21_42.log",35],["dbfs:/datasets/streamingFiles/21_44.log","21_44.log",35],["dbfs:/datasets/streamingFiles/21_46.log","21_46.log",35],["dbfs:/datasets/streamingFiles/21_48.log","21_48.log",35],["dbfs:/datasets/streamingFiles/21_50.log","21_50.log",35],["dbfs:/datasets/streamingFiles/21_52.log","21_52.log",35],["dbfs:/datasets/streamingFiles/21_54.log","21_54.log",35],["dbfs:/datasets/streamingFiles/21_56.log","21_56.log",35],["dbfs:/datasets/streamingFiles/21_58.log","21_58.log",35],["dbfs:/datasets/streamingFiles/22_00.log","22_00.log",35],["dbfs:/datasets/streamingFiles/22_02.log","22_02.log",35],["dbfs:/datasets/streamingFiles/22_04.log","22_04.log",35],["dbfs:/datasets/streamingFiles/22_06.log","22_06.log",35],["dbfs:/datasets/streamingFiles/22_08.log","22_08.log",35],["dbfs:/datasets/streamingFiles/22_11.log","22_11.log",35],["dbfs:/datasets/streamingFiles/22_13.log","22_13.log",35],["dbfs:/datasets/streamingFiles/22_15.log","22_15.log",35],["dbfs:/datasets/streamingFiles/22_17.log","22_17.log",35],["dbfs:/datasets/streamingFiles/22_19.log","22_19.log",35],["dbfs:/datasets/streamingFiles/22_21.log","22_21.log",35],["dbfs:/datasets/streamingFiles/22_23.log","22_23.log",35],["dbfs:/datasets/streamingFiles/22_25.log","22_25.log",35],["dbfs:/datasets/streamingFiles/22_27.log","22_27.log",35],["dbfs:/datasets/streamingFiles/22_29.log","22_29.log",35],["dbfs:/datasets/streamingFiles/22_31.log","22_31.log",35],["dbfs:/datasets/streamingFiles/22_33.log","22_33.log",35],["dbfs:/datasets/streamingFiles/22_35.log","22_35.log",35],["dbfs:/datasets/streamingFiles/22_37.log","22_37.log",35],["dbfs:/datasets/streamingFiles/22_39.log","22_39.log",35],["dbfs:/datasets/streamingFiles/22_41.log","22_41.log",35],["dbfs:/datasets/streamingFiles/22_43.log","22_43.log",35],["dbfs:/datasets/streamingFiles/22_45.log","22_45.log",35],["dbfs:/datasets/streamingFiles/22_47.log","22_47.log",35],["dbfs:/datasets/streamingFiles/22_49.log","22_49.log",35],["dbfs:/datasets/streamingFiles/22_51.log","22_51.log",35],["dbfs:/datasets/streamingFiles/22_53.log","22_53.log",35],["dbfs:/datasets/streamingFiles/22_55.log","22_55.log",35],["dbfs:/datasets/streamingFiles/22_57.log","22_57.log",35],["dbfs:/datasets/streamingFiles/22_59.log","22_59.log",35],["dbfs:/datasets/streamingFiles/23_01.log","23_01.log",35],["dbfs:/datasets/streamingFiles/23_03.log","23_03.log",35],["dbfs:/datasets/streamingFiles/23_05.log","23_05.log",35],["dbfs:/datasets/streamingFiles/23_07.log","23_07.log",35],["dbfs:/datasets/streamingFiles/23_09.log","23_09.log",35],["dbfs:/datasets/streamingFiles/23_11.log","23_11.log",35],["dbfs:/datasets/streamingFiles/23_13.log","23_13.log",35],["dbfs:/datasets/streamingFiles/23_15.log","23_15.log",35],["dbfs:/datasets/streamingFiles/23_17.log","23_17.log",35],["dbfs:/datasets/streamingFiles/23_19.log","23_19.log",35],["dbfs:/datasets/streamingFiles/23_21.log","23_21.log",35],["dbfs:/datasets/streamingFiles/23_23.log","23_23.log",35],["dbfs:/datasets/streamingFiles/23_25.log","23_25.log",35],["dbfs:/datasets/streamingFiles/23_27.log","23_27.log",35],["dbfs:/datasets/streamingFiles/23_29.log","23_29.log",35],["dbfs:/datasets/streamingFiles/23_31.log","23_31.log",35],["dbfs:/datasets/streamingFiles/23_33.log","23_33.log",35],["dbfs:/datasets/streamingFiles/23_36.log","23_36.log",35],["dbfs:/datasets/streamingFiles/23_38.log","23_38.log",35],["dbfs:/datasets/streamingFiles/23_40.log","23_40.log",35],["dbfs:/datasets/streamingFiles/23_42.log","23_42.log",35],["dbfs:/datasets/streamingFiles/23_44.log","23_44.log",35],["dbfs:/datasets/streamingFiles/23_46.log","23_46.log",35],["dbfs:/datasets/streamingFiles/23_48.log","23_48.log",35],["dbfs:/datasets/streamingFiles/23_50.log","23_50.log",35],["dbfs:/datasets/streamingFiles/23_52.log","23_52.log",35],["dbfs:/datasets/streamingFiles/23_54.log","23_54.log",35],["dbfs:/datasets/streamingFiles/23_56.log","23_56.log",35],["dbfs:/datasets/streamingFiles/23_58.log","23_58.log",35],["dbfs:/datasets/streamingFiles/24_00.log","24_00.log",35],["dbfs:/datasets/streamingFiles/24_02.log","24_02.log",35],["dbfs:/datasets/streamingFiles/24_04.log","24_04.log",35],["dbfs:/datasets/streamingFiles/24_06.log","24_06.log",35],["dbfs:/datasets/streamingFiles/24_08.log","24_08.log",35],["dbfs:/datasets/streamingFiles/24_10.log","24_10.log",35],["dbfs:/datasets/streamingFiles/24_12.log","24_12.log",35],["dbfs:/datasets/streamingFiles/24_14.log","24_14.log",35],["dbfs:/datasets/streamingFiles/24_16.log","24_16.log",35],["dbfs:/datasets/streamingFiles/24_18.log","24_18.log",35],["dbfs:/datasets/streamingFiles/24_20.log","24_20.log",35],["dbfs:/datasets/streamingFiles/24_22.log","24_22.log",35],["dbfs:/datasets/streamingFiles/24_24.log","24_24.log",35],["dbfs:/datasets/streamingFiles/24_26.log","24_26.log",35],["dbfs:/datasets/streamingFiles/24_28.log","24_28.log",35],["dbfs:/datasets/streamingFiles/24_30.log","24_30.log",35],["dbfs:/datasets/streamingFiles/24_32.log","24_32.log",35],["dbfs:/datasets/streamingFiles/24_34.log","24_34.log",35],["dbfs:/datasets/streamingFiles/24_36.log","24_36.log",35],["dbfs:/datasets/streamingFiles/24_38.log","24_38.log",35],["dbfs:/datasets/streamingFiles/24_40.log","24_40.log",35],["dbfs:/datasets/streamingFiles/24_42.log","24_42.log",35],["dbfs:/datasets/streamingFiles/24_44.log","24_44.log",35],["dbfs:/datasets/streamingFiles/24_46.log","24_46.log",35],["dbfs:/datasets/streamingFiles/24_48.log","24_48.log",35],["dbfs:/datasets/streamingFiles/24_50.log","24_50.log",35],["dbfs:/datasets/streamingFiles/24_52.log","24_52.log",35],["dbfs:/datasets/streamingFiles/24_54.log","24_54.log",35],["dbfs:/datasets/streamingFiles/24_56.log","24_56.log",35],["dbfs:/datasets/streamingFiles/24_58.log","24_58.log",35],["dbfs:/datasets/streamingFiles/25_00.log","25_00.log",35],["dbfs:/datasets/streamingFiles/25_02.log","25_02.log",35],["dbfs:/datasets/streamingFiles/25_04.log","25_04.log",35],["dbfs:/datasets/streamingFiles/25_06.log","25_06.log",35],["dbfs:/datasets/streamingFiles/25_08.log","25_08.log",35],["dbfs:/datasets/streamingFiles/25_10.log","25_10.log",35],["dbfs:/datasets/streamingFiles/25_12.log","25_12.log",35],["dbfs:/datasets/streamingFiles/25_14.log","25_14.log",35],["dbfs:/datasets/streamingFiles/25_16.log","25_16.log",35],["dbfs:/datasets/streamingFiles/25_18.log","25_18.log",35],["dbfs:/datasets/streamingFiles/25_21.log","25_21.log",35],["dbfs:/datasets/streamingFiles/25_23.log","25_23.log",35],["dbfs:/datasets/streamingFiles/25_25.log","25_25.log",35],["dbfs:/datasets/streamingFiles/25_27.log","25_27.log",35],["dbfs:/datasets/streamingFiles/25_29.log","25_29.log",35],["dbfs:/datasets/streamingFiles/25_31.log","25_31.log",35],["dbfs:/datasets/streamingFiles/25_33.log","25_33.log",35],["dbfs:/datasets/streamingFiles/25_35.log","25_35.log",35],["dbfs:/datasets/streamingFiles/25_37.log","25_37.log",35],["dbfs:/datasets/streamingFiles/25_39.log","25_39.log",35],["dbfs:/datasets/streamingFiles/25_41.log","25_41.log",35],["dbfs:/datasets/streamingFiles/25_43.log","25_43.log",35],["dbfs:/datasets/streamingFiles/25_45.log","25_45.log",35],["dbfs:/datasets/streamingFiles/25_47.log","25_47.log",35],["dbfs:/datasets/streamingFiles/25_49.log","25_49.log",35],["dbfs:/datasets/streamingFiles/25_51.log","25_51.log",35],["dbfs:/datasets/streamingFiles/25_53.log","25_53.log",35],["dbfs:/datasets/streamingFiles/25_55.log","25_55.log",35],["dbfs:/datasets/streamingFiles/25_57.log","25_57.log",35],["dbfs:/datasets/streamingFiles/25_59.log","25_59.log",35],["dbfs:/datasets/streamingFiles/26_01.log","26_01.log",35],["dbfs:/datasets/streamingFiles/26_03.log","26_03.log",35],["dbfs:/datasets/streamingFiles/26_05.log","26_05.log",35],["dbfs:/datasets/streamingFiles/26_07.log","26_07.log",35],["dbfs:/datasets/streamingFiles/26_09.log","26_09.log",35],["dbfs:/datasets/streamingFiles/26_11.log","26_11.log",35],["dbfs:/datasets/streamingFiles/26_13.log","26_13.log",35],["dbfs:/datasets/streamingFiles/26_15.log","26_15.log",35],["dbfs:/datasets/streamingFiles/26_17.log","26_17.log",35],["dbfs:/datasets/streamingFiles/26_19.log","26_19.log",35],["dbfs:/datasets/streamingFiles/26_21.log","26_21.log",35],["dbfs:/datasets/streamingFiles/26_23.log","26_23.log",35],["dbfs:/datasets/streamingFiles/26_25.log","26_25.log",35],["dbfs:/datasets/streamingFiles/26_27.log","26_27.log",35],["dbfs:/datasets/streamingFiles/26_29.log","26_29.log",35],["dbfs:/datasets/streamingFiles/26_31.log","26_31.log",35],["dbfs:/datasets/streamingFiles/26_33.log","26_33.log",35],["dbfs:/datasets/streamingFiles/26_35.log","26_35.log",35],["dbfs:/datasets/streamingFiles/26_37.log","26_37.log",35],["dbfs:/datasets/streamingFiles/26_39.log","26_39.log",35],["dbfs:/datasets/streamingFiles/26_41.log","26_41.log",35],["dbfs:/datasets/streamingFiles/26_43.log","26_43.log",35],["dbfs:/datasets/streamingFiles/26_45.log","26_45.log",35],["dbfs:/datasets/streamingFiles/26_47.log","26_47.log",35],["dbfs:/datasets/streamingFiles/26_49.log","26_49.log",35],["dbfs:/datasets/streamingFiles/26_51.log","26_51.log",35],["dbfs:/datasets/streamingFiles/26_53.log","26_53.log",35],["dbfs:/datasets/streamingFiles/26_55.log","26_55.log",35],["dbfs:/datasets/streamingFiles/26_57.log","26_57.log",35],["dbfs:/datasets/streamingFiles/26_59.log","26_59.log",35],["dbfs:/datasets/streamingFiles/27_02.log","27_02.log",35],["dbfs:/datasets/streamingFiles/27_04.log","27_04.log",35],["dbfs:/datasets/streamingFiles/27_06.log","27_06.log",35],["dbfs:/datasets/streamingFiles/27_08.log","27_08.log",35],["dbfs:/datasets/streamingFiles/27_10.log","27_10.log",35],["dbfs:/datasets/streamingFiles/27_12.log","27_12.log",35],["dbfs:/datasets/streamingFiles/27_14.log","27_14.log",35],["dbfs:/datasets/streamingFiles/27_16.log","27_16.log",35],["dbfs:/datasets/streamingFiles/27_18.log","27_18.log",35],["dbfs:/datasets/streamingFiles/27_20.log","27_20.log",35],["dbfs:/datasets/streamingFiles/27_22.log","27_22.log",35],["dbfs:/datasets/streamingFiles/27_24.log","27_24.log",35],["dbfs:/datasets/streamingFiles/27_26.log","27_26.log",35],["dbfs:/datasets/streamingFiles/27_28.log","27_28.log",35],["dbfs:/datasets/streamingFiles/27_30.log","27_30.log",35],["dbfs:/datasets/streamingFiles/27_32.log","27_32.log",35],["dbfs:/datasets/streamingFiles/27_34.log","27_34.log",35],["dbfs:/datasets/streamingFiles/27_36.log","27_36.log",35],["dbfs:/datasets/streamingFiles/27_38.log","27_38.log",35],["dbfs:/datasets/streamingFiles/27_40.log","27_40.log",35],["dbfs:/datasets/streamingFiles/27_42.log","27_42.log",35],["dbfs:/datasets/streamingFiles/27_44.log","27_44.log",35],["dbfs:/datasets/streamingFiles/27_46.log","27_46.log",35],["dbfs:/datasets/streamingFiles/27_48.log","27_48.log",35],["dbfs:/datasets/streamingFiles/27_50.log","27_50.log",35],["dbfs:/datasets/streamingFiles/27_52.log","27_52.log",35],["dbfs:/datasets/streamingFiles/27_54.log","27_54.log",35],["dbfs:/datasets/streamingFiles/27_56.log","27_56.log",35],["dbfs:/datasets/streamingFiles/27_58.log","27_58.log",35],["dbfs:/datasets/streamingFiles/28_00.log","28_00.log",35],["dbfs:/datasets/streamingFiles/28_02.log","28_02.log",35],["dbfs:/datasets/streamingFiles/28_04.log","28_04.log",35],["dbfs:/datasets/streamingFiles/28_06.log","28_06.log",35],["dbfs:/datasets/streamingFiles/28_08.log","28_08.log",35],["dbfs:/datasets/streamingFiles/28_10.log","28_10.log",35],["dbfs:/datasets/streamingFiles/28_12.log","28_12.log",35],["dbfs:/datasets/streamingFiles/28_14.log","28_14.log",35],["dbfs:/datasets/streamingFiles/28_16.log","28_16.log",35],["dbfs:/datasets/streamingFiles/28_18.log","28_18.log",35],["dbfs:/datasets/streamingFiles/28_21.log","28_21.log",35],["dbfs:/datasets/streamingFiles/28_23.log","28_23.log",35],["dbfs:/datasets/streamingFiles/28_25.log","28_25.log",35],["dbfs:/datasets/streamingFiles/28_27.log","28_27.log",35],["dbfs:/datasets/streamingFiles/28_29.log","28_29.log",35],["dbfs:/datasets/streamingFiles/28_31.log","28_31.log",35],["dbfs:/datasets/streamingFiles/28_33.log","28_33.log",35],["dbfs:/datasets/streamingFiles/28_35.log","28_35.log",35],["dbfs:/datasets/streamingFiles/28_37.log","28_37.log",35],["dbfs:/datasets/streamingFiles/28_39.log","28_39.log",35],["dbfs:/datasets/streamingFiles/28_41.log","28_41.log",35],["dbfs:/datasets/streamingFiles/28_43.log","28_43.log",35],["dbfs:/datasets/streamingFiles/28_45.log","28_45.log",35],["dbfs:/datasets/streamingFiles/28_47.log","28_47.log",35],["dbfs:/datasets/streamingFiles/28_49.log","28_49.log",35],["dbfs:/datasets/streamingFiles/28_51.log","28_51.log",35],["dbfs:/datasets/streamingFiles/28_53.log","28_53.log",35],["dbfs:/datasets/streamingFiles/28_55.log","28_55.log",35],["dbfs:/datasets/streamingFiles/28_57.log","28_57.log",35],["dbfs:/datasets/streamingFiles/28_59.log","28_59.log",35],["dbfs:/datasets/streamingFiles/29_01.log","29_01.log",35],["dbfs:/datasets/streamingFiles/29_03.log","29_03.log",35],["dbfs:/datasets/streamingFiles/29_05.log","29_05.log",35],["dbfs:/datasets/streamingFiles/29_07.log","29_07.log",35],["dbfs:/datasets/streamingFiles/29_09.log","29_09.log",35],["dbfs:/datasets/streamingFiles/29_11.log","29_11.log",35],["dbfs:/datasets/streamingFiles/29_13.log","29_13.log",35],["dbfs:/datasets/streamingFiles/29_15.log","29_15.log",35],["dbfs:/datasets/streamingFiles/29_17.log","29_17.log",35],["dbfs:/datasets/streamingFiles/29_19.log","29_19.log",35],["dbfs:/datasets/streamingFiles/29_21.log","29_21.log",35],["dbfs:/datasets/streamingFiles/29_23.log","29_23.log",35],["dbfs:/datasets/streamingFiles/29_25.log","29_25.log",35],["dbfs:/datasets/streamingFiles/29_27.log","29_27.log",35],["dbfs:/datasets/streamingFiles/29_29.log","29_29.log",35],["dbfs:/datasets/streamingFiles/29_31.log","29_31.log",35],["dbfs:/datasets/streamingFiles/29_33.log","29_33.log",35],["dbfs:/datasets/streamingFiles/29_35.log","29_35.log",35],["dbfs:/datasets/streamingFiles/29_37.log","29_37.log",35],["dbfs:/datasets/streamingFiles/29_39.log","29_39.log",35],["dbfs:/datasets/streamingFiles/29_41.log","29_41.log",35],["dbfs:/datasets/streamingFiles/29_43.log","29_43.log",35],["dbfs:/datasets/streamingFiles/29_45.log","29_45.log",35],["dbfs:/datasets/streamingFiles/29_47.log","29_47.log",35],["dbfs:/datasets/streamingFiles/29_49.log","29_49.log",35],["dbfs:/datasets/streamingFiles/29_51.log","29_51.log",35],["dbfs:/datasets/streamingFiles/29_53.log","29_53.log",35],["dbfs:/datasets/streamingFiles/29_55.log","29_55.log",35],["dbfs:/datasets/streamingFiles/29_57.log","29_57.log",35],["dbfs:/datasets/streamingFiles/29_59.log","29_59.log",35],["dbfs:/datasets/streamingFiles/30_01.log","30_01.log",35],["dbfs:/datasets/streamingFiles/30_03.log","30_03.log",35],["dbfs:/datasets/streamingFiles/30_05.log","30_05.log",35],["dbfs:/datasets/streamingFiles/30_07.log","30_07.log",35],["dbfs:/datasets/streamingFiles/30_09.log","30_09.log",35],["dbfs:/datasets/streamingFiles/30_11.log","30_11.log",35],["dbfs:/datasets/streamingFiles/30_13.log","30_13.log",35],["dbfs:/datasets/streamingFiles/30_15.log","30_15.log",35],["dbfs:/datasets/streamingFiles/30_17.log","30_17.log",35],["dbfs:/datasets/streamingFiles/30_19.log","30_19.log",35],["dbfs:/datasets/streamingFiles/30_21.log","30_21.log",35],["dbfs:/datasets/streamingFiles/30_23.log","30_23.log",35],["dbfs:/datasets/streamingFiles/30_25.log","30_25.log",35],["dbfs:/datasets/streamingFiles/30_27.log","30_27.log",35],["dbfs:/datasets/streamingFiles/30_29.log","30_29.log",35],["dbfs:/datasets/streamingFiles/30_31.log","30_31.log",35],["dbfs:/datasets/streamingFiles/30_33.log","30_33.log",35],["dbfs:/datasets/streamingFiles/30_36.log","30_36.log",35],["dbfs:/datasets/streamingFiles/30_38.log","30_38.log",35],["dbfs:/datasets/streamingFiles/30_40.log","30_40.log",35],["dbfs:/datasets/streamingFiles/30_42.log","30_42.log",35],["dbfs:/datasets/streamingFiles/30_44.log","30_44.log",35],["dbfs:/datasets/streamingFiles/30_46.log","30_46.log",35],["dbfs:/datasets/streamingFiles/30_48.log","30_48.log",35],["dbfs:/datasets/streamingFiles/30_50.log","30_50.log",35],["dbfs:/datasets/streamingFiles/30_52.log","30_52.log",35],["dbfs:/datasets/streamingFiles/30_54.log","30_54.log",35],["dbfs:/datasets/streamingFiles/30_56.log","30_56.log",35],["dbfs:/datasets/streamingFiles/30_58.log","30_58.log",35],["dbfs:/datasets/streamingFiles/31_00.log","31_00.log",35],["dbfs:/datasets/streamingFiles/31_02.log","31_02.log",35],["dbfs:/datasets/streamingFiles/31_04.log","31_04.log",35],["dbfs:/datasets/streamingFiles/31_06.log","31_06.log",35],["dbfs:/datasets/streamingFiles/31_08.log","31_08.log",35],["dbfs:/datasets/streamingFiles/31_10.log","31_10.log",35],["dbfs:/datasets/streamingFiles/31_12.log","31_12.log",35],["dbfs:/datasets/streamingFiles/31_14.log","31_14.log",35],["dbfs:/datasets/streamingFiles/31_16.log","31_16.log",35],["dbfs:/datasets/streamingFiles/31_18.log","31_18.log",35],["dbfs:/datasets/streamingFiles/31_20.log","31_20.log",35],["dbfs:/datasets/streamingFiles/31_22.log","31_22.log",35],["dbfs:/datasets/streamingFiles/31_24.log","31_24.log",35],["dbfs:/datasets/streamingFiles/31_26.log","31_26.log",35],["dbfs:/datasets/streamingFiles/31_28.log","31_28.log",35],["dbfs:/datasets/streamingFiles/31_30.log","31_30.log",35],["dbfs:/datasets/streamingFiles/31_32.log","31_32.log",35],["dbfs:/datasets/streamingFiles/31_34.log","31_34.log",35],["dbfs:/datasets/streamingFiles/31_36.log","31_36.log",35],["dbfs:/datasets/streamingFiles/31_38.log","31_38.log",35],["dbfs:/datasets/streamingFiles/31_40.log","31_40.log",35],["dbfs:/datasets/streamingFiles/31_42.log","31_42.log",35],["dbfs:/datasets/streamingFiles/31_44.log","31_44.log",35],["dbfs:/datasets/streamingFiles/31_46.log","31_46.log",35],["dbfs:/datasets/streamingFiles/31_48.log","31_48.log",35],["dbfs:/datasets/streamingFiles/31_50.log","31_50.log",35],["dbfs:/datasets/streamingFiles/31_52.log","31_52.log",35],["dbfs:/datasets/streamingFiles/31_54.log","31_54.log",35],["dbfs:/datasets/streamingFiles/31_56.log","31_56.log",35],["dbfs:/datasets/streamingFiles/31_58.log","31_58.log",35],["dbfs:/datasets/streamingFiles/32_00.log","32_00.log",35],["dbfs:/datasets/streamingFiles/32_02.log","32_02.log",35],["dbfs:/datasets/streamingFiles/32_04.log","32_04.log",35],["dbfs:/datasets/streamingFiles/32_06.log","32_06.log",35],["dbfs:/datasets/streamingFiles/32_08.log","32_08.log",35],["dbfs:/datasets/streamingFiles/32_10.log","32_10.log",35],["dbfs:/datasets/streamingFiles/32_12.log","32_12.log",35],["dbfs:/datasets/streamingFiles/32_14.log","32_14.log",35],["dbfs:/datasets/streamingFiles/32_16.log","32_16.log",35],["dbfs:/datasets/streamingFiles/32_18.log","32_18.log",35],["dbfs:/datasets/streamingFiles/32_20.log","32_20.log",35],["dbfs:/datasets/streamingFiles/32_22.log","32_22.log",35],["dbfs:/datasets/streamingFiles/32_24.log","32_24.log",35],["dbfs:/datasets/streamingFiles/32_26.log","32_26.log",35],["dbfs:/datasets/streamingFiles/32_28.log","32_28.log",35],["dbfs:/datasets/streamingFiles/32_30.log","32_30.log",35],["dbfs:/datasets/streamingFiles/32_32.log","32_32.log",35],["dbfs:/datasets/streamingFiles/32_34.log","32_34.log",35],["dbfs:/datasets/streamingFiles/32_36.log","32_36.log",35],["dbfs:/datasets/streamingFiles/32_38.log","32_38.log",35],["dbfs:/datasets/streamingFiles/32_40.log","32_40.log",35],["dbfs:/datasets/streamingFiles/32_42.log","32_42.log",35],["dbfs:/datasets/streamingFiles/32_44.log","32_44.log",35],["dbfs:/datasets/streamingFiles/32_46.log","32_46.log",35],["dbfs:/datasets/streamingFiles/32_48.log","32_48.log",35],["dbfs:/datasets/streamingFiles/32_50.log","32_50.log",35],["dbfs:/datasets/streamingFiles/32_52.log","32_52.log",35],["dbfs:/datasets/streamingFiles/32_54.log","32_54.log",35],["dbfs:/datasets/streamingFiles/32_56.log","32_56.log",35],["dbfs:/datasets/streamingFiles/32_58.log","32_58.log",35],["dbfs:/datasets/streamingFiles/33_01.log","33_01.log",35],["dbfs:/datasets/streamingFiles/33_03.log","33_03.log",35],["dbfs:/datasets/streamingFiles/33_05.log","33_05.log",35],["dbfs:/datasets/streamingFiles/33_07.log","33_07.log",35],["dbfs:/datasets/streamingFiles/33_09.log","33_09.log",35],["dbfs:/datasets/streamingFiles/33_11.log","33_11.log",35],["dbfs:/datasets/streamingFiles/33_13.log","33_13.log",35],["dbfs:/datasets/streamingFiles/33_15.log","33_15.log",35],["dbfs:/datasets/streamingFiles/33_17.log","33_17.log",35],["dbfs:/datasets/streamingFiles/33_19.log","33_19.log",35],["dbfs:/datasets/streamingFiles/33_21.log","33_21.log",35],["dbfs:/datasets/streamingFiles/33_23.log","33_23.log",35],["dbfs:/datasets/streamingFiles/33_25.log","33_25.log",35],["dbfs:/datasets/streamingFiles/33_27.log","33_27.log",35],["dbfs:/datasets/streamingFiles/33_29.log","33_29.log",35],["dbfs:/datasets/streamingFiles/33_31.log","33_31.log",35],["dbfs:/datasets/streamingFiles/33_33.log","33_33.log",35]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"overflow":true,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511291106194,"submitTime":1511291117134,"finishTime":1511291108179,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d85a55c5-3163-48b7-905a-a525defc06a2"},{"version":"CommandV1","origId":207883,"guid":"3ca04e21-2f67-424f-9f98-348c9862d4dc","subtype":"command","commandType":"auto","position":3.375,"command":"dbutils.fs.head(\"/datasets/streamingFiles/00_00.log\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res43: String =\n&quot;2017-11-21 18:00:00+00:00; pig owl\n&quot;\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"java.io.FileNotFoundException: /datasets/streamingFiles/*.log","error":"<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:52)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.getFileStatus(DatabricksFileSystemV1.scala:222)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:142)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.head(DBUtilsCore.scala:125)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.head(DbfsUtilsImpl.scala:49)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:106)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:113)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:115)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:117)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:119)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:121)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:123)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:125)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:127)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:129)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:131)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:133)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:135)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:137)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:139)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:141)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:143)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:145)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:147)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:149)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:151)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:153)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:155)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:157)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:159)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:161)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:163)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:165)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:167)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:169)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:171)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:173)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:175)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:177)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:179)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:181)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:183)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:185)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:187)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:189)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:191)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:193)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:195)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:197)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:199)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:201)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:203)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:205)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:207)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:209)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:211)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$read$$iw.&lt;init&gt;(&lt;console&gt;:213)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$eval$.$print$lzycompute(&lt;console&gt;:7)\n\tat line454116f1a10b4ff6bb98088b402e945a289.$eval$.$print(&lt;console&gt;:6)</div>","workflows":[],"startTime":1511291113404,"submitTime":1511291124333,"finishTime":1511291115795,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4bba3823-7ed3-4f4a-9e24-660b39344a69"},{"version":"CommandV1","origId":204191,"guid":"3a4a0c67-9106-418d-afda-80ffe8ceeef2","subtype":"command","commandType":"auto","position":3.5,"command":"%md\nNext, let’s create a streaming DataFrame that represents text data\nreceived from the directory, and transform the\nDataFrame to calculate word counts.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2946df99-1b8a-42a4-95bd-9a5b2a310a68"},{"version":"CommandV1","origId":204269,"guid":"f21fddaf-5f35-4690-bdc9-e8e0e7aa284f","subtype":"command","commandType":"auto","position":3.75,"command":"import org.apache.spark.sql.types._\n\n// Create DataFrame representing the stream of input lines from files in distributed file store\n//val textFileSchema = new StructType().add(\"line\", \"string\") // for a custom schema\n\nval streamingLines = spark\n  .readStream\n  //.schema(textFileSchema) // using default -> makes a column of String named value\n  .option(\"MaxFilesPerTrigger\", 1) //  maximum number of new files to be considered in every trigger (default: no max) \n  .format(\"text\")\n  .load(\"/datasets/streamingFiles\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.sql.types._\nstreamingLines: org.apache.spark.sql.DataFrame = [value: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511291127050,"submitTime":1511291137992,"finishTime":1511291128504,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3d629113-0e8e-4e93-bb53-029430cf5edd"},{"version":"CommandV1","origId":207884,"guid":"22766561-ea5f-4c5b-acd0-54991c2d6609","subtype":"command","commandType":"auto","position":4.375,"command":"%md\nThis `streamingLines` DataFrame represents an unbounded table containing the\nstreaming text data. This table contains one column of strings named\n“value”, and each line in the streaming text data becomes a row in the\ntable. Note, that this is not currently receiving any data as we are\njust setting up the transformation, and have not yet started it. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"576960da-8273-488f-b03f-e855560f0b2f"},{"version":"CommandV1","origId":204271,"guid":"b7bde2b1-6963-4300-9afd-2e9179cffcae","subtype":"command","commandType":"auto","position":5.0,"command":"//display(streamingLines)  // display will show you the contents of the DF","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:37: error: not found: value streamingLogs\n       display(streamingLogs)\n               ^\n</div>","error":null,"workflows":[],"startTime":1511269450331,"submitTime":1511269460837,"finishTime":1511269450729,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"989f5fd2-cc02-4029-82a2-b8e1773abfd5"},{"version":"CommandV1","origId":204193,"guid":"b66f84e2-5bd4-4045-ae6f-edd8bfc935f5","subtype":"command","commandType":"auto","position":5.75,"command":"\n%md\nNext, we will convert the DataFrame to a Dataset of String using\n`.as[String]`, so that we can apply the `flatMap` operation to split\neach line into multiple words. The resultant `words` Dataset contains\nall the words. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bc68e5dd-9f7a-4838-ade1-83662cd1cebc"},{"version":"CommandV1","origId":204285,"guid":"150f9745-9421-436c-90e5-50a834bf5c85","subtype":"command","commandType":"auto","position":5.875,"command":"val words = streamingLines.as[String]\n                          .map(line => line.split(\";\").drop(1)(0)) // this is to simply cut out the timestamp from this stream\n                          .flatMap(_.split(\" \")) // flat map by splitting the animal words separated by whitespace\n                          .filter( _ != \"\") // remove empty words that may be artifacts of opening whitespace","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">words: org.apache.spark.sql.Dataset[String] = [value: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:96: error: not found: value animal\n                                 .filter( animal != &quot;&quot;)\n                                          ^\n</div>","error":null,"workflows":[],"startTime":1511292252251,"submitTime":1511292263210,"finishTime":1511292252836,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2bdfeb8b-f193-4d8e-b067-777ac193db97"},{"version":"CommandV1","origId":207885,"guid":"97214ccf-1f30-4e6d-b2e4-0add2d2ad439","subtype":"command","commandType":"auto","position":6.4375,"command":"%md\nFinally, we define the `wordCounts` DataFrame by\ngrouping by the unique values in the Dataset and counting them. Note\nthat this is a streaming DataFrame which represents the **running word\ncounts of the stream**.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"69e69178-fc75-4dc6-a3b5-f3f22bce76f1"},{"version":"CommandV1","origId":204286,"guid":"15dc3e51-e37a-4171-8ddb-dfdb66f92114","subtype":"command","commandType":"auto","position":7.0,"command":"// Generate running word count\nval wordCounts = words\n                  .groupBy(\"value\").count() // this does the word count\n                  .orderBy($\"count\".desc) // we are simply sorting by the most frequent words","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">wordCounts: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: string, count: bigint]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:65: error: missing parameter type for expanded function ((x$1) =&gt; x$1.startsWith(&quot;2017&quot;))\n       val wordCounts = words.filter(!(_.startsWith(&quot;2017&quot;))).groupBy(&quot;value&quot;).count().orderBy($&quot;count&quot;.desc)\n                                       ^\n</div>","error":null,"workflows":[],"startTime":1511292296122,"submitTime":1511292307081,"finishTime":1511292298237,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6f97cb8d-bfec-4cba-867e-5e2883a58777"},{"version":"CommandV1","origId":204194,"guid":"fac766b2-2949-491a-8716-63aee0d79fea","subtype":"command","commandType":"auto","position":8.0,"command":"%md\nWe have now set up the query on the streaming data. All that is left is\nto actually start receiving data and computing the counts. To do this,\nwe set it up to print the complete set of counts (specified by\n`outputMode(\"complete\")`) to the console every time they are updated.\nAnd then start the streaming computation using `start()`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"94dbdf9e-fa6d-4213-949f-630bc3e6de2d"},{"version":"CommandV1","origId":204195,"guid":"b7a40263-b490-415a-9418-eb950104dd1b","subtype":"command","commandType":"auto","position":9.0,"command":"// Start running the query that prints the running counts to the console\nval query = wordCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .start()\n\nquery.awaitTermination() // hit cancel to terminate - killall the bash script in 037a_AnimalNamesStructStreamingFiles","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\">-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n|  cat|    1|\n|  pig|    1|\n+-----+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n|  cat|    2|\n|  dog|    1|\n|  pig|    1|\n+-----+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n|  cat|    2|\n|  rat|    1|\n|  pig|    1|\n|  owl|    1|\n|  dog|    1|\n+-----+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n|  cat|    3|\n|  rat|    1|\n|  owl|    1|\n|  pig|    1|\n|  dog|    1|\n|  bat|    1|\n+-----+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n|  cat|    3|\n|  dog|    2|\n|  pig|    2|\n|  rat|    1|\n|  owl|    1|\n|  bat|    1|\n+-----+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1511292311916,"submitTime":1511292311916,"finishTime":1511292483035,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"352e8ece-b3b1-4b79-8a82-5e2c9372d49f"},{"version":"CommandV1","origId":204288,"guid":"e09c415c-4f9a-4271-ae53-23b242fab5f5","subtype":"command","commandType":"auto","position":11.0,"command":"%md\nAfter this code is executed, the streaming computation will have started\nin the background. The `query` object is a handle to that active\nstreaming query, and we have decided to wait for the termination of the\nquery using `awaitTermination()` to prevent the process from exiting\nwhile the query is active.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"65823ece-b999-45cd-b718-54e178085a29"},{"version":"CommandV1","origId":204290,"guid":"4b0661dd-0764-4e3b-a91a-267c2f446842","subtype":"command","commandType":"auto","position":13.0,"command":"%md\nHandling Event-time and Late Data\n---------------------------------\n\nEvent-time is the time embedded in the data itself. For many\napplications, you may want to operate on this event-time. For example,\nif you want to get the number of events generated by IoT devices every\nminute, then you probably want to use the time when the data was\ngenerated (that is, event-time in the data), rather than the time Spark\nreceives them. This event-time is very naturally expressed in this model\n– each event from the devices is a row in the table, and event-time is a\ncolumn value in the row. This allows window-based aggregations (e.g.\nnumber of events every minute) to be just a special type of grouping and\naggregation on the event-time column – each time window is a group and\neach row can belong to multiple windows/groups. Therefore, such\nevent-time-window-based aggregation queries can be defined consistently\non both a static dataset (e.g. from collected device events logs) as\nwell as on a data stream, making the life of the user much easier.\n\nFurthermore, this model naturally handles data that has arrived later\nthan expected based on its event-time. Since Spark is updating the\nResult Table, it has full control over updating old aggregates when\nthere is late data, as well as cleaning up old aggregates to limit the\nsize of intermediate state data. Since Spark 2.1, we have support for\nwatermarking which allows the user to specify the threshold of late\ndata, and allows the engine to accordingly clean up old state. These are\nexplained later in more detail in the **Window Operations** section below.\n\nFault Tolerance Semantics\n-------------------------\n\nDelivering end-to-end exactly-once semantics was one of key goals behind\nthe design of Structured Streaming. To achieve that, we have designed\nthe Structured Streaming sources, the sinks and the execution engine to\nreliably track the exact progress of the processing so that it can\nhandle any kind of failure by restarting and/or reprocessing. Every\nstreaming source is assumed to have offsets (similar to Kafka offsets,\nor Kinesis sequence numbers) to track the read position in the stream.\nThe engine uses checkpointing and write ahead logs to record the offset\nrange of the data being processed in each trigger. The streaming sinks\nare designed to be idempotent for handling reprocessing. Together, using\nreplayable sources and idempotent sinks, Structured Streaming can ensure\n**end-to-end exactly-once semantics** under any failure.\n\nAPI using Datasets and DataFrames\n=================================\n\nSince Spark 2.0, DataFrames and Datasets can represent static, bounded\ndata, as well as streaming, unbounded data. Similar to static\nDatasets/DataFrames, you can use the common entry point `SparkSession`\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.SparkSession)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/SparkSession.html)/[Python](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession)/[R](https://spark.apache.org/docs/2.2.0/api/R/sparkR.session.html)\ndocs) to create streaming DataFrames/Datasets from streaming sources,\nand apply the same operations on them as static DataFrames/Datasets. If\nyou are not familiar with Datasets/DataFrames, you are strongly advised\nto familiarize yourself with them using the [DataFrame/Dataset\nProgramming\nGuide](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).\n\nCreating streaming DataFrames and streaming Datasets\n----------------------------------------------------\n\nStreaming DataFrames can be created through the `DataStreamReader`\ninterface\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/DataStreamReader.html)/[Python](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader)\ndocs) returned by `SparkSession.readStream()`. In\n[R](https://spark.apache.org/docs/2.2.0/api/R/read.stream.html), with\nthe `read.stream()` method. Similar to the read interface for creating\nstatic DataFrame, you can specify the details of the source – data\nformat, schema, options, etc.\n\n#### Input Sources\n\nIn Spark 2.0, there are a few built-in sources.\n\n-   **File source** - Reads files written in a directory as a stream\n    of data. Supported file formats are text, csv, json, parquet. See\n    the docs of the DataStreamReader interface for a more up-to-date\n    list, and supported options for each file format. Note that the\n    files must be atomically placed in the given directory, which in\n    most file systems, can be achieved by file move operations.\n\n-   **Kafka source** - Poll data from Kafka. It’s compatible with Kafka\n    broker versions 0.10.0 or higher. See the [Kafka Integration\n    Guide](https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html)\n    for more details.\n\n-   **Socket source (for testing)** - Reads UTF8 text data from a\n    socket connection. The listening server socket is at the driver.\n    Note that this should be used only for testing as this does not\n    provide end-to-end fault-tolerance guarantees.\n\nSome sources are not fault-tolerant because they do not guarantee that\ndata can be replayed using checkpointed offsets after a failure. See the\nearlier section on [fault-tolerance\nsemantics](structured-streaming-programming-guide.html#fault-tolerance-semantics).\nHere are the details of all the sources in Spark.\n\n<table class=\"table\">\n  <tr>\n    <th>Source</th>\n    <th>Options</th>\n    <th>Fault-tolerant</th>\n    <th>Notes</th>\n  </tr>\n  <tr>\n    <td><b>File source</b></td>\n    <td>\n        <code>path</code>: path to the input directory, and common to all file formats.\n        <br />\n        <code>maxFilesPerTrigger</code>: maximum number of new files to be considered in every trigger (default: no max)\n        <br />\n        <code>latestFirst</code>: whether to processs the latest new files first, useful when there is a large backlog of files (default: false)\n        <br />\n        <code>fileNameOnly</code>: whether to check new files based on only the filename instead of on the full path (default: false). With this set to `true`, the following files would be considered as the same file, because their filenames, \"dataset.txt\", are the same:\n        <br />\n        · \"file:///dataset.txt\"<br />\n        · \"s3://a/dataset.txt\"<br />\n        · \"s3n://a/b/dataset.txt\"<br />\n        · \"s3a://a/b/c/dataset.txt\"<br />\n        <br />\n\n        <br />\n        For file-format-specific options, see the related methods in <code>DataStreamReader</code>\n        (<a href=\"https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader\">Scala</a>/<a href=\"https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/DataStreamReader.html\">Java</a>/<a href=\"https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader\">Python</a>/<a href=\"https://spark.apache.org/docs/2.2.0/api/R/read.stream.html\">R</a>).\n        E.g. for \"parquet\" format options see <code>DataStreamReader.parquet()</code></td>\n    <td>Yes</td>\n    <td>Supports glob paths, but does not support multiple comma-separated paths/globs.</td>\n  </tr>\n  <tr>\n    <td><b>Socket Source</b></td>\n    <td>\n        <code>host</code>: host to connect to, must be specified<br />\n        <code>port</code>: port to connect to, must be specified\n    </td>\n    <td>No</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><b>Kafka Source</b></td>\n    <td>\n        See the <a href=\"https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html\">Kafka Integration Guide</a>.\n    </td>\n    <td>Yes</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\nSee [https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#input-sources](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#input-sources).\n\n### Schema inference and partition of streaming DataFrames/Datasets\n\nBy default, Structured Streaming from file based sources requires you to\nspecify the schema, rather than rely on Spark to infer it automatically (*this is what we did with `userSchema` above*).\nThis restriction ensures a consistent schema will be used for the\nstreaming query, even in the case of failures. For ad-hoc use cases, you\ncan reenable schema inference by setting\n`spark.sql.streaming.schemaInference` to `true`.\n\nPartition discovery does occur when subdirectories that are named\n`/key=value/` are present and listing will automatically recurse into\nthese directories. If these columns appear in the user provided schema,\nthey will be filled in by Spark based on the path of the file being\nread. The directories that make up the partitioning scheme must be\npresent when the query starts and must remain static. For example, it is\nokay to add `/data/year=2016/` when `/data/year=2015/` was present, but\nit is invalid to change the partitioning column (i.e. by creating the\ndirectory `/data/date=2016-04-17/`).\n\nOperations on streaming DataFrames/Datasets\n-------------------------------------------\n\nYou can apply all kinds of operations on streaming DataFrames/Datasets –\nranging from untyped, SQL-like operations (e.g. `select`, `where`,\n`groupBy`), to typed RDD-like operations (e.g. `map`, `filter`,\n`flatMap`). See the [SQL programming\nguide](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html)\nfor more details. Let’s take a look at a few example operations that you\ncan use.\n\n### Basic Operations - Selection, Projection, Aggregation\n\nMost of the common operations on DataFrame/Dataset are supported for\nstreaming. The few operations that are not supported are discussed\nlater in **unsupported-operations** section.\n\n```%scala\n    case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)\n\n    val df: DataFrame = ... // streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }\n    val ds: Dataset[DeviceData] = df.as[DeviceData]    // streaming Dataset with IOT device data\n\n    // Select the devices which have signal more than 10\n    df.select(\"device\").where(\"signal > 10\")      // using untyped APIs\n    ds.filter(_.signal > 10).map(_.device)         // using typed APIs\n\n    // Running count of the number of updates for each device type\n    df.groupBy(\"deviceType\").count()                          // using untyped API\n\n    // Running average signal for each device type\n    import org.apache.spark.sql.expressions.scalalang.typed\n    ds.groupByKey(_.deviceType).agg(typed.avg(_.signal))    // using typed API\n\n```\n\n## A Quick Mixture Example\n\nWe will work below with a file stream that simulates random animal names or a simple mixture of two Normal Random Variables.\n\nThe two file streams can be acieved by running the codes in the following two databricks notebooks in the same cluster:\n\n* `037a_AnimalNamesStructStreamingFiles`\n* `037b_Mix2NormalsStructStreamingFiles`\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"63e10ef5-deab-4329-96aa-a9459e142080"},{"version":"CommandV1","origId":204709,"guid":"bfaa9ee7-c525-4c8d-b1f1-2f0565f7d7c8","subtype":"command","commandType":"auto","position":13.125,"command":"%md\nYou should have the following set of csv files (it won't be exactly the same names depending on when you start the stream of files).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bfbd2b75-7965-44de-97dd-fde37f45b1ba"},{"version":"CommandV1","origId":204707,"guid":"d69d8732-2468-4c3d-bc42-87ee2f6b58c7","subtype":"command","commandType":"auto","position":13.25,"command":"display(dbutils.fs.ls(\"/datasets/streamingFilesNormalMixture/\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/datasets/streamingFilesNormalMixture/23_35/","23_35/",0],["dbfs:/datasets/streamingFilesNormalMixture/23_43/","23_43/",0],["dbfs:/datasets/streamingFilesNormalMixture/23_49/","23_49/",0],["dbfs:/datasets/streamingFilesNormalMixture/23_57/","23_57/",0],["dbfs:/datasets/streamingFilesNormalMixture/24_03/","24_03/",0],["dbfs:/datasets/streamingFilesNormalMixture/24_10/","24_10/",0],["dbfs:/datasets/streamingFilesNormalMixture/24_17/","24_17/",0],["dbfs:/datasets/streamingFilesNormalMixture/24_23/","24_23/",0],["dbfs:/datasets/streamingFilesNormalMixture/24_31/","24_31/",0],["dbfs:/datasets/streamingFilesNormalMixture/24_38/","24_38/",0],["dbfs:/datasets/streamingFilesNormalMixture/52_05/","52_05/",0],["dbfs:/datasets/streamingFilesNormalMixture/52_12/","52_12/",0],["dbfs:/datasets/streamingFilesNormalMixture/52_19/","52_19/",0],["dbfs:/datasets/streamingFilesNormalMixture/52_31/","52_31/",0],["dbfs:/datasets/streamingFilesNormalMixture/52_39/","52_39/",0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511292524190,"submitTime":1511292535131,"finishTime":1511292524756,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"97ed65ec-9e0b-4d2f-b50b-3bcb5746c136"},{"version":"CommandV1","origId":207891,"guid":"d21763a6-9035-4152-9d7f-0fd9cb45b9ea","subtype":"command","commandType":"auto","position":13.3125,"command":"%md\n## Static and Streaming DataFrames\n\nLet's check out the files and their contents both via static as well as streaming DataFrames. \n\nThis will also cement the fact that structured streaming allows interoperability between static and streaming data and can be useful for debugging.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fc2092e6-0723-4cbc-b449-494ee87f1ca6"},{"version":"CommandV1","origId":204708,"guid":"f7547c40-12c8-49d5-bc66-8be90bb775f7","subtype":"command","commandType":"auto","position":13.375,"command":"val peekIn = spark.read.format(\"csv\").load(\"/datasets/streamingFilesNormalMixture/*/*.csv\")\npeekIn.count() // total count of all the samples in all the files","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">peekIn: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\nres51: Long = 1500\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:4: error: ';' expected but '.' found.\nprintln peekIn.count() // total count of all the samples in all the files\n              ^\n</div>","error":null,"workflows":[],"startTime":1511292531146,"submitTime":1511292542095,"finishTime":1511292537753,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"44f40702-0127-4227-b0ee-82b0323ed803"},{"version":"CommandV1","origId":207894,"guid":"b0f914f8-8d35-4a98-8ded-c8d7fa14a6b8","subtype":"command","commandType":"auto","position":13.6875,"command":"peekIn.show(5) // let's take a quick peek at what's in the CSV files","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+--------------------+--------------------+\n|                 _c0|                 _c1|\n+--------------------+--------------------+\n|2017-11-21 14:23:...| 0.21791376679544772|\n|2017-11-21 14:23:...|0.011291967445604012|\n|2017-11-21 14:23:...|-0.30293144696154806|\n|2017-11-21 14:23:...|  0.4303254534802833|\n|2017-11-21 14:23:...|  1.5521304466388752|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511292540703,"submitTime":1511292551668,"finishTime":1511292543494,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"acb154aa-96c2-40c9-91e7-bed393ff191e"},{"version":"CommandV1","origId":204437,"guid":"579fac70-0890-4a98-befe-5795ef4baac8","subtype":"command","commandType":"auto","position":14.0,"command":"// Read all the csv files written atomically from a directory\n//import org.apache.spark.sql.types._\n\n//make a user-specified schema - this is needed for structured streaming from files\nval userSchema = new StructType()\n                      .add(\"time\", \"timestamp\")\n                      .add(\"score\", \"Double\")\n\n// a static DF is convenient \nval csvStaticDF = spark\n  .read\n  .option(\"sep\", \",\") // delimiter is ','\n  .schema(userSchema) // Specify schema of the csv files as pre-defined by user\n  .csv(\"/datasets/streamingFilesNormalMixture/*/*.csv\")    // Equivalent to format(\"csv\").load(\"/path/to/directory\")\n\n// streaming DF\nval csvStreamingDF = spark\n  .readStream\n  .option(\"sep\", \",\") // delimiter is ','\n  .schema(userSchema) // Specify schema of the csv files as pre-defined by user\n  .option(\"MaxFilesPerTrigger\", 1) //  maximum number of new files to be considered in every trigger (default: no max) \n  .csv(\"/datasets/streamingFilesNormalMixture/*/*.csv\")    // Equivalent to format(\"csv\").load(\"/path/to/directory\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">userSchema: org.apache.spark.sql.types.StructType = StructType(StructField(time,TimestampType,true), StructField(score,DoubleType,true))\ncsvStaticDF: org.apache.spark.sql.DataFrame = [time: timestamp, score: double]\ncsvStreamingDF: org.apache.spark.sql.DataFrame = [time: timestamp, score: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1511292608969,"submitTime":1511292619907,"finishTime":1511292612977,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"322f5718-ec39-4ec9-90a5-8e47f2071394"},{"version":"CommandV1","origId":204717,"guid":"5235d603-948a-45af-86ca-4123dda78b89","subtype":"command","commandType":"auto","position":14.25,"command":"csvStreamingDF.isStreaming    // Returns True for DataFrames that have streaming sources","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res53: Boolean = true\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511292616928,"submitTime":1511292627874,"finishTime":1511292618932,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e09bed69-d6b8-400b-aa0b-a060e3cc38bd"},{"version":"CommandV1","origId":204718,"guid":"65e038e7-3111-4f34-8d1c-b7e64b849f0e","subtype":"command","commandType":"auto","position":14.375,"command":"csvStreamingDF.printSchema","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- time: timestamp (nullable = true)\n |-- score: double (nullable = true)\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511292621939,"submitTime":1511292632911,"finishTime":1511292622247,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5c221bce-0f6c-426d-9962-851a778ae162"},{"version":"CommandV1","origId":204714,"guid":"9a028fc0-3d71-4235-8e99-1a3e1059f5ff","subtype":"command","commandType":"auto","position":14.5,"command":"//display(csvStreamingDF) // if you want to see the stream coming at you as csvDF","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9e78753e-5c96-4f31-a2d7-d54dc125c3d0"},{"version":"CommandV1","origId":204710,"guid":"106e3d7e-e826-4c94-be47-5fb23ff160f0","subtype":"command","commandType":"auto","position":15.0,"command":"import org.apache.spark.sql.functions._\n\n// Start running the query that prints the running counts to the console\nval query = csvStreamingDF\n                 // bround simply rounds the double to the desired decimal place - 0 in our case here. \n                   // see https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html#bround-org.apache.spark.sql.Column-\n                   // we are using bround to simply coarsen out data into bins for counts\n                 .select(bround($\"score\", 0).as(\"binnedScore\")) \n                 .groupBy($\"binnedScore\")\n                 .agg(count($\"binnedScore\") as \"binnedScoreCounts\")\n                 .orderBy($\"binnedScore\")\n                 .writeStream\n                 .outputMode(\"complete\")\n                 .format(\"console\")\n                 .start()\n                 \nquery.awaitTermination() // hit cancel to terminate","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\">-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -1.0|                9|\n|        0.0|               18|\n|        1.0|               41|\n|        2.0|               25|\n|        3.0|                5|\n|        4.0|                1|\n|       10.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                1|\n|       -1.0|               13|\n|        0.0|               44|\n|        1.0|               83|\n|        2.0|               46|\n|        3.0|               10|\n|        4.0|                1|\n|       10.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                2|\n|       -1.0|               20|\n|        0.0|               74|\n|        1.0|              118|\n|        2.0|               70|\n|        3.0|               12|\n|        4.0|                1|\n|        9.0|                1|\n|       10.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                4|\n|       -1.0|               27|\n|        0.0|              104|\n|        1.0|              144|\n|        2.0|               96|\n|        3.0|               21|\n|        4.0|                1|\n|        9.0|                1|\n|       10.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                4|\n|       -1.0|               32|\n|        0.0|              125|\n|        1.0|              179|\n|        2.0|              125|\n|        3.0|               30|\n|        4.0|                2|\n|        9.0|                1|\n|       10.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                4|\n|       -1.0|               38|\n|        0.0|              148|\n|        1.0|              218|\n|        2.0|              150|\n|        3.0|               34|\n|        4.0|                3|\n|        9.0|                2|\n|       10.0|                2|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                4|\n|       -1.0|               49|\n|        0.0|              172|\n|        1.0|              252|\n|        2.0|              172|\n|        3.0|               40|\n|        4.0|                3|\n|        9.0|                2|\n|       10.0|                4|\n|       11.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                4|\n|       -1.0|               58|\n|        0.0|              198|\n|        1.0|              284|\n|        2.0|              197|\n|        3.0|               46|\n|        4.0|                4|\n|        9.0|                2|\n|       10.0|                5|\n|       11.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                5|\n|       -1.0|               68|\n|        0.0|              218|\n|        1.0|              323|\n|        2.0|              219|\n|        3.0|               54|\n|        4.0|                4|\n|        9.0|                2|\n|       10.0|                5|\n|       11.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                5|\n|       -1.0|               75|\n|        0.0|              252|\n|        1.0|              351|\n|        2.0|              239|\n|        3.0|               61|\n|        4.0|                6|\n|        8.0|                1|\n|        9.0|                3|\n|       10.0|                5|\n|       11.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 10\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                5|\n|       -1.0|               84|\n|        0.0|              270|\n|        1.0|              392|\n|        2.0|              264|\n|        3.0|               66|\n|        4.0|                7|\n|        8.0|                1|\n|        9.0|                3|\n|       10.0|                6|\n|       11.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 11\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                6|\n|       -1.0|               88|\n|        0.0|              296|\n|        1.0|              434|\n|        2.0|              285|\n|        3.0|               71|\n|        4.0|                7|\n|        8.0|                1|\n|        9.0|                3|\n|       10.0|                6|\n|       11.0|                1|\n|       12.0|                2|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 12\n-------------------------------------------\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1511292683236,"submitTime":1511292683236,"finishTime":1511293029439,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"edba2de9-5306-4af9-9189-f9caf46e0577"},{"version":"CommandV1","origId":207895,"guid":"36aa548d-8500-4efc-9167-187f3f45fb90","subtype":"command","commandType":"auto","position":16.0,"command":"%md\nOnce the above streaming job has processed all the files in the directory, it will continue to \"listen\" in for new files in the directory. You could for example return to the other notebook `037b_Mix2NormalsStructStreamingFiles` and rerun the cell that writes another lot of newer files into the directory and return to this notebook to watch the above streaming job continue with additional batches.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"123db1fe-46d6-4020-ac4d-ab007ba1604e"},{"version":"CommandV1","origId":204713,"guid":"fb0343cc-d821-4700-9e09-559e94aaa125","subtype":"command","commandType":"auto","position":17.0,"command":"%md\n## Static and Streaming DataSets\n\nThese examples generate streaming DataFrames that are untyped, meaning\nthat the schema of the DataFrame is not checked at compile time, only\nchecked at runtime when the query is submitted. Some operations like\n`map`, `flatMap`, etc. need the type to be known at compile time. To do\nthose, you can convert these untyped streaming DataFrames to typed\nstreaming Datasets using the same methods as static DataFrame. See the\n[SQL Programming Guide](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html)\nfor more details. Additionally, more details on the supported streaming\nsources are discussed later in the document.\n\nLet us make a `dataset` version of the streaming dataframe.\n\nBut first let us try it make the datset from the static dataframe and then apply it to the streming dataframe.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"be7dbf31-14d4-4e25-aed0-961db73b49a9"},{"version":"CommandV1","origId":204720,"guid":"a9e1a997-ce16-4b46-bafa-5bf6217d5ab8","subtype":"command","commandType":"auto","position":18.5,"command":"csvStaticDF.printSchema // schema of the static DF","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- time: timestamp (nullable = true)\n |-- score: double (nullable = true)\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:88: error: not found: value csvStatisDF\n       csvStatisDF\n       ^\n</div>","error":null,"workflows":[],"startTime":1511293080969,"submitTime":1511293091926,"finishTime":1511293081384,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"013e1d1a-541f-4be3-a5e3-69c239222a66"},{"version":"CommandV1","origId":204721,"guid":"a6584703-e9c4-4473-8884-223d9e8708ef","subtype":"command","commandType":"auto","position":18.75,"command":"import org.apache.spark.sql.types._\nimport java.sql.Timestamp\n\n// create a case class to make the datset\ncase class timedScores(time: Timestamp, score: Double)\n\nval csvStaticDS = csvStaticDF.as[timedScores] // create a dataset from the dataframe","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.sql.types._\nimport java.sql.Timestamp\ndefined class timedScores\ncsvStaticDS: org.apache.spark.sql.Dataset[timedScores] = [time: timestamp, score: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:70: error: not found: value csvStaticDF\n       val csvStaticDS = csvStaticDF.as[timedScores] // create a dataset from the dataframe\n                         ^\n</div>","error":null,"workflows":[],"startTime":1511293086293,"submitTime":1511293097246,"finishTime":1511293089251,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"caa46821-47ac-435c-a597-1c5af1865320"},{"version":"CommandV1","origId":204716,"guid":"68d55b17-cc0d-4154-a214-cd52ac17fd2a","subtype":"command","commandType":"auto","position":19.0,"command":"csvStaticDS.show(5,false) // looks like we got the dataset we want with strong typing","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----------------------+--------------------+\n|time                   |score               |\n+-----------------------+--------------------+\n|2017-11-21 14:23:49.375|0.21791376679544772 |\n|2017-11-21 14:23:49.381|0.011291967445604012|\n|2017-11-21 14:23:49.386|-0.30293144696154806|\n|2017-11-21 14:23:49.391|0.4303254534802833  |\n|2017-11-21 14:23:49.396|1.5521304466388752  |\n+-----------------------+--------------------+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:99: error: not found: value csvStaticDs\n       csvStaticDs.show(5,false)\n       ^\n</div>","error":null,"workflows":[],"startTime":1511293105077,"submitTime":1511293116028,"finishTime":1511293105587,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f84e51ee-27d1-48c8-b9da-42a0684f64e0"},{"version":"CommandV1","origId":204722,"guid":"95394040-bded-43f3-ac93-0d912e2580d0","subtype":"command","commandType":"auto","position":19.5,"command":"%md\nNow let us use the same code for making a streaming dataset.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"626b4834-44a7-4e96-a42d-8ba0e4d2666e"},{"version":"CommandV1","origId":204723,"guid":"a7b4ad0a-ecc9-48cf-9ef3-9b8daf113e3e","subtype":"command","commandType":"auto","position":19.75,"command":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport java.sql.Timestamp\n\n// create a case class to make the datset\ncase class timedScores(time: Timestamp, score: Double)\n\nval csvStreamingDS = csvStreamingDF.as[timedScores] // create a dataset from the dataframe\n\n// Start running the query that prints the running counts to the console\nval query = csvStreamingDS\n                  // bround simply rounds the double to the desired decimal place - 0 in our case here. \n                   // see https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html#bround-org.apache.spark.sql.Column-\n                   // we are using bround to simply coarsen out data into bins for counts\n                 .select(bround($\"score\", 0).as(\"binnedScore\")) \n                 .groupBy($\"binnedScore\")\n                 .agg(count($\"binnedScore\") as \"binnedScoreCounts\")\n                 .orderBy($\"binnedScore\")\n                 .writeStream\n                 .outputMode(\"complete\")\n                 .format(\"console\")\n                 .start()\n\nquery.awaitTermination() // hit cancel to terminate","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\">-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -1.0|                9|\n|        0.0|               18|\n|        1.0|               41|\n|        2.0|               25|\n|        3.0|                5|\n|        4.0|                1|\n|       10.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----------+-----------------+\n|binnedScore|binnedScoreCounts|\n+-----------+-----------------+\n|       -2.0|                1|\n|       -1.0|               13|\n|        0.0|               44|\n|        1.0|               83|\n|        2.0|               46|\n|        3.0|               10|\n|        4.0|                1|\n|       10.0|                1|\n|       12.0|                1|\n+-----------+-----------------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1511293124469,"submitTime":1511293124469,"finishTime":1511293207710,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e04858bb-97fd-4d97-a14e-276e9f0a6130"},{"version":"CommandV1","origId":204725,"guid":"aa2be271-4a2d-4c11-aaaa-f548986188c3","subtype":"command","commandType":"auto","position":20.5,"command":"%md\n### Window Operations on Event Time\n\nAggregations over a sliding event-time window are straightforward with\nStructured Streaming and are very similar to grouped aggregations. In a\ngrouped aggregation, aggregate values (e.g. counts) are maintained for\neach unique value in the user-specified grouping column. In case of\nwindow-based aggregations, aggregate values are maintained for each\nwindow the event-time of a row falls into. Let’s understand this with an\nillustration.\n\nImagine our **quick example** is\nmodified and the stream now contains lines along with the time when the\nline was generated. Instead of running word counts, we want to count\nwords within 10 minute windows, updating every 5 minutes. That is, word\ncounts in words received between 10 minute windows \n12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20, etc. Note that 12:00 - 12:10 means data that\narrived after 12:00 but before 12:10. Now, consider a word that was\nreceived at 12:07. This word should increment the counts corresponding\nto two windows 12:00 - 12:10 and 12:05 - 12:15. So the counts will be\nindexed by both, the grouping key (i.e. the word) and the window (can be\ncalculated from the event-time).\n\nThe result tables would look something like the following.\n\n![Window\nOperations](https://spark.apache.org/docs/2.2.0/img/structured-streaming-window.png)\n\nSince this windowing is similar to grouping, in code, you can use\n`groupBy()` and `window()` operations to express windowed aggregations.\nYou can see the full code for the below examples in\n[Scala](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala)/[Java](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java)/[Python](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py).\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"20abd809-6ea7-486b-8ab7-0258caae3e12"},{"version":"CommandV1","origId":204726,"guid":"c2959c78-d3ef-4e3c-b308-a091b4becc79","subtype":"command","commandType":"auto","position":20.75,"command":"%md\nMake sure the streaming job with animal names is running (or finished running) with files in `/datasets/streamingFiles` directory - this is the Quick Example in `037a_FilesForStructuredStreaming` notebook.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c3498372-745d-4ab7-97e0-442bc81b4834"},{"version":"CommandV1","origId":204727,"guid":"69738cab-ff2e-4f58-8171-a5b540a3e538","subtype":"command","commandType":"auto","position":20.875,"command":"display(dbutils.fs.ls(\"/datasets/streamingFiles\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/datasets/streamingFiles/00_00.log","00_00.log",35],["dbfs:/datasets/streamingFiles/00_02.log","00_02.log",35],["dbfs:/datasets/streamingFiles/00_04.log","00_04.log",35],["dbfs:/datasets/streamingFiles/00_06.log","00_06.log",35],["dbfs:/datasets/streamingFiles/00_08.log","00_08.log",35],["dbfs:/datasets/streamingFiles/00_10.log","00_10.log",35],["dbfs:/datasets/streamingFiles/00_12.log","00_12.log",35],["dbfs:/datasets/streamingFiles/00_14.log","00_14.log",35],["dbfs:/datasets/streamingFiles/00_16.log","00_16.log",35],["dbfs:/datasets/streamingFiles/00_18.log","00_18.log",35],["dbfs:/datasets/streamingFiles/00_20.log","00_20.log",35],["dbfs:/datasets/streamingFiles/00_22.log","00_22.log",35],["dbfs:/datasets/streamingFiles/00_24.log","00_24.log",35],["dbfs:/datasets/streamingFiles/00_26.log","00_26.log",35],["dbfs:/datasets/streamingFiles/00_28.log","00_28.log",35],["dbfs:/datasets/streamingFiles/00_30.log","00_30.log",35],["dbfs:/datasets/streamingFiles/00_32.log","00_32.log",35],["dbfs:/datasets/streamingFiles/00_34.log","00_34.log",35],["dbfs:/datasets/streamingFiles/00_37.log","00_37.log",35],["dbfs:/datasets/streamingFiles/00_39.log","00_39.log",35],["dbfs:/datasets/streamingFiles/00_41.log","00_41.log",35],["dbfs:/datasets/streamingFiles/00_43.log","00_43.log",35],["dbfs:/datasets/streamingFiles/00_45.log","00_45.log",35],["dbfs:/datasets/streamingFiles/00_47.log","00_47.log",35],["dbfs:/datasets/streamingFiles/00_49.log","00_49.log",35],["dbfs:/datasets/streamingFiles/00_51.log","00_51.log",35],["dbfs:/datasets/streamingFiles/00_53.log","00_53.log",35],["dbfs:/datasets/streamingFiles/00_55.log","00_55.log",35],["dbfs:/datasets/streamingFiles/00_57.log","00_57.log",35],["dbfs:/datasets/streamingFiles/00_59.log","00_59.log",35],["dbfs:/datasets/streamingFiles/01_01.log","01_01.log",35],["dbfs:/datasets/streamingFiles/01_03.log","01_03.log",35],["dbfs:/datasets/streamingFiles/01_05.log","01_05.log",35],["dbfs:/datasets/streamingFiles/01_07.log","01_07.log",35],["dbfs:/datasets/streamingFiles/01_09.log","01_09.log",35],["dbfs:/datasets/streamingFiles/01_11.log","01_11.log",35],["dbfs:/datasets/streamingFiles/01_13.log","01_13.log",35],["dbfs:/datasets/streamingFiles/01_15.log","01_15.log",35],["dbfs:/datasets/streamingFiles/01_17.log","01_17.log",35],["dbfs:/datasets/streamingFiles/01_19.log","01_19.log",35],["dbfs:/datasets/streamingFiles/01_21.log","01_21.log",35],["dbfs:/datasets/streamingFiles/01_23.log","01_23.log",35],["dbfs:/datasets/streamingFiles/01_25.log","01_25.log",35],["dbfs:/datasets/streamingFiles/01_27.log","01_27.log",35],["dbfs:/datasets/streamingFiles/01_29.log","01_29.log",35],["dbfs:/datasets/streamingFiles/01_31.log","01_31.log",35],["dbfs:/datasets/streamingFiles/01_33.log","01_33.log",35],["dbfs:/datasets/streamingFiles/01_35.log","01_35.log",35],["dbfs:/datasets/streamingFiles/01_37.log","01_37.log",35],["dbfs:/datasets/streamingFiles/01_39.log","01_39.log",35],["dbfs:/datasets/streamingFiles/01_41.log","01_41.log",35],["dbfs:/datasets/streamingFiles/01_43.log","01_43.log",35],["dbfs:/datasets/streamingFiles/01_45.log","01_45.log",35],["dbfs:/datasets/streamingFiles/01_47.log","01_47.log",35],["dbfs:/datasets/streamingFiles/01_49.log","01_49.log",35],["dbfs:/datasets/streamingFiles/01_51.log","01_51.log",35],["dbfs:/datasets/streamingFiles/01_53.log","01_53.log",35],["dbfs:/datasets/streamingFiles/01_55.log","01_55.log",35],["dbfs:/datasets/streamingFiles/01_57.log","01_57.log",35],["dbfs:/datasets/streamingFiles/01_59.log","01_59.log",35],["dbfs:/datasets/streamingFiles/02_01.log","02_01.log",35],["dbfs:/datasets/streamingFiles/02_03.log","02_03.log",35],["dbfs:/datasets/streamingFiles/02_05.log","02_05.log",35],["dbfs:/datasets/streamingFiles/02_07.log","02_07.log",35],["dbfs:/datasets/streamingFiles/02_09.log","02_09.log",35],["dbfs:/datasets/streamingFiles/02_11.log","02_11.log",35],["dbfs:/datasets/streamingFiles/02_13.log","02_13.log",35],["dbfs:/datasets/streamingFiles/02_15.log","02_15.log",35],["dbfs:/datasets/streamingFiles/02_17.log","02_17.log",35],["dbfs:/datasets/streamingFiles/02_19.log","02_19.log",35],["dbfs:/datasets/streamingFiles/02_21.log","02_21.log",35],["dbfs:/datasets/streamingFiles/02_23.log","02_23.log",35],["dbfs:/datasets/streamingFiles/02_25.log","02_25.log",35],["dbfs:/datasets/streamingFiles/02_27.log","02_27.log",35],["dbfs:/datasets/streamingFiles/02_29.log","02_29.log",35],["dbfs:/datasets/streamingFiles/02_31.log","02_31.log",35],["dbfs:/datasets/streamingFiles/02_33.log","02_33.log",35],["dbfs:/datasets/streamingFiles/02_35.log","02_35.log",35],["dbfs:/datasets/streamingFiles/02_37.log","02_37.log",35],["dbfs:/datasets/streamingFiles/02_39.log","02_39.log",35],["dbfs:/datasets/streamingFiles/02_41.log","02_41.log",35],["dbfs:/datasets/streamingFiles/02_43.log","02_43.log",35],["dbfs:/datasets/streamingFiles/02_45.log","02_45.log",35],["dbfs:/datasets/streamingFiles/02_47.log","02_47.log",35],["dbfs:/datasets/streamingFiles/02_49.log","02_49.log",35],["dbfs:/datasets/streamingFiles/02_51.log","02_51.log",35],["dbfs:/datasets/streamingFiles/02_53.log","02_53.log",35],["dbfs:/datasets/streamingFiles/02_55.log","02_55.log",35],["dbfs:/datasets/streamingFiles/02_57.log","02_57.log",35],["dbfs:/datasets/streamingFiles/02_59.log","02_59.log",35],["dbfs:/datasets/streamingFiles/03_01.log","03_01.log",35],["dbfs:/datasets/streamingFiles/03_03.log","03_03.log",35],["dbfs:/datasets/streamingFiles/03_06.log","03_06.log",35],["dbfs:/datasets/streamingFiles/03_08.log","03_08.log",35],["dbfs:/datasets/streamingFiles/03_10.log","03_10.log",35],["dbfs:/datasets/streamingFiles/03_12.log","03_12.log",35],["dbfs:/datasets/streamingFiles/03_14.log","03_14.log",35],["dbfs:/datasets/streamingFiles/03_16.log","03_16.log",35],["dbfs:/datasets/streamingFiles/03_18.log","03_18.log",35],["dbfs:/datasets/streamingFiles/03_20.log","03_20.log",35],["dbfs:/datasets/streamingFiles/03_22.log","03_22.log",35],["dbfs:/datasets/streamingFiles/03_24.log","03_24.log",35],["dbfs:/datasets/streamingFiles/03_26.log","03_26.log",35],["dbfs:/datasets/streamingFiles/03_28.log","03_28.log",35],["dbfs:/datasets/streamingFiles/03_30.log","03_30.log",35],["dbfs:/datasets/streamingFiles/03_32.log","03_32.log",35],["dbfs:/datasets/streamingFiles/03_34.log","03_34.log",35],["dbfs:/datasets/streamingFiles/03_36.log","03_36.log",35],["dbfs:/datasets/streamingFiles/03_38.log","03_38.log",35],["dbfs:/datasets/streamingFiles/03_40.log","03_40.log",35],["dbfs:/datasets/streamingFiles/03_42.log","03_42.log",35],["dbfs:/datasets/streamingFiles/03_44.log","03_44.log",35],["dbfs:/datasets/streamingFiles/03_46.log","03_46.log",35],["dbfs:/datasets/streamingFiles/03_48.log","03_48.log",35],["dbfs:/datasets/streamingFiles/03_50.log","03_50.log",35],["dbfs:/datasets/streamingFiles/03_52.log","03_52.log",35],["dbfs:/datasets/streamingFiles/03_54.log","03_54.log",35],["dbfs:/datasets/streamingFiles/03_56.log","03_56.log",35],["dbfs:/datasets/streamingFiles/03_58.log","03_58.log",35],["dbfs:/datasets/streamingFiles/04_00.log","04_00.log",35],["dbfs:/datasets/streamingFiles/04_02.log","04_02.log",35],["dbfs:/datasets/streamingFiles/04_04.log","04_04.log",35],["dbfs:/datasets/streamingFiles/04_06.log","04_06.log",35],["dbfs:/datasets/streamingFiles/04_08.log","04_08.log",35],["dbfs:/datasets/streamingFiles/04_10.log","04_10.log",35],["dbfs:/datasets/streamingFiles/04_12.log","04_12.log",35],["dbfs:/datasets/streamingFiles/04_14.log","04_14.log",35],["dbfs:/datasets/streamingFiles/04_16.log","04_16.log",35],["dbfs:/datasets/streamingFiles/04_18.log","04_18.log",35],["dbfs:/datasets/streamingFiles/04_20.log","04_20.log",35],["dbfs:/datasets/streamingFiles/04_22.log","04_22.log",35],["dbfs:/datasets/streamingFiles/04_24.log","04_24.log",35],["dbfs:/datasets/streamingFiles/04_26.log","04_26.log",35],["dbfs:/datasets/streamingFiles/04_28.log","04_28.log",35],["dbfs:/datasets/streamingFiles/04_30.log","04_30.log",35],["dbfs:/datasets/streamingFiles/04_32.log","04_32.log",35],["dbfs:/datasets/streamingFiles/04_34.log","04_34.log",35],["dbfs:/datasets/streamingFiles/04_36.log","04_36.log",35],["dbfs:/datasets/streamingFiles/04_38.log","04_38.log",35],["dbfs:/datasets/streamingFiles/04_40.log","04_40.log",35],["dbfs:/datasets/streamingFiles/04_42.log","04_42.log",35],["dbfs:/datasets/streamingFiles/04_44.log","04_44.log",35],["dbfs:/datasets/streamingFiles/04_46.log","04_46.log",35],["dbfs:/datasets/streamingFiles/04_48.log","04_48.log",35],["dbfs:/datasets/streamingFiles/04_50.log","04_50.log",35],["dbfs:/datasets/streamingFiles/04_52.log","04_52.log",35],["dbfs:/datasets/streamingFiles/04_54.log","04_54.log",35],["dbfs:/datasets/streamingFiles/04_56.log","04_56.log",35],["dbfs:/datasets/streamingFiles/04_58.log","04_58.log",35],["dbfs:/datasets/streamingFiles/05_00.log","05_00.log",35],["dbfs:/datasets/streamingFiles/05_02.log","05_02.log",35],["dbfs:/datasets/streamingFiles/05_04.log","05_04.log",35],["dbfs:/datasets/streamingFiles/05_06.log","05_06.log",35],["dbfs:/datasets/streamingFiles/05_08.log","05_08.log",35],["dbfs:/datasets/streamingFiles/05_10.log","05_10.log",35],["dbfs:/datasets/streamingFiles/05_12.log","05_12.log",35],["dbfs:/datasets/streamingFiles/05_14.log","05_14.log",35],["dbfs:/datasets/streamingFiles/05_16.log","05_16.log",35],["dbfs:/datasets/streamingFiles/05_18.log","05_18.log",35],["dbfs:/datasets/streamingFiles/05_20.log","05_20.log",35],["dbfs:/datasets/streamingFiles/05_22.log","05_22.log",35],["dbfs:/datasets/streamingFiles/05_24.log","05_24.log",35],["dbfs:/datasets/streamingFiles/05_26.log","05_26.log",35],["dbfs:/datasets/streamingFiles/05_28.log","05_28.log",35],["dbfs:/datasets/streamingFiles/05_30.log","05_30.log",35],["dbfs:/datasets/streamingFiles/05_32.log","05_32.log",35],["dbfs:/datasets/streamingFiles/05_34.log","05_34.log",35],["dbfs:/datasets/streamingFiles/05_36.log","05_36.log",35],["dbfs:/datasets/streamingFiles/05_38.log","05_38.log",35],["dbfs:/datasets/streamingFiles/05_40.log","05_40.log",35],["dbfs:/datasets/streamingFiles/05_42.log","05_42.log",35],["dbfs:/datasets/streamingFiles/05_44.log","05_44.log",35],["dbfs:/datasets/streamingFiles/05_46.log","05_46.log",35],["dbfs:/datasets/streamingFiles/05_48.log","05_48.log",35],["dbfs:/datasets/streamingFiles/05_50.log","05_50.log",35],["dbfs:/datasets/streamingFiles/05_52.log","05_52.log",35],["dbfs:/datasets/streamingFiles/05_54.log","05_54.log",35],["dbfs:/datasets/streamingFiles/05_56.log","05_56.log",35],["dbfs:/datasets/streamingFiles/05_58.log","05_58.log",35],["dbfs:/datasets/streamingFiles/06_00.log","06_00.log",35],["dbfs:/datasets/streamingFiles/06_02.log","06_02.log",35],["dbfs:/datasets/streamingFiles/06_04.log","06_04.log",35],["dbfs:/datasets/streamingFiles/06_06.log","06_06.log",35],["dbfs:/datasets/streamingFiles/06_08.log","06_08.log",35],["dbfs:/datasets/streamingFiles/06_10.log","06_10.log",35],["dbfs:/datasets/streamingFiles/06_12.log","06_12.log",35],["dbfs:/datasets/streamingFiles/06_15.log","06_15.log",35],["dbfs:/datasets/streamingFiles/06_17.log","06_17.log",35],["dbfs:/datasets/streamingFiles/06_19.log","06_19.log",35],["dbfs:/datasets/streamingFiles/06_21.log","06_21.log",35],["dbfs:/datasets/streamingFiles/06_23.log","06_23.log",35],["dbfs:/datasets/streamingFiles/06_25.log","06_25.log",35],["dbfs:/datasets/streamingFiles/06_27.log","06_27.log",35],["dbfs:/datasets/streamingFiles/06_29.log","06_29.log",35],["dbfs:/datasets/streamingFiles/06_31.log","06_31.log",35],["dbfs:/datasets/streamingFiles/06_33.log","06_33.log",35],["dbfs:/datasets/streamingFiles/06_35.log","06_35.log",35],["dbfs:/datasets/streamingFiles/06_37.log","06_37.log",35],["dbfs:/datasets/streamingFiles/06_39.log","06_39.log",35],["dbfs:/datasets/streamingFiles/06_41.log","06_41.log",35],["dbfs:/datasets/streamingFiles/06_43.log","06_43.log",35],["dbfs:/datasets/streamingFiles/06_45.log","06_45.log",35],["dbfs:/datasets/streamingFiles/06_47.log","06_47.log",35],["dbfs:/datasets/streamingFiles/06_49.log","06_49.log",35],["dbfs:/datasets/streamingFiles/06_51.log","06_51.log",35],["dbfs:/datasets/streamingFiles/06_53.log","06_53.log",35],["dbfs:/datasets/streamingFiles/06_55.log","06_55.log",35],["dbfs:/datasets/streamingFiles/06_57.log","06_57.log",35],["dbfs:/datasets/streamingFiles/06_59.log","06_59.log",35],["dbfs:/datasets/streamingFiles/07_01.log","07_01.log",35],["dbfs:/datasets/streamingFiles/07_03.log","07_03.log",35],["dbfs:/datasets/streamingFiles/07_05.log","07_05.log",35],["dbfs:/datasets/streamingFiles/07_07.log","07_07.log",35],["dbfs:/datasets/streamingFiles/07_09.log","07_09.log",35],["dbfs:/datasets/streamingFiles/07_11.log","07_11.log",35],["dbfs:/datasets/streamingFiles/07_13.log","07_13.log",35],["dbfs:/datasets/streamingFiles/07_15.log","07_15.log",35],["dbfs:/datasets/streamingFiles/07_17.log","07_17.log",35],["dbfs:/datasets/streamingFiles/07_19.log","07_19.log",35],["dbfs:/datasets/streamingFiles/07_21.log","07_21.log",35],["dbfs:/datasets/streamingFiles/07_23.log","07_23.log",35],["dbfs:/datasets/streamingFiles/07_25.log","07_25.log",35],["dbfs:/datasets/streamingFiles/07_27.log","07_27.log",35],["dbfs:/datasets/streamingFiles/07_29.log","07_29.log",35],["dbfs:/datasets/streamingFiles/07_31.log","07_31.log",35],["dbfs:/datasets/streamingFiles/07_33.log","07_33.log",35],["dbfs:/datasets/streamingFiles/07_35.log","07_35.log",35],["dbfs:/datasets/streamingFiles/07_37.log","07_37.log",35],["dbfs:/datasets/streamingFiles/07_39.log","07_39.log",35],["dbfs:/datasets/streamingFiles/07_41.log","07_41.log",35],["dbfs:/datasets/streamingFiles/07_43.log","07_43.log",35],["dbfs:/datasets/streamingFiles/07_45.log","07_45.log",35],["dbfs:/datasets/streamingFiles/07_47.log","07_47.log",35],["dbfs:/datasets/streamingFiles/07_49.log","07_49.log",35],["dbfs:/datasets/streamingFiles/07_51.log","07_51.log",35],["dbfs:/datasets/streamingFiles/07_53.log","07_53.log",35],["dbfs:/datasets/streamingFiles/07_55.log","07_55.log",35],["dbfs:/datasets/streamingFiles/07_57.log","07_57.log",35],["dbfs:/datasets/streamingFiles/07_59.log","07_59.log",35],["dbfs:/datasets/streamingFiles/08_01.log","08_01.log",35],["dbfs:/datasets/streamingFiles/08_03.log","08_03.log",35],["dbfs:/datasets/streamingFiles/08_05.log","08_05.log",35],["dbfs:/datasets/streamingFiles/08_07.log","08_07.log",35],["dbfs:/datasets/streamingFiles/08_09.log","08_09.log",35],["dbfs:/datasets/streamingFiles/08_11.log","08_11.log",35],["dbfs:/datasets/streamingFiles/08_13.log","08_13.log",35],["dbfs:/datasets/streamingFiles/08_15.log","08_15.log",35],["dbfs:/datasets/streamingFiles/08_17.log","08_17.log",35],["dbfs:/datasets/streamingFiles/08_19.log","08_19.log",35],["dbfs:/datasets/streamingFiles/08_21.log","08_21.log",35],["dbfs:/datasets/streamingFiles/08_23.log","08_23.log",35],["dbfs:/datasets/streamingFiles/08_25.log","08_25.log",35],["dbfs:/datasets/streamingFiles/08_27.log","08_27.log",35],["dbfs:/datasets/streamingFiles/08_29.log","08_29.log",35],["dbfs:/datasets/streamingFiles/08_31.log","08_31.log",35],["dbfs:/datasets/streamingFiles/08_33.log","08_33.log",35],["dbfs:/datasets/streamingFiles/08_36.log","08_36.log",35],["dbfs:/datasets/streamingFiles/08_38.log","08_38.log",35],["dbfs:/datasets/streamingFiles/08_40.log","08_40.log",35],["dbfs:/datasets/streamingFiles/08_42.log","08_42.log",35],["dbfs:/datasets/streamingFiles/08_44.log","08_44.log",35],["dbfs:/datasets/streamingFiles/08_46.log","08_46.log",35],["dbfs:/datasets/streamingFiles/08_48.log","08_48.log",35],["dbfs:/datasets/streamingFiles/08_50.log","08_50.log",35],["dbfs:/datasets/streamingFiles/08_52.log","08_52.log",35],["dbfs:/datasets/streamingFiles/08_54.log","08_54.log",35],["dbfs:/datasets/streamingFiles/08_56.log","08_56.log",35],["dbfs:/datasets/streamingFiles/08_58.log","08_58.log",35],["dbfs:/datasets/streamingFiles/09_00.log","09_00.log",35],["dbfs:/datasets/streamingFiles/09_02.log","09_02.log",35],["dbfs:/datasets/streamingFiles/09_04.log","09_04.log",35],["dbfs:/datasets/streamingFiles/09_06.log","09_06.log",35],["dbfs:/datasets/streamingFiles/09_08.log","09_08.log",35],["dbfs:/datasets/streamingFiles/09_10.log","09_10.log",35],["dbfs:/datasets/streamingFiles/09_12.log","09_12.log",35],["dbfs:/datasets/streamingFiles/09_14.log","09_14.log",35],["dbfs:/datasets/streamingFiles/09_16.log","09_16.log",35],["dbfs:/datasets/streamingFiles/09_18.log","09_18.log",35],["dbfs:/datasets/streamingFiles/09_20.log","09_20.log",35],["dbfs:/datasets/streamingFiles/09_22.log","09_22.log",35],["dbfs:/datasets/streamingFiles/09_24.log","09_24.log",35],["dbfs:/datasets/streamingFiles/09_26.log","09_26.log",35],["dbfs:/datasets/streamingFiles/09_28.log","09_28.log",35],["dbfs:/datasets/streamingFiles/09_30.log","09_30.log",35],["dbfs:/datasets/streamingFiles/09_32.log","09_32.log",35],["dbfs:/datasets/streamingFiles/09_34.log","09_34.log",35],["dbfs:/datasets/streamingFiles/09_36.log","09_36.log",35],["dbfs:/datasets/streamingFiles/09_38.log","09_38.log",35],["dbfs:/datasets/streamingFiles/09_40.log","09_40.log",35],["dbfs:/datasets/streamingFiles/09_42.log","09_42.log",35],["dbfs:/datasets/streamingFiles/09_44.log","09_44.log",35],["dbfs:/datasets/streamingFiles/09_46.log","09_46.log",35],["dbfs:/datasets/streamingFiles/09_48.log","09_48.log",35],["dbfs:/datasets/streamingFiles/09_50.log","09_50.log",35],["dbfs:/datasets/streamingFiles/09_52.log","09_52.log",35],["dbfs:/datasets/streamingFiles/09_54.log","09_54.log",35],["dbfs:/datasets/streamingFiles/09_56.log","09_56.log",35],["dbfs:/datasets/streamingFiles/09_58.log","09_58.log",35],["dbfs:/datasets/streamingFiles/10_00.log","10_00.log",35],["dbfs:/datasets/streamingFiles/10_02.log","10_02.log",35],["dbfs:/datasets/streamingFiles/10_04.log","10_04.log",35],["dbfs:/datasets/streamingFiles/10_06.log","10_06.log",35],["dbfs:/datasets/streamingFiles/10_08.log","10_08.log",35],["dbfs:/datasets/streamingFiles/10_10.log","10_10.log",35],["dbfs:/datasets/streamingFiles/10_12.log","10_12.log",35],["dbfs:/datasets/streamingFiles/10_14.log","10_14.log",35],["dbfs:/datasets/streamingFiles/10_16.log","10_16.log",35],["dbfs:/datasets/streamingFiles/10_18.log","10_18.log",35],["dbfs:/datasets/streamingFiles/10_20.log","10_20.log",35],["dbfs:/datasets/streamingFiles/10_22.log","10_22.log",35],["dbfs:/datasets/streamingFiles/10_24.log","10_24.log",35],["dbfs:/datasets/streamingFiles/10_26.log","10_26.log",35],["dbfs:/datasets/streamingFiles/10_28.log","10_28.log",35],["dbfs:/datasets/streamingFiles/10_30.log","10_30.log",35],["dbfs:/datasets/streamingFiles/10_32.log","10_32.log",35],["dbfs:/datasets/streamingFiles/10_34.log","10_34.log",35],["dbfs:/datasets/streamingFiles/10_36.log","10_36.log",35],["dbfs:/datasets/streamingFiles/10_38.log","10_38.log",35],["dbfs:/datasets/streamingFiles/10_40.log","10_40.log",35],["dbfs:/datasets/streamingFiles/10_42.log","10_42.log",35],["dbfs:/datasets/streamingFiles/10_44.log","10_44.log",35],["dbfs:/datasets/streamingFiles/10_46.log","10_46.log",35],["dbfs:/datasets/streamingFiles/10_48.log","10_48.log",35],["dbfs:/datasets/streamingFiles/10_50.log","10_50.log",35],["dbfs:/datasets/streamingFiles/10_52.log","10_52.log",35],["dbfs:/datasets/streamingFiles/10_54.log","10_54.log",35],["dbfs:/datasets/streamingFiles/10_56.log","10_56.log",35],["dbfs:/datasets/streamingFiles/10_58.log","10_58.log",35],["dbfs:/datasets/streamingFiles/11_00.log","11_00.log",35],["dbfs:/datasets/streamingFiles/11_02.log","11_02.log",35],["dbfs:/datasets/streamingFiles/11_04.log","11_04.log",35],["dbfs:/datasets/streamingFiles/11_06.log","11_06.log",35],["dbfs:/datasets/streamingFiles/11_09.log","11_09.log",35],["dbfs:/datasets/streamingFiles/11_11.log","11_11.log",35],["dbfs:/datasets/streamingFiles/11_13.log","11_13.log",35],["dbfs:/datasets/streamingFiles/11_15.log","11_15.log",35],["dbfs:/datasets/streamingFiles/11_17.log","11_17.log",35],["dbfs:/datasets/streamingFiles/11_19.log","11_19.log",35],["dbfs:/datasets/streamingFiles/11_21.log","11_21.log",35],["dbfs:/datasets/streamingFiles/11_23.log","11_23.log",35],["dbfs:/datasets/streamingFiles/11_25.log","11_25.log",35],["dbfs:/datasets/streamingFiles/11_27.log","11_27.log",35],["dbfs:/datasets/streamingFiles/11_29.log","11_29.log",35],["dbfs:/datasets/streamingFiles/11_31.log","11_31.log",35],["dbfs:/datasets/streamingFiles/11_33.log","11_33.log",35],["dbfs:/datasets/streamingFiles/11_35.log","11_35.log",35],["dbfs:/datasets/streamingFiles/11_37.log","11_37.log",35],["dbfs:/datasets/streamingFiles/11_39.log","11_39.log",35],["dbfs:/datasets/streamingFiles/11_41.log","11_41.log",35],["dbfs:/datasets/streamingFiles/11_43.log","11_43.log",35],["dbfs:/datasets/streamingFiles/11_45.log","11_45.log",35],["dbfs:/datasets/streamingFiles/11_47.log","11_47.log",35],["dbfs:/datasets/streamingFiles/11_49.log","11_49.log",35],["dbfs:/datasets/streamingFiles/11_51.log","11_51.log",35],["dbfs:/datasets/streamingFiles/11_53.log","11_53.log",35],["dbfs:/datasets/streamingFiles/11_55.log","11_55.log",35],["dbfs:/datasets/streamingFiles/11_57.log","11_57.log",35],["dbfs:/datasets/streamingFiles/11_59.log","11_59.log",35],["dbfs:/datasets/streamingFiles/12_01.log","12_01.log",35],["dbfs:/datasets/streamingFiles/12_03.log","12_03.log",35],["dbfs:/datasets/streamingFiles/12_05.log","12_05.log",35],["dbfs:/datasets/streamingFiles/12_07.log","12_07.log",35],["dbfs:/datasets/streamingFiles/12_09.log","12_09.log",35],["dbfs:/datasets/streamingFiles/12_11.log","12_11.log",35],["dbfs:/datasets/streamingFiles/12_13.log","12_13.log",35],["dbfs:/datasets/streamingFiles/12_15.log","12_15.log",35],["dbfs:/datasets/streamingFiles/12_17.log","12_17.log",35],["dbfs:/datasets/streamingFiles/12_19.log","12_19.log",35],["dbfs:/datasets/streamingFiles/12_21.log","12_21.log",35],["dbfs:/datasets/streamingFiles/12_23.log","12_23.log",35],["dbfs:/datasets/streamingFiles/12_25.log","12_25.log",35],["dbfs:/datasets/streamingFiles/12_27.log","12_27.log",35],["dbfs:/datasets/streamingFiles/12_29.log","12_29.log",35],["dbfs:/datasets/streamingFiles/12_31.log","12_31.log",35],["dbfs:/datasets/streamingFiles/12_33.log","12_33.log",35],["dbfs:/datasets/streamingFiles/12_35.log","12_35.log",35],["dbfs:/datasets/streamingFiles/12_37.log","12_37.log",35],["dbfs:/datasets/streamingFiles/12_39.log","12_39.log",35],["dbfs:/datasets/streamingFiles/12_41.log","12_41.log",35],["dbfs:/datasets/streamingFiles/12_43.log","12_43.log",35],["dbfs:/datasets/streamingFiles/12_45.log","12_45.log",35],["dbfs:/datasets/streamingFiles/12_47.log","12_47.log",35],["dbfs:/datasets/streamingFiles/12_49.log","12_49.log",35],["dbfs:/datasets/streamingFiles/12_51.log","12_51.log",35],["dbfs:/datasets/streamingFiles/12_53.log","12_53.log",35],["dbfs:/datasets/streamingFiles/12_55.log","12_55.log",35],["dbfs:/datasets/streamingFiles/12_57.log","12_57.log",35],["dbfs:/datasets/streamingFiles/12_59.log","12_59.log",35],["dbfs:/datasets/streamingFiles/13_01.log","13_01.log",35],["dbfs:/datasets/streamingFiles/13_03.log","13_03.log",35],["dbfs:/datasets/streamingFiles/13_05.log","13_05.log",35],["dbfs:/datasets/streamingFiles/13_07.log","13_07.log",35],["dbfs:/datasets/streamingFiles/13_09.log","13_09.log",35],["dbfs:/datasets/streamingFiles/13_11.log","13_11.log",35],["dbfs:/datasets/streamingFiles/13_13.log","13_13.log",35],["dbfs:/datasets/streamingFiles/13_15.log","13_15.log",35],["dbfs:/datasets/streamingFiles/13_17.log","13_17.log",35],["dbfs:/datasets/streamingFiles/13_19.log","13_19.log",35],["dbfs:/datasets/streamingFiles/13_21.log","13_21.log",35],["dbfs:/datasets/streamingFiles/13_23.log","13_23.log",35],["dbfs:/datasets/streamingFiles/13_25.log","13_25.log",35],["dbfs:/datasets/streamingFiles/13_27.log","13_27.log",35],["dbfs:/datasets/streamingFiles/13_29.log","13_29.log",35],["dbfs:/datasets/streamingFiles/13_31.log","13_31.log",35],["dbfs:/datasets/streamingFiles/13_33.log","13_33.log",35],["dbfs:/datasets/streamingFiles/13_35.log","13_35.log",35],["dbfs:/datasets/streamingFiles/13_37.log","13_37.log",35],["dbfs:/datasets/streamingFiles/13_39.log","13_39.log",35],["dbfs:/datasets/streamingFiles/13_41.log","13_41.log",35],["dbfs:/datasets/streamingFiles/13_43.log","13_43.log",35],["dbfs:/datasets/streamingFiles/13_45.log","13_45.log",35],["dbfs:/datasets/streamingFiles/13_47.log","13_47.log",35],["dbfs:/datasets/streamingFiles/13_49.log","13_49.log",35],["dbfs:/datasets/streamingFiles/13_51.log","13_51.log",35],["dbfs:/datasets/streamingFiles/13_53.log","13_53.log",35],["dbfs:/datasets/streamingFiles/13_55.log","13_55.log",35],["dbfs:/datasets/streamingFiles/13_57.log","13_57.log",35],["dbfs:/datasets/streamingFiles/13_59.log","13_59.log",35],["dbfs:/datasets/streamingFiles/14_01.log","14_01.log",35],["dbfs:/datasets/streamingFiles/14_03.log","14_03.log",35],["dbfs:/datasets/streamingFiles/14_06.log","14_06.log",35],["dbfs:/datasets/streamingFiles/14_08.log","14_08.log",35],["dbfs:/datasets/streamingFiles/14_10.log","14_10.log",35],["dbfs:/datasets/streamingFiles/14_12.log","14_12.log",35],["dbfs:/datasets/streamingFiles/14_14.log","14_14.log",35],["dbfs:/datasets/streamingFiles/14_16.log","14_16.log",35],["dbfs:/datasets/streamingFiles/14_18.log","14_18.log",35],["dbfs:/datasets/streamingFiles/14_20.log","14_20.log",35],["dbfs:/datasets/streamingFiles/14_22.log","14_22.log",35],["dbfs:/datasets/streamingFiles/14_24.log","14_24.log",35],["dbfs:/datasets/streamingFiles/14_26.log","14_26.log",35],["dbfs:/datasets/streamingFiles/14_28.log","14_28.log",35],["dbfs:/datasets/streamingFiles/14_30.log","14_30.log",35],["dbfs:/datasets/streamingFiles/14_32.log","14_32.log",35],["dbfs:/datasets/streamingFiles/14_34.log","14_34.log",35],["dbfs:/datasets/streamingFiles/14_36.log","14_36.log",35],["dbfs:/datasets/streamingFiles/14_38.log","14_38.log",35],["dbfs:/datasets/streamingFiles/14_40.log","14_40.log",35],["dbfs:/datasets/streamingFiles/14_42.log","14_42.log",35],["dbfs:/datasets/streamingFiles/14_44.log","14_44.log",35],["dbfs:/datasets/streamingFiles/14_46.log","14_46.log",35],["dbfs:/datasets/streamingFiles/14_48.log","14_48.log",35],["dbfs:/datasets/streamingFiles/14_50.log","14_50.log",35],["dbfs:/datasets/streamingFiles/14_52.log","14_52.log",35],["dbfs:/datasets/streamingFiles/14_54.log","14_54.log",35],["dbfs:/datasets/streamingFiles/14_56.log","14_56.log",35],["dbfs:/datasets/streamingFiles/14_58.log","14_58.log",35],["dbfs:/datasets/streamingFiles/15_00.log","15_00.log",35],["dbfs:/datasets/streamingFiles/15_02.log","15_02.log",35],["dbfs:/datasets/streamingFiles/15_04.log","15_04.log",35],["dbfs:/datasets/streamingFiles/15_06.log","15_06.log",35],["dbfs:/datasets/streamingFiles/15_08.log","15_08.log",35],["dbfs:/datasets/streamingFiles/15_10.log","15_10.log",35],["dbfs:/datasets/streamingFiles/15_12.log","15_12.log",35],["dbfs:/datasets/streamingFiles/15_14.log","15_14.log",35],["dbfs:/datasets/streamingFiles/15_16.log","15_16.log",35],["dbfs:/datasets/streamingFiles/15_18.log","15_18.log",35],["dbfs:/datasets/streamingFiles/15_20.log","15_20.log",35],["dbfs:/datasets/streamingFiles/15_22.log","15_22.log",35],["dbfs:/datasets/streamingFiles/15_24.log","15_24.log",35],["dbfs:/datasets/streamingFiles/15_26.log","15_26.log",35],["dbfs:/datasets/streamingFiles/15_28.log","15_28.log",35],["dbfs:/datasets/streamingFiles/15_30.log","15_30.log",35],["dbfs:/datasets/streamingFiles/15_32.log","15_32.log",35],["dbfs:/datasets/streamingFiles/15_34.log","15_34.log",35],["dbfs:/datasets/streamingFiles/15_36.log","15_36.log",35],["dbfs:/datasets/streamingFiles/15_38.log","15_38.log",35],["dbfs:/datasets/streamingFiles/15_40.log","15_40.log",35],["dbfs:/datasets/streamingFiles/15_42.log","15_42.log",35],["dbfs:/datasets/streamingFiles/15_44.log","15_44.log",35],["dbfs:/datasets/streamingFiles/15_46.log","15_46.log",35],["dbfs:/datasets/streamingFiles/15_48.log","15_48.log",35],["dbfs:/datasets/streamingFiles/15_50.log","15_50.log",35],["dbfs:/datasets/streamingFiles/15_52.log","15_52.log",35],["dbfs:/datasets/streamingFiles/15_54.log","15_54.log",35],["dbfs:/datasets/streamingFiles/15_56.log","15_56.log",35],["dbfs:/datasets/streamingFiles/15_58.log","15_58.log",35],["dbfs:/datasets/streamingFiles/16_00.log","16_00.log",35],["dbfs:/datasets/streamingFiles/16_02.log","16_02.log",35],["dbfs:/datasets/streamingFiles/16_04.log","16_04.log",35],["dbfs:/datasets/streamingFiles/16_06.log","16_06.log",35],["dbfs:/datasets/streamingFiles/16_08.log","16_08.log",35],["dbfs:/datasets/streamingFiles/16_10.log","16_10.log",35],["dbfs:/datasets/streamingFiles/16_12.log","16_12.log",35],["dbfs:/datasets/streamingFiles/16_14.log","16_14.log",35],["dbfs:/datasets/streamingFiles/16_16.log","16_16.log",35],["dbfs:/datasets/streamingFiles/16_18.log","16_18.log",35],["dbfs:/datasets/streamingFiles/16_20.log","16_20.log",35],["dbfs:/datasets/streamingFiles/16_22.log","16_22.log",35],["dbfs:/datasets/streamingFiles/16_24.log","16_24.log",35],["dbfs:/datasets/streamingFiles/16_26.log","16_26.log",35],["dbfs:/datasets/streamingFiles/16_28.log","16_28.log",35],["dbfs:/datasets/streamingFiles/16_30.log","16_30.log",35],["dbfs:/datasets/streamingFiles/16_32.log","16_32.log",35],["dbfs:/datasets/streamingFiles/16_34.log","16_34.log",35],["dbfs:/datasets/streamingFiles/16_36.log","16_36.log",35],["dbfs:/datasets/streamingFiles/16_38.log","16_38.log",35],["dbfs:/datasets/streamingFiles/16_40.log","16_40.log",35],["dbfs:/datasets/streamingFiles/16_42.log","16_42.log",35],["dbfs:/datasets/streamingFiles/16_44.log","16_44.log",35],["dbfs:/datasets/streamingFiles/16_46.log","16_46.log",35],["dbfs:/datasets/streamingFiles/16_48.log","16_48.log",35],["dbfs:/datasets/streamingFiles/16_50.log","16_50.log",35],["dbfs:/datasets/streamingFiles/16_52.log","16_52.log",35],["dbfs:/datasets/streamingFiles/16_54.log","16_54.log",35],["dbfs:/datasets/streamingFiles/16_56.log","16_56.log",35],["dbfs:/datasets/streamingFiles/16_58.log","16_58.log",35],["dbfs:/datasets/streamingFiles/17_00.log","17_00.log",35],["dbfs:/datasets/streamingFiles/17_02.log","17_02.log",35],["dbfs:/datasets/streamingFiles/17_04.log","17_04.log",35],["dbfs:/datasets/streamingFiles/17_06.log","17_06.log",35],["dbfs:/datasets/streamingFiles/17_08.log","17_08.log",35],["dbfs:/datasets/streamingFiles/17_10.log","17_10.log",35],["dbfs:/datasets/streamingFiles/17_12.log","17_12.log",35],["dbfs:/datasets/streamingFiles/17_14.log","17_14.log",35],["dbfs:/datasets/streamingFiles/17_17.log","17_17.log",35],["dbfs:/datasets/streamingFiles/17_19.log","17_19.log",35],["dbfs:/datasets/streamingFiles/17_21.log","17_21.log",35],["dbfs:/datasets/streamingFiles/17_23.log","17_23.log",35],["dbfs:/datasets/streamingFiles/17_25.log","17_25.log",35],["dbfs:/datasets/streamingFiles/17_27.log","17_27.log",35],["dbfs:/datasets/streamingFiles/17_29.log","17_29.log",35],["dbfs:/datasets/streamingFiles/17_31.log","17_31.log",35],["dbfs:/datasets/streamingFiles/17_33.log","17_33.log",35],["dbfs:/datasets/streamingFiles/17_35.log","17_35.log",35],["dbfs:/datasets/streamingFiles/17_37.log","17_37.log",35],["dbfs:/datasets/streamingFiles/17_39.log","17_39.log",35],["dbfs:/datasets/streamingFiles/17_41.log","17_41.log",35],["dbfs:/datasets/streamingFiles/17_43.log","17_43.log",35],["dbfs:/datasets/streamingFiles/17_45.log","17_45.log",35],["dbfs:/datasets/streamingFiles/17_47.log","17_47.log",35],["dbfs:/datasets/streamingFiles/17_49.log","17_49.log",35],["dbfs:/datasets/streamingFiles/17_51.log","17_51.log",35],["dbfs:/datasets/streamingFiles/17_53.log","17_53.log",35],["dbfs:/datasets/streamingFiles/17_55.log","17_55.log",35],["dbfs:/datasets/streamingFiles/17_57.log","17_57.log",35],["dbfs:/datasets/streamingFiles/17_59.log","17_59.log",35],["dbfs:/datasets/streamingFiles/18_01.log","18_01.log",35],["dbfs:/datasets/streamingFiles/18_03.log","18_03.log",35],["dbfs:/datasets/streamingFiles/18_05.log","18_05.log",35],["dbfs:/datasets/streamingFiles/18_07.log","18_07.log",35],["dbfs:/datasets/streamingFiles/18_09.log","18_09.log",35],["dbfs:/datasets/streamingFiles/18_11.log","18_11.log",35],["dbfs:/datasets/streamingFiles/18_13.log","18_13.log",35],["dbfs:/datasets/streamingFiles/18_15.log","18_15.log",35],["dbfs:/datasets/streamingFiles/18_17.log","18_17.log",35],["dbfs:/datasets/streamingFiles/18_19.log","18_19.log",35],["dbfs:/datasets/streamingFiles/18_21.log","18_21.log",35],["dbfs:/datasets/streamingFiles/18_23.log","18_23.log",35],["dbfs:/datasets/streamingFiles/18_25.log","18_25.log",35],["dbfs:/datasets/streamingFiles/18_27.log","18_27.log",35],["dbfs:/datasets/streamingFiles/18_29.log","18_29.log",35],["dbfs:/datasets/streamingFiles/18_31.log","18_31.log",35],["dbfs:/datasets/streamingFiles/18_33.log","18_33.log",35],["dbfs:/datasets/streamingFiles/18_35.log","18_35.log",35],["dbfs:/datasets/streamingFiles/18_37.log","18_37.log",35],["dbfs:/datasets/streamingFiles/18_39.log","18_39.log",35],["dbfs:/datasets/streamingFiles/18_41.log","18_41.log",35],["dbfs:/datasets/streamingFiles/18_43.log","18_43.log",35],["dbfs:/datasets/streamingFiles/18_45.log","18_45.log",35],["dbfs:/datasets/streamingFiles/18_47.log","18_47.log",35],["dbfs:/datasets/streamingFiles/18_49.log","18_49.log",35],["dbfs:/datasets/streamingFiles/18_51.log","18_51.log",35],["dbfs:/datasets/streamingFiles/18_53.log","18_53.log",35],["dbfs:/datasets/streamingFiles/18_55.log","18_55.log",35],["dbfs:/datasets/streamingFiles/18_57.log","18_57.log",35],["dbfs:/datasets/streamingFiles/18_59.log","18_59.log",35],["dbfs:/datasets/streamingFiles/19_01.log","19_01.log",35],["dbfs:/datasets/streamingFiles/19_03.log","19_03.log",35],["dbfs:/datasets/streamingFiles/19_05.log","19_05.log",35],["dbfs:/datasets/streamingFiles/19_07.log","19_07.log",35],["dbfs:/datasets/streamingFiles/19_09.log","19_09.log",35],["dbfs:/datasets/streamingFiles/19_11.log","19_11.log",35],["dbfs:/datasets/streamingFiles/19_13.log","19_13.log",35],["dbfs:/datasets/streamingFiles/19_15.log","19_15.log",35],["dbfs:/datasets/streamingFiles/19_17.log","19_17.log",35],["dbfs:/datasets/streamingFiles/19_19.log","19_19.log",35],["dbfs:/datasets/streamingFiles/19_21.log","19_21.log",35],["dbfs:/datasets/streamingFiles/19_23.log","19_23.log",35],["dbfs:/datasets/streamingFiles/19_25.log","19_25.log",35],["dbfs:/datasets/streamingFiles/19_27.log","19_27.log",35],["dbfs:/datasets/streamingFiles/19_29.log","19_29.log",35],["dbfs:/datasets/streamingFiles/19_31.log","19_31.log",35],["dbfs:/datasets/streamingFiles/19_33.log","19_33.log",35],["dbfs:/datasets/streamingFiles/19_35.log","19_35.log",35],["dbfs:/datasets/streamingFiles/19_37.log","19_37.log",35],["dbfs:/datasets/streamingFiles/19_39.log","19_39.log",35],["dbfs:/datasets/streamingFiles/19_41.log","19_41.log",35],["dbfs:/datasets/streamingFiles/19_43.log","19_43.log",35],["dbfs:/datasets/streamingFiles/19_45.log","19_45.log",35],["dbfs:/datasets/streamingFiles/19_47.log","19_47.log",35],["dbfs:/datasets/streamingFiles/19_49.log","19_49.log",35],["dbfs:/datasets/streamingFiles/19_51.log","19_51.log",35],["dbfs:/datasets/streamingFiles/19_53.log","19_53.log",35],["dbfs:/datasets/streamingFiles/19_55.log","19_55.log",35],["dbfs:/datasets/streamingFiles/19_57.log","19_57.log",35],["dbfs:/datasets/streamingFiles/19_59.log","19_59.log",35],["dbfs:/datasets/streamingFiles/20_01.log","20_01.log",35],["dbfs:/datasets/streamingFiles/20_03.log","20_03.log",35],["dbfs:/datasets/streamingFiles/20_05.log","20_05.log",35],["dbfs:/datasets/streamingFiles/20_08.log","20_08.log",35],["dbfs:/datasets/streamingFiles/20_10.log","20_10.log",35],["dbfs:/datasets/streamingFiles/20_12.log","20_12.log",35],["dbfs:/datasets/streamingFiles/20_14.log","20_14.log",35],["dbfs:/datasets/streamingFiles/20_16.log","20_16.log",35],["dbfs:/datasets/streamingFiles/20_18.log","20_18.log",35],["dbfs:/datasets/streamingFiles/20_20.log","20_20.log",35],["dbfs:/datasets/streamingFiles/20_22.log","20_22.log",35],["dbfs:/datasets/streamingFiles/20_24.log","20_24.log",35],["dbfs:/datasets/streamingFiles/20_26.log","20_26.log",35],["dbfs:/datasets/streamingFiles/20_28.log","20_28.log",35],["dbfs:/datasets/streamingFiles/20_30.log","20_30.log",35],["dbfs:/datasets/streamingFiles/20_32.log","20_32.log",35],["dbfs:/datasets/streamingFiles/20_34.log","20_34.log",35],["dbfs:/datasets/streamingFiles/20_36.log","20_36.log",35],["dbfs:/datasets/streamingFiles/20_38.log","20_38.log",35],["dbfs:/datasets/streamingFiles/20_40.log","20_40.log",35],["dbfs:/datasets/streamingFiles/20_42.log","20_42.log",35],["dbfs:/datasets/streamingFiles/20_44.log","20_44.log",35],["dbfs:/datasets/streamingFiles/20_46.log","20_46.log",35],["dbfs:/datasets/streamingFiles/20_48.log","20_48.log",35],["dbfs:/datasets/streamingFiles/20_50.log","20_50.log",35],["dbfs:/datasets/streamingFiles/20_52.log","20_52.log",35],["dbfs:/datasets/streamingFiles/20_54.log","20_54.log",35],["dbfs:/datasets/streamingFiles/20_56.log","20_56.log",35],["dbfs:/datasets/streamingFiles/20_58.log","20_58.log",35],["dbfs:/datasets/streamingFiles/21_00.log","21_00.log",35],["dbfs:/datasets/streamingFiles/21_02.log","21_02.log",35],["dbfs:/datasets/streamingFiles/21_04.log","21_04.log",35],["dbfs:/datasets/streamingFiles/21_06.log","21_06.log",35],["dbfs:/datasets/streamingFiles/21_08.log","21_08.log",35],["dbfs:/datasets/streamingFiles/21_10.log","21_10.log",35],["dbfs:/datasets/streamingFiles/21_12.log","21_12.log",35],["dbfs:/datasets/streamingFiles/21_14.log","21_14.log",35],["dbfs:/datasets/streamingFiles/21_16.log","21_16.log",35],["dbfs:/datasets/streamingFiles/21_18.log","21_18.log",35],["dbfs:/datasets/streamingFiles/21_20.log","21_20.log",35],["dbfs:/datasets/streamingFiles/21_22.log","21_22.log",35],["dbfs:/datasets/streamingFiles/21_24.log","21_24.log",35],["dbfs:/datasets/streamingFiles/21_26.log","21_26.log",35],["dbfs:/datasets/streamingFiles/21_28.log","21_28.log",35],["dbfs:/datasets/streamingFiles/21_30.log","21_30.log",35],["dbfs:/datasets/streamingFiles/21_32.log","21_32.log",35],["dbfs:/datasets/streamingFiles/21_34.log","21_34.log",35],["dbfs:/datasets/streamingFiles/21_36.log","21_36.log",35],["dbfs:/datasets/streamingFiles/21_38.log","21_38.log",35],["dbfs:/datasets/streamingFiles/21_40.log","21_40.log",35],["dbfs:/datasets/streamingFiles/21_42.log","21_42.log",35],["dbfs:/datasets/streamingFiles/21_44.log","21_44.log",35],["dbfs:/datasets/streamingFiles/21_46.log","21_46.log",35],["dbfs:/datasets/streamingFiles/21_48.log","21_48.log",35],["dbfs:/datasets/streamingFiles/21_50.log","21_50.log",35],["dbfs:/datasets/streamingFiles/21_52.log","21_52.log",35],["dbfs:/datasets/streamingFiles/21_54.log","21_54.log",35],["dbfs:/datasets/streamingFiles/21_56.log","21_56.log",35],["dbfs:/datasets/streamingFiles/21_58.log","21_58.log",35],["dbfs:/datasets/streamingFiles/22_00.log","22_00.log",35],["dbfs:/datasets/streamingFiles/22_02.log","22_02.log",35],["dbfs:/datasets/streamingFiles/22_04.log","22_04.log",35],["dbfs:/datasets/streamingFiles/22_06.log","22_06.log",35],["dbfs:/datasets/streamingFiles/22_08.log","22_08.log",35],["dbfs:/datasets/streamingFiles/22_11.log","22_11.log",35],["dbfs:/datasets/streamingFiles/22_13.log","22_13.log",35],["dbfs:/datasets/streamingFiles/22_15.log","22_15.log",35],["dbfs:/datasets/streamingFiles/22_17.log","22_17.log",35],["dbfs:/datasets/streamingFiles/22_19.log","22_19.log",35],["dbfs:/datasets/streamingFiles/22_21.log","22_21.log",35],["dbfs:/datasets/streamingFiles/22_23.log","22_23.log",35],["dbfs:/datasets/streamingFiles/22_25.log","22_25.log",35],["dbfs:/datasets/streamingFiles/22_27.log","22_27.log",35],["dbfs:/datasets/streamingFiles/22_29.log","22_29.log",35],["dbfs:/datasets/streamingFiles/22_31.log","22_31.log",35],["dbfs:/datasets/streamingFiles/22_33.log","22_33.log",35],["dbfs:/datasets/streamingFiles/22_35.log","22_35.log",35],["dbfs:/datasets/streamingFiles/22_37.log","22_37.log",35],["dbfs:/datasets/streamingFiles/22_39.log","22_39.log",35],["dbfs:/datasets/streamingFiles/22_41.log","22_41.log",35],["dbfs:/datasets/streamingFiles/22_43.log","22_43.log",35],["dbfs:/datasets/streamingFiles/22_45.log","22_45.log",35],["dbfs:/datasets/streamingFiles/22_47.log","22_47.log",35],["dbfs:/datasets/streamingFiles/22_49.log","22_49.log",35],["dbfs:/datasets/streamingFiles/22_51.log","22_51.log",35],["dbfs:/datasets/streamingFiles/22_53.log","22_53.log",35],["dbfs:/datasets/streamingFiles/22_55.log","22_55.log",35],["dbfs:/datasets/streamingFiles/22_57.log","22_57.log",35],["dbfs:/datasets/streamingFiles/22_59.log","22_59.log",35],["dbfs:/datasets/streamingFiles/23_01.log","23_01.log",35],["dbfs:/datasets/streamingFiles/23_03.log","23_03.log",35],["dbfs:/datasets/streamingFiles/23_05.log","23_05.log",35],["dbfs:/datasets/streamingFiles/23_07.log","23_07.log",35],["dbfs:/datasets/streamingFiles/23_09.log","23_09.log",35],["dbfs:/datasets/streamingFiles/23_11.log","23_11.log",35],["dbfs:/datasets/streamingFiles/23_13.log","23_13.log",35],["dbfs:/datasets/streamingFiles/23_15.log","23_15.log",35],["dbfs:/datasets/streamingFiles/23_17.log","23_17.log",35],["dbfs:/datasets/streamingFiles/23_19.log","23_19.log",35],["dbfs:/datasets/streamingFiles/23_21.log","23_21.log",35],["dbfs:/datasets/streamingFiles/23_23.log","23_23.log",35],["dbfs:/datasets/streamingFiles/23_25.log","23_25.log",35],["dbfs:/datasets/streamingFiles/23_27.log","23_27.log",35],["dbfs:/datasets/streamingFiles/23_29.log","23_29.log",35],["dbfs:/datasets/streamingFiles/23_31.log","23_31.log",35],["dbfs:/datasets/streamingFiles/23_33.log","23_33.log",35],["dbfs:/datasets/streamingFiles/23_36.log","23_36.log",35],["dbfs:/datasets/streamingFiles/23_38.log","23_38.log",35],["dbfs:/datasets/streamingFiles/23_40.log","23_40.log",35],["dbfs:/datasets/streamingFiles/23_42.log","23_42.log",35],["dbfs:/datasets/streamingFiles/23_44.log","23_44.log",35],["dbfs:/datasets/streamingFiles/23_46.log","23_46.log",35],["dbfs:/datasets/streamingFiles/23_48.log","23_48.log",35],["dbfs:/datasets/streamingFiles/23_50.log","23_50.log",35],["dbfs:/datasets/streamingFiles/23_52.log","23_52.log",35],["dbfs:/datasets/streamingFiles/23_54.log","23_54.log",35],["dbfs:/datasets/streamingFiles/23_56.log","23_56.log",35],["dbfs:/datasets/streamingFiles/23_58.log","23_58.log",35],["dbfs:/datasets/streamingFiles/24_00.log","24_00.log",35],["dbfs:/datasets/streamingFiles/24_02.log","24_02.log",35],["dbfs:/datasets/streamingFiles/24_04.log","24_04.log",35],["dbfs:/datasets/streamingFiles/24_06.log","24_06.log",35],["dbfs:/datasets/streamingFiles/24_08.log","24_08.log",35],["dbfs:/datasets/streamingFiles/24_10.log","24_10.log",35],["dbfs:/datasets/streamingFiles/24_12.log","24_12.log",35],["dbfs:/datasets/streamingFiles/24_14.log","24_14.log",35],["dbfs:/datasets/streamingFiles/24_16.log","24_16.log",35],["dbfs:/datasets/streamingFiles/24_18.log","24_18.log",35],["dbfs:/datasets/streamingFiles/24_20.log","24_20.log",35],["dbfs:/datasets/streamingFiles/24_22.log","24_22.log",35],["dbfs:/datasets/streamingFiles/24_24.log","24_24.log",35],["dbfs:/datasets/streamingFiles/24_26.log","24_26.log",35],["dbfs:/datasets/streamingFiles/24_28.log","24_28.log",35],["dbfs:/datasets/streamingFiles/24_30.log","24_30.log",35],["dbfs:/datasets/streamingFiles/24_32.log","24_32.log",35],["dbfs:/datasets/streamingFiles/24_34.log","24_34.log",35],["dbfs:/datasets/streamingFiles/24_36.log","24_36.log",35],["dbfs:/datasets/streamingFiles/24_38.log","24_38.log",35],["dbfs:/datasets/streamingFiles/24_40.log","24_40.log",35],["dbfs:/datasets/streamingFiles/24_42.log","24_42.log",35],["dbfs:/datasets/streamingFiles/24_44.log","24_44.log",35],["dbfs:/datasets/streamingFiles/24_46.log","24_46.log",35],["dbfs:/datasets/streamingFiles/24_48.log","24_48.log",35],["dbfs:/datasets/streamingFiles/24_50.log","24_50.log",35],["dbfs:/datasets/streamingFiles/24_52.log","24_52.log",35],["dbfs:/datasets/streamingFiles/24_54.log","24_54.log",35],["dbfs:/datasets/streamingFiles/24_56.log","24_56.log",35],["dbfs:/datasets/streamingFiles/24_58.log","24_58.log",35],["dbfs:/datasets/streamingFiles/25_00.log","25_00.log",35],["dbfs:/datasets/streamingFiles/25_02.log","25_02.log",35],["dbfs:/datasets/streamingFiles/25_04.log","25_04.log",35],["dbfs:/datasets/streamingFiles/25_06.log","25_06.log",35],["dbfs:/datasets/streamingFiles/25_08.log","25_08.log",35],["dbfs:/datasets/streamingFiles/25_10.log","25_10.log",35],["dbfs:/datasets/streamingFiles/25_12.log","25_12.log",35],["dbfs:/datasets/streamingFiles/25_14.log","25_14.log",35],["dbfs:/datasets/streamingFiles/25_16.log","25_16.log",35],["dbfs:/datasets/streamingFiles/25_18.log","25_18.log",35],["dbfs:/datasets/streamingFiles/25_21.log","25_21.log",35],["dbfs:/datasets/streamingFiles/25_23.log","25_23.log",35],["dbfs:/datasets/streamingFiles/25_25.log","25_25.log",35],["dbfs:/datasets/streamingFiles/25_27.log","25_27.log",35],["dbfs:/datasets/streamingFiles/25_29.log","25_29.log",35],["dbfs:/datasets/streamingFiles/25_31.log","25_31.log",35],["dbfs:/datasets/streamingFiles/25_33.log","25_33.log",35],["dbfs:/datasets/streamingFiles/25_35.log","25_35.log",35],["dbfs:/datasets/streamingFiles/25_37.log","25_37.log",35],["dbfs:/datasets/streamingFiles/25_39.log","25_39.log",35],["dbfs:/datasets/streamingFiles/25_41.log","25_41.log",35],["dbfs:/datasets/streamingFiles/25_43.log","25_43.log",35],["dbfs:/datasets/streamingFiles/25_45.log","25_45.log",35],["dbfs:/datasets/streamingFiles/25_47.log","25_47.log",35],["dbfs:/datasets/streamingFiles/25_49.log","25_49.log",35],["dbfs:/datasets/streamingFiles/25_51.log","25_51.log",35],["dbfs:/datasets/streamingFiles/25_53.log","25_53.log",35],["dbfs:/datasets/streamingFiles/25_55.log","25_55.log",35],["dbfs:/datasets/streamingFiles/25_57.log","25_57.log",35],["dbfs:/datasets/streamingFiles/25_59.log","25_59.log",35],["dbfs:/datasets/streamingFiles/26_01.log","26_01.log",35],["dbfs:/datasets/streamingFiles/26_03.log","26_03.log",35],["dbfs:/datasets/streamingFiles/26_05.log","26_05.log",35],["dbfs:/datasets/streamingFiles/26_07.log","26_07.log",35],["dbfs:/datasets/streamingFiles/26_09.log","26_09.log",35],["dbfs:/datasets/streamingFiles/26_11.log","26_11.log",35],["dbfs:/datasets/streamingFiles/26_13.log","26_13.log",35],["dbfs:/datasets/streamingFiles/26_15.log","26_15.log",35],["dbfs:/datasets/streamingFiles/26_17.log","26_17.log",35],["dbfs:/datasets/streamingFiles/26_19.log","26_19.log",35],["dbfs:/datasets/streamingFiles/26_21.log","26_21.log",35],["dbfs:/datasets/streamingFiles/26_23.log","26_23.log",35],["dbfs:/datasets/streamingFiles/26_25.log","26_25.log",35],["dbfs:/datasets/streamingFiles/26_27.log","26_27.log",35],["dbfs:/datasets/streamingFiles/26_29.log","26_29.log",35],["dbfs:/datasets/streamingFiles/26_31.log","26_31.log",35],["dbfs:/datasets/streamingFiles/26_33.log","26_33.log",35],["dbfs:/datasets/streamingFiles/26_35.log","26_35.log",35],["dbfs:/datasets/streamingFiles/26_37.log","26_37.log",35],["dbfs:/datasets/streamingFiles/26_39.log","26_39.log",35],["dbfs:/datasets/streamingFiles/26_41.log","26_41.log",35],["dbfs:/datasets/streamingFiles/26_43.log","26_43.log",35],["dbfs:/datasets/streamingFiles/26_45.log","26_45.log",35],["dbfs:/datasets/streamingFiles/26_47.log","26_47.log",35],["dbfs:/datasets/streamingFiles/26_49.log","26_49.log",35],["dbfs:/datasets/streamingFiles/26_51.log","26_51.log",35],["dbfs:/datasets/streamingFiles/26_53.log","26_53.log",35],["dbfs:/datasets/streamingFiles/26_55.log","26_55.log",35],["dbfs:/datasets/streamingFiles/26_57.log","26_57.log",35],["dbfs:/datasets/streamingFiles/26_59.log","26_59.log",35],["dbfs:/datasets/streamingFiles/27_02.log","27_02.log",35],["dbfs:/datasets/streamingFiles/27_04.log","27_04.log",35],["dbfs:/datasets/streamingFiles/27_06.log","27_06.log",35],["dbfs:/datasets/streamingFiles/27_08.log","27_08.log",35],["dbfs:/datasets/streamingFiles/27_10.log","27_10.log",35],["dbfs:/datasets/streamingFiles/27_12.log","27_12.log",35],["dbfs:/datasets/streamingFiles/27_14.log","27_14.log",35],["dbfs:/datasets/streamingFiles/27_16.log","27_16.log",35],["dbfs:/datasets/streamingFiles/27_18.log","27_18.log",35],["dbfs:/datasets/streamingFiles/27_20.log","27_20.log",35],["dbfs:/datasets/streamingFiles/27_22.log","27_22.log",35],["dbfs:/datasets/streamingFiles/27_24.log","27_24.log",35],["dbfs:/datasets/streamingFiles/27_26.log","27_26.log",35],["dbfs:/datasets/streamingFiles/27_28.log","27_28.log",35],["dbfs:/datasets/streamingFiles/27_30.log","27_30.log",35],["dbfs:/datasets/streamingFiles/27_32.log","27_32.log",35],["dbfs:/datasets/streamingFiles/27_34.log","27_34.log",35],["dbfs:/datasets/streamingFiles/27_36.log","27_36.log",35],["dbfs:/datasets/streamingFiles/27_38.log","27_38.log",35],["dbfs:/datasets/streamingFiles/27_40.log","27_40.log",35],["dbfs:/datasets/streamingFiles/27_42.log","27_42.log",35],["dbfs:/datasets/streamingFiles/27_44.log","27_44.log",35],["dbfs:/datasets/streamingFiles/27_46.log","27_46.log",35],["dbfs:/datasets/streamingFiles/27_48.log","27_48.log",35],["dbfs:/datasets/streamingFiles/27_50.log","27_50.log",35],["dbfs:/datasets/streamingFiles/27_52.log","27_52.log",35],["dbfs:/datasets/streamingFiles/27_54.log","27_54.log",35],["dbfs:/datasets/streamingFiles/27_56.log","27_56.log",35],["dbfs:/datasets/streamingFiles/27_58.log","27_58.log",35],["dbfs:/datasets/streamingFiles/28_00.log","28_00.log",35],["dbfs:/datasets/streamingFiles/28_02.log","28_02.log",35],["dbfs:/datasets/streamingFiles/28_04.log","28_04.log",35],["dbfs:/datasets/streamingFiles/28_06.log","28_06.log",35],["dbfs:/datasets/streamingFiles/28_08.log","28_08.log",35],["dbfs:/datasets/streamingFiles/28_10.log","28_10.log",35],["dbfs:/datasets/streamingFiles/28_12.log","28_12.log",35],["dbfs:/datasets/streamingFiles/28_14.log","28_14.log",35],["dbfs:/datasets/streamingFiles/28_16.log","28_16.log",35],["dbfs:/datasets/streamingFiles/28_18.log","28_18.log",35],["dbfs:/datasets/streamingFiles/28_21.log","28_21.log",35],["dbfs:/datasets/streamingFiles/28_23.log","28_23.log",35],["dbfs:/datasets/streamingFiles/28_25.log","28_25.log",35],["dbfs:/datasets/streamingFiles/28_27.log","28_27.log",35],["dbfs:/datasets/streamingFiles/28_29.log","28_29.log",35],["dbfs:/datasets/streamingFiles/28_31.log","28_31.log",35],["dbfs:/datasets/streamingFiles/28_33.log","28_33.log",35],["dbfs:/datasets/streamingFiles/28_35.log","28_35.log",35],["dbfs:/datasets/streamingFiles/28_37.log","28_37.log",35],["dbfs:/datasets/streamingFiles/28_39.log","28_39.log",35],["dbfs:/datasets/streamingFiles/28_41.log","28_41.log",35],["dbfs:/datasets/streamingFiles/28_43.log","28_43.log",35],["dbfs:/datasets/streamingFiles/28_45.log","28_45.log",35],["dbfs:/datasets/streamingFiles/28_47.log","28_47.log",35],["dbfs:/datasets/streamingFiles/28_49.log","28_49.log",35],["dbfs:/datasets/streamingFiles/28_51.log","28_51.log",35],["dbfs:/datasets/streamingFiles/28_53.log","28_53.log",35],["dbfs:/datasets/streamingFiles/28_55.log","28_55.log",35],["dbfs:/datasets/streamingFiles/28_57.log","28_57.log",35],["dbfs:/datasets/streamingFiles/28_59.log","28_59.log",35],["dbfs:/datasets/streamingFiles/29_01.log","29_01.log",35],["dbfs:/datasets/streamingFiles/29_03.log","29_03.log",35],["dbfs:/datasets/streamingFiles/29_05.log","29_05.log",35],["dbfs:/datasets/streamingFiles/29_07.log","29_07.log",35],["dbfs:/datasets/streamingFiles/29_09.log","29_09.log",35],["dbfs:/datasets/streamingFiles/29_11.log","29_11.log",35],["dbfs:/datasets/streamingFiles/29_13.log","29_13.log",35],["dbfs:/datasets/streamingFiles/29_15.log","29_15.log",35],["dbfs:/datasets/streamingFiles/29_17.log","29_17.log",35],["dbfs:/datasets/streamingFiles/29_19.log","29_19.log",35],["dbfs:/datasets/streamingFiles/29_21.log","29_21.log",35],["dbfs:/datasets/streamingFiles/29_23.log","29_23.log",35],["dbfs:/datasets/streamingFiles/29_25.log","29_25.log",35],["dbfs:/datasets/streamingFiles/29_27.log","29_27.log",35],["dbfs:/datasets/streamingFiles/29_29.log","29_29.log",35],["dbfs:/datasets/streamingFiles/29_31.log","29_31.log",35],["dbfs:/datasets/streamingFiles/29_33.log","29_33.log",35],["dbfs:/datasets/streamingFiles/29_35.log","29_35.log",35],["dbfs:/datasets/streamingFiles/29_37.log","29_37.log",35],["dbfs:/datasets/streamingFiles/29_39.log","29_39.log",35],["dbfs:/datasets/streamingFiles/29_41.log","29_41.log",35],["dbfs:/datasets/streamingFiles/29_43.log","29_43.log",35],["dbfs:/datasets/streamingFiles/29_45.log","29_45.log",35],["dbfs:/datasets/streamingFiles/29_47.log","29_47.log",35],["dbfs:/datasets/streamingFiles/29_49.log","29_49.log",35],["dbfs:/datasets/streamingFiles/29_51.log","29_51.log",35],["dbfs:/datasets/streamingFiles/29_53.log","29_53.log",35],["dbfs:/datasets/streamingFiles/29_55.log","29_55.log",35],["dbfs:/datasets/streamingFiles/29_57.log","29_57.log",35],["dbfs:/datasets/streamingFiles/29_59.log","29_59.log",35],["dbfs:/datasets/streamingFiles/30_01.log","30_01.log",35],["dbfs:/datasets/streamingFiles/30_03.log","30_03.log",35],["dbfs:/datasets/streamingFiles/30_05.log","30_05.log",35],["dbfs:/datasets/streamingFiles/30_07.log","30_07.log",35],["dbfs:/datasets/streamingFiles/30_09.log","30_09.log",35],["dbfs:/datasets/streamingFiles/30_11.log","30_11.log",35],["dbfs:/datasets/streamingFiles/30_13.log","30_13.log",35],["dbfs:/datasets/streamingFiles/30_15.log","30_15.log",35],["dbfs:/datasets/streamingFiles/30_17.log","30_17.log",35],["dbfs:/datasets/streamingFiles/30_19.log","30_19.log",35],["dbfs:/datasets/streamingFiles/30_21.log","30_21.log",35],["dbfs:/datasets/streamingFiles/30_23.log","30_23.log",35],["dbfs:/datasets/streamingFiles/30_25.log","30_25.log",35],["dbfs:/datasets/streamingFiles/30_27.log","30_27.log",35],["dbfs:/datasets/streamingFiles/30_29.log","30_29.log",35],["dbfs:/datasets/streamingFiles/30_31.log","30_31.log",35],["dbfs:/datasets/streamingFiles/30_33.log","30_33.log",35],["dbfs:/datasets/streamingFiles/30_36.log","30_36.log",35],["dbfs:/datasets/streamingFiles/30_38.log","30_38.log",35],["dbfs:/datasets/streamingFiles/30_40.log","30_40.log",35],["dbfs:/datasets/streamingFiles/30_42.log","30_42.log",35],["dbfs:/datasets/streamingFiles/30_44.log","30_44.log",35],["dbfs:/datasets/streamingFiles/30_46.log","30_46.log",35],["dbfs:/datasets/streamingFiles/30_48.log","30_48.log",35],["dbfs:/datasets/streamingFiles/30_50.log","30_50.log",35],["dbfs:/datasets/streamingFiles/30_52.log","30_52.log",35],["dbfs:/datasets/streamingFiles/30_54.log","30_54.log",35],["dbfs:/datasets/streamingFiles/30_56.log","30_56.log",35],["dbfs:/datasets/streamingFiles/30_58.log","30_58.log",35],["dbfs:/datasets/streamingFiles/31_00.log","31_00.log",35],["dbfs:/datasets/streamingFiles/31_02.log","31_02.log",35],["dbfs:/datasets/streamingFiles/31_04.log","31_04.log",35],["dbfs:/datasets/streamingFiles/31_06.log","31_06.log",35],["dbfs:/datasets/streamingFiles/31_08.log","31_08.log",35],["dbfs:/datasets/streamingFiles/31_10.log","31_10.log",35],["dbfs:/datasets/streamingFiles/31_12.log","31_12.log",35],["dbfs:/datasets/streamingFiles/31_14.log","31_14.log",35],["dbfs:/datasets/streamingFiles/31_16.log","31_16.log",35],["dbfs:/datasets/streamingFiles/31_18.log","31_18.log",35],["dbfs:/datasets/streamingFiles/31_20.log","31_20.log",35],["dbfs:/datasets/streamingFiles/31_22.log","31_22.log",35],["dbfs:/datasets/streamingFiles/31_24.log","31_24.log",35],["dbfs:/datasets/streamingFiles/31_26.log","31_26.log",35],["dbfs:/datasets/streamingFiles/31_28.log","31_28.log",35],["dbfs:/datasets/streamingFiles/31_30.log","31_30.log",35],["dbfs:/datasets/streamingFiles/31_32.log","31_32.log",35],["dbfs:/datasets/streamingFiles/31_34.log","31_34.log",35],["dbfs:/datasets/streamingFiles/31_36.log","31_36.log",35],["dbfs:/datasets/streamingFiles/31_38.log","31_38.log",35],["dbfs:/datasets/streamingFiles/31_40.log","31_40.log",35],["dbfs:/datasets/streamingFiles/31_42.log","31_42.log",35],["dbfs:/datasets/streamingFiles/31_44.log","31_44.log",35],["dbfs:/datasets/streamingFiles/31_46.log","31_46.log",35],["dbfs:/datasets/streamingFiles/31_48.log","31_48.log",35],["dbfs:/datasets/streamingFiles/31_50.log","31_50.log",35],["dbfs:/datasets/streamingFiles/31_52.log","31_52.log",35],["dbfs:/datasets/streamingFiles/31_54.log","31_54.log",35],["dbfs:/datasets/streamingFiles/31_56.log","31_56.log",35],["dbfs:/datasets/streamingFiles/31_58.log","31_58.log",35],["dbfs:/datasets/streamingFiles/32_00.log","32_00.log",35],["dbfs:/datasets/streamingFiles/32_02.log","32_02.log",35],["dbfs:/datasets/streamingFiles/32_04.log","32_04.log",35],["dbfs:/datasets/streamingFiles/32_06.log","32_06.log",35],["dbfs:/datasets/streamingFiles/32_08.log","32_08.log",35],["dbfs:/datasets/streamingFiles/32_10.log","32_10.log",35],["dbfs:/datasets/streamingFiles/32_12.log","32_12.log",35],["dbfs:/datasets/streamingFiles/32_14.log","32_14.log",35],["dbfs:/datasets/streamingFiles/32_16.log","32_16.log",35],["dbfs:/datasets/streamingFiles/32_18.log","32_18.log",35],["dbfs:/datasets/streamingFiles/32_20.log","32_20.log",35],["dbfs:/datasets/streamingFiles/32_22.log","32_22.log",35],["dbfs:/datasets/streamingFiles/32_24.log","32_24.log",35],["dbfs:/datasets/streamingFiles/32_26.log","32_26.log",35],["dbfs:/datasets/streamingFiles/32_28.log","32_28.log",35],["dbfs:/datasets/streamingFiles/32_30.log","32_30.log",35],["dbfs:/datasets/streamingFiles/32_32.log","32_32.log",35],["dbfs:/datasets/streamingFiles/32_34.log","32_34.log",35],["dbfs:/datasets/streamingFiles/32_36.log","32_36.log",35],["dbfs:/datasets/streamingFiles/32_38.log","32_38.log",35],["dbfs:/datasets/streamingFiles/32_40.log","32_40.log",35],["dbfs:/datasets/streamingFiles/32_42.log","32_42.log",35],["dbfs:/datasets/streamingFiles/32_44.log","32_44.log",35],["dbfs:/datasets/streamingFiles/32_46.log","32_46.log",35],["dbfs:/datasets/streamingFiles/32_48.log","32_48.log",35],["dbfs:/datasets/streamingFiles/32_50.log","32_50.log",35],["dbfs:/datasets/streamingFiles/32_52.log","32_52.log",35],["dbfs:/datasets/streamingFiles/32_54.log","32_54.log",35],["dbfs:/datasets/streamingFiles/32_56.log","32_56.log",35],["dbfs:/datasets/streamingFiles/32_58.log","32_58.log",35],["dbfs:/datasets/streamingFiles/33_01.log","33_01.log",35],["dbfs:/datasets/streamingFiles/33_03.log","33_03.log",35],["dbfs:/datasets/streamingFiles/33_05.log","33_05.log",35],["dbfs:/datasets/streamingFiles/33_07.log","33_07.log",35],["dbfs:/datasets/streamingFiles/33_09.log","33_09.log",35],["dbfs:/datasets/streamingFiles/33_11.log","33_11.log",35],["dbfs:/datasets/streamingFiles/33_13.log","33_13.log",35],["dbfs:/datasets/streamingFiles/33_15.log","33_15.log",35],["dbfs:/datasets/streamingFiles/33_17.log","33_17.log",35],["dbfs:/datasets/streamingFiles/33_19.log","33_19.log",35],["dbfs:/datasets/streamingFiles/33_21.log","33_21.log",35],["dbfs:/datasets/streamingFiles/33_23.log","33_23.log",35],["dbfs:/datasets/streamingFiles/33_25.log","33_25.log",35],["dbfs:/datasets/streamingFiles/33_27.log","33_27.log",35],["dbfs:/datasets/streamingFiles/33_29.log","33_29.log",35],["dbfs:/datasets/streamingFiles/33_31.log","33_31.log",35],["dbfs:/datasets/streamingFiles/33_33.log","33_33.log",35]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"overflow":true,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null,"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511293385951,"submitTime":1511293396930,"finishTime":1511293387933,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b2d27cd9-26d0-40e3-91be-5f4ce2b318ae"},{"version":"CommandV1","origId":204728,"guid":"a5b410ac-0211-43e9-92e5-ae44b8e7eeed","subtype":"command","commandType":"auto","position":20.9375,"command":"spark.read.format(\"text\").load(\"/datasets/streamingFiles\").show(5,false) // let's just read five  entries","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------------------------------+\n|value                             |\n+----------------------------------+\n|2017-11-21 18:00:00+00:00; pig owl|\n|2017-11-21 18:00:02+00:00; rat bat|\n|2017-11-21 18:00:04+00:00; pig rat|\n|2017-11-21 18:00:06+00:00; dog pig|\n|2017-11-21 18:00:08+00:00; rat cat|\n+----------------------------------+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:108: error: value println is not a member of org.apache.spark.sql.Row\n       spark.read.format(&quot;text&quot;).load(&quot;/datasets/streamingFiles&quot;).take(5).map( _.println)\n                                                                                 ^\n</div>","error":null,"workflows":[],"startTime":1511293394598,"submitTime":1511293405584,"finishTime":1511293398128,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"640b8fd2-36ff-4090-89c7-62108af0790c"},{"version":"CommandV1","origId":207904,"guid":"18296028-e42d-4803-b7a0-ae990e7efb9e","subtype":"command","commandType":"auto","position":20.95361328125,"command":"import spark.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport java.sql.Timestamp\n\n// a static DF is convenient to work with\nval csvStaticDS = spark\n   .read\n   .option(\"sep\", \";\") // delimiter is ';'\n   .csv(\"/datasets/streamingFiles/*.log\")    // Equivalent to format(\"csv\").load(\"/path/to/directory\")\n   .toDF(\"time\",\"animals\")\n   .as[(Timestamp, String)]\n   .flatMap(\n     line => line._2.split(\" \").map(animal => (line._1, animal))\n    )\n   .filter(_._2 != \"\") // remove empty strings from the leading whitespaces\n   .toDF(\"timestamp\", \"animal\")\n   .as[(Timestamp, String)]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import spark.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport java.sql.Timestamp\ncsvStaticDS: org.apache.spark.sql.Dataset[(java.sql.Timestamp, String)] = [timestamp: timestamp, animal: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:93: error: value === is not a member of String\n           ).filter(_._2 === &quot;&quot;).toDF(&quot;time&quot;, &quot;animal&quot;)\n                         ^\n</div>","error":null,"workflows":[],"startTime":1511293589451,"submitTime":1511293600442,"finishTime":1511293623443,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f28b578d-9698-4bca-9934-5f1ea57cc778"},{"version":"CommandV1","origId":207905,"guid":"4b84b77e-b3b0-4225-8965-312f5cb036d4","subtype":"command","commandType":"auto","position":20.9541015625,"command":"csvStaticDS.show(5,false)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-------------------+------+\n|timestamp          |animal|\n+-------------------+------+\n|2017-11-21 18:00:00|pig   |\n|2017-11-21 18:00:00|owl   |\n|2017-11-21 18:00:02|rat   |\n|2017-11-21 18:00:02|bat   |\n|2017-11-21 18:00:04|pig   |\n+-------------------+------+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:119: error: overloaded method value show with alternatives:\n  (numRows: Int,truncate: Int)Unit &lt;and&gt;\n  (numRows: Int,truncate: Boolean)Unit\n cannot be applied to (Boolean, Int)\n       csvStaticDS.show(false,5)\n                   ^\n</div>","error":null,"workflows":[],"startTime":1511293671769,"submitTime":1511293682740,"finishTime":1511293674150,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"006724a1-6413-40f2-8bca-2ed0fec3d4d8"},{"version":"CommandV1","origId":207900,"guid":"59a8c802-1081-4ae1-a8ec-268d193a5c46","subtype":"command","commandType":"auto","position":20.955078125,"command":"//make a user-specified schema for structured streaming\nval userSchema = new StructType()\n                      .add(\"time\", \"String\") // we will read it as String and then convert into timestamp later\n                      .add(\"animals\", \"String\")\n\n// streaming DS\nval csvStreamingDS = spark\n// the next three lines are needed for structured streaming from file streams\n  .readStream // for streaming\n  .option(\"MaxFilesPerTrigger\", 1) //  for streaming\n  .schema(userSchema) // for streaming\n  .option(\"sep\", \";\") // delimiter is ';'\n  .csv(\"/datasets/streamingFiles/*.log\")    // Equivalent to format(\"csv\").load(\"/path/to/directory\")\n  .toDF(\"time\",\"animals\")\n  .as[(Timestamp, String)]\n  .flatMap(\n     line => line._2.split(\" \").map(animal => (line._1, animal))\n    )\n  .filter(_._2 != \"\")\n  .toDF(\"timestamp\", \"animal\")\n  .as[(Timestamp, String)]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">userSchema: org.apache.spark.sql.types.StructType = StructType(StructField(time,StringType,true), StructField(animals,StringType,true))\ncsvStreamingDS: org.apache.spark.sql.Dataset[(java.sql.Timestamp, String)] = [timestamp: timestamp, animal: string]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"java.lang.IllegalArgumentException: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.","error":"<div class=\"ansiout\">\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:227)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:88)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:88)\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:150)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:159)\n\tat org.apache.spark.sql.streaming.DataStreamReader.csv(DataStreamReader.scala:284)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:88)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:104)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:106)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:108)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:110)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:112)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:114)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:116)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:118)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:120)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:122)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:124)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:126)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:128)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:130)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:132)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:134)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:136)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:138)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:140)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:142)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:144)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:146)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:148)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:150)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:152)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:154)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:156)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:158)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:160)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:162)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$read$$iw.&lt;init&gt;(&lt;console&gt;:164)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$eval$.$print$lzycompute(&lt;console&gt;:7)\n\tat line66c0b7c789bd41309e591cb10023e59b159.$eval$.$print(&lt;console&gt;:6)</div>","workflows":[],"startTime":1511294328544,"submitTime":1511294339524,"finishTime":1511294331604,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"495a8439-d8e8-4f2b-ac95-aafb518d2151"},{"version":"CommandV1","origId":207907,"guid":"0556d1e1-618c-4884-9514-bb19e30e0a05","subtype":"command","commandType":"auto","position":20.9560546875,"command":"display(csvStreamingDS) // evaluate to see the animal words with timestamps streaming in","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511289145049,"submitTime":1511289155938,"finishTime":1511289145356,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"015bf156-0383-4b4a-8e71-c994a7891c60"},{"version":"CommandV1","origId":207899,"guid":"edb442a7-6883-4c12-9944-a84c2f7a38a8","subtype":"command","commandType":"auto","position":20.95703125,"command":"// Group the data by window and word and compute the count of each group\nval windowDuration = \"180 seconds\"\nval slideDuration = \"90 seconds\"\nval windowedCounts = csvStreamingDS.groupBy(\n      window($\"timestamp\", windowDuration, slideDuration), $\"animal\"\n    ).count().orderBy(\"window\")\n\n// Start running the query that prints the windowed word counts to the console\nval query = windowedCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .option(\"truncate\", \"false\")\n      .start()\n\nquery.awaitTermination()","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\">-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |3    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |3    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 10\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 11\n-------------------------------------------\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1511294345981,"submitTime":1511294345981,"finishTime":1511294707079,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e3b83479-7b04-4e95-8ad5-cf6851d5b89e"},{"version":"CommandV1","origId":207898,"guid":"e497bc98-808e-4819-b0bc-045aa75063aa","subtype":"command","commandType":"auto","position":20.9609375,"command":"%md\n### Handling Late Data and Watermarking\n\nNow consider what happens if one of the events arrives late to the\napplication. For example, say, a word generated at 12:04 (i.e. event\ntime) could be received by the application at 12:11. The application\nshould use the time 12:04 instead of 12:11 to update the older counts\nfor the window `12:00 - 12:10`. This occurs naturally in our\nwindow-based grouping – Structured Streaming can maintain the\nintermediate state for partial aggregates for a long period of time such\nthat late data can update aggregates of old windows correctly, as\nillustrated below.\n\n![Handling Late\nData](https://spark.apache.org/docs/2.2.0/img/structured-streaming-late-data.png)\n\nHowever, to run this query for days, it’s necessary for the system to\nbound the amount of intermediate in-memory state it accumulates. This\nmeans the system needs to know when an old aggregate can be dropped from\nthe in-memory state because the application is not going to receive late\ndata for that aggregate any more. To enable this, in Spark 2.1, we have\nintroduced **watermarking**, which lets the engine automatically track\nthe current event time in the data and attempt to clean up old state\naccordingly. You can define the watermark of a query by specifying the\nevent time column and the threshold on how late the data is expected to\nbe in terms of event time. For a specific window starting at time `T`,\nthe engine will maintain state and allow late data to update the state\nuntil `(max event time seen by the engine - late threshold > T)`. In\nother words, late data within the threshold will be aggregated, but data\nlater than the threshold will be dropped. Let’s understand this with an\nexample. We can easily define watermarking on the previous example using\n`withWatermark()` as shown below.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1741f4f7-64ca-4056-8637-261d3e4a45d9"},{"version":"CommandV1","origId":207909,"guid":"68a07e24-1c05-4082-8a21-9dd38709d550","subtype":"command","commandType":"auto","position":21.9609375,"command":"// Group the data by window and word and compute the count of each group\nval windowDuration = \"180 seconds\"\nval slideDuration = \"90 seconds\"\nval watermarkDuration = \"10 minutes\"\nval windowedCounts = csvStreamingDS\n     .withWatermark(\"timestamp\", watermarkDuration)\n     .groupBy(\n      window($\"timestamp\", windowDuration, slideDuration), $\"animal\"\n    ).count().orderBy(\"window\")\n\n// Start running the query that prints the windowed word counts to the console\nval query = windowedCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .option(\"truncate\", \"false\")\n      .start()\n\nquery.awaitTermination()","commandVersion":0,"state":"error","results":{"type":"html","data":"<div class=\"ansiout\">-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|bat   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |2    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|rat   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |3    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|bat   |1    |\n+---------------------------------------------+------+-----+\n\n-------------------------------------------\nBatch: 10\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|dog   |2    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|owl   |3    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|bat   |1    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 11\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|pig   |1    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 12\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |5    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |5    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:01:30.0,2017-11-21 20:04:30.0]|rat   |1    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 13\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |4    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |5    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |5    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 14\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |5    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |5    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |1    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |6    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 15\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |5    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |6    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |5    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 16\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |5    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |5    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |6    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 17\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |7    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |6    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |1    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |2    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 18\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |6    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |4    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |3    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |2    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |7    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |4    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |3    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |5    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |1    |\n\n*** WARNING: skipped 73843 bytes of output ***\n\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |9    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |10   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 66\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |10   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |12   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 67\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |10   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 68\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 69\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |7    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |13   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 70\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |8    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 71\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |7    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 72\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |8    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 73\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |13   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 74\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |10   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 75\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |8    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |14   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |9    |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 76\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |9    |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |10   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 77\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |17   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 78\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |17   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |16   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |15   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 79\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |17   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |15   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |11   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|pig   |13   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 80\n-------------------------------------------\n+---------------------------------------------+------+-----+\n|window                                       |animal|count|\n+---------------------------------------------+------+-----+\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|cat   |2    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|bat   |1    |\n|[2017-11-21 19:55:30.0,2017-11-21 19:58:30.0]|dog   |3    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|owl   |6    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|dog   |9    |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|rat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|cat   |11   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|pig   |10   |\n|[2017-11-21 19:57:00.0,2017-11-21 20:00:00.0]|bat   |11   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|owl   |12   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|rat   |17   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|cat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|bat   |16   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|pig   |15   |\n|[2017-11-21 19:58:30.0,2017-11-21 20:01:30.0]|dog   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|cat   |15   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|owl   |12   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|rat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|bat   |14   |\n|[2017-11-21 20:00:00.0,2017-11-21 20:03:00.0]|dog   |15   |\n+---------------------------------------------+------+-----+\nonly showing top 20 rows\n\n-------------------------------------------\nBatch: 81\n-------------------------------------------\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1511295804324,"submitTime":1511295804324,"finishTime":1511298000230,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8e9f0073-4a9e-4440-927a-b5c7c97a1149"},{"version":"CommandV1","origId":207910,"guid":"b3907b01-0534-4955-859a-f6654fa43c6d","subtype":"command","commandType":"auto","position":22.9609375,"command":"%md\nIn this example, we are defining the watermark of the query on the value\nof the column “timestamp”, and also defining “10 minutes” as the\nthreshold of how late is the data allowed to be. If this query is run in\nUpdate output mode (discussed later in [Output\nModes](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes)\nsection), the engine will keep updating counts of a window in the Result\nTable until the window is older than the watermark, which lags behind\nthe current event time in column “timestamp” by 10 minutes. Here is an\nillustration.\n\n![Watermarking in Update\nMode](https://spark.apache.org/docs/2.2.0/img/structured-streaming-watermark-update-mode.png)\n\nAs shown in the illustration, the maximum event time tracked by the\nengine is the *blue dashed line*, and the watermark set as\n`(max event time - '10 mins')` at the beginning of every trigger is the\nred line For example, when the engine observes the data `(12:14, dog)`,\nit sets the watermark for the next trigger as `12:04`. This watermark\nlets the engine maintain intermediate state for additional 10 minutes to\nallow late data to be counted. For example, the data `(12:09, cat)` is\nout of order and late, and it falls in windows `12:05 - 12:15` and\n`12:10 - 12:20`. Since, it is still ahead of the watermark `12:04` in\nthe trigger, the engine still maintains the intermediate counts as state\nand correctly updates the counts of the related windows. However, when\nthe watermark is updated to `12:11`, the intermediate state for window\n`(12:00 - 12:10)` is cleared, and all subsequent data (e.g.\n`(12:04, donkey)`) is considered “too late” and therefore ignored. Note\nthat after every trigger, the updated counts (i.e. purple rows) are\nwritten to sink as the trigger output, as dictated by the Update mode.\n\nSome sinks (e.g. files) may not supported fine-grained updates that\nUpdate Mode requires. To work with them, we have also support Append\nMode, where only the *final counts* are written to sink. This is\nillustrated below.\n\nNote that using `withWatermark` on a non-streaming Dataset is no-op. As\nthe watermark should not affect any batch query in any way, we will\nignore it directly.\n\n![Watermarking in Append\nMode](https://spark.apache.org/docs/2.2.0/img/structured-streaming-watermark-append-mode.png)\n\nSimilar to the Update Mode earlier, the engine maintains intermediate\ncounts for each window. However, the partial counts are not updated to\nthe Result Table and not written to sink. The engine waits for “10 mins”\nfor late date to be counted, then drops intermediate state of a window\n&lt; watermark, and appends the final counts to the Result Table/sink.\nFor example, the final counts of window `12:00 - 12:10` is appended to\nthe Result Table only after the watermark is updated to `12:11`.\n\n**Conditions for watermarking to clean aggregation state** It is\nimportant to note that the following conditions must be satisfied for\nthe watermarking to clean the state in aggregation queries *(as of Spark\n2.1.1, subject to change in the future)*.\n\n-   **Output mode must be Append or Update.** Complete mode requires all\n    aggregate data to be preserved, and hence cannot use watermarking to\n    drop intermediate state. See the [Output\n    Modes](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes)\n    section for detailed explanation of the semantics of each\n    output mode.\n\n-   The aggregation must have either the event-time column, or a\n    `window` on the event-time column.\n\n-   `withWatermark` must be called on the same column as the timestamp\n    column used in the aggregate. For example,\n    `df.withWatermark(\"time\", \"1 min\").groupBy(\"time2\").count()` is\n    invalid in Append output mode, as watermark is defined on a\n    different column from the aggregation column.\n\n-   `withWatermark` must be called before the aggregation for the\n    watermark details to be used. For example,\n    `df.groupBy(\"time\").count().withWatermark(\"time\", \"1 min\")` is\n    invalid in Append output mode.\n\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c0bcfc33-a832-4468-be84-6aafefd14679"},{"version":"CommandV1","origId":207911,"guid":"d99d04e6-7fe3-4cce-ac36-13fb06ba9693","subtype":"command","commandType":"auto","position":23.9609375,"command":"%md\n### Join Operations\n\nStreaming DataFrames can be joined with static DataFrames to create new\nstreaming DataFrames. Here are a few examples.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"95da5bdd-1e31-469f-80d9-7bd09b32fa85"},{"version":"CommandV1","origId":207912,"guid":"63e11bef-b355-4e50-9b43-f99b2cff9094","subtype":"command","commandType":"auto","position":24.9609375,"command":"%md\n```\nval staticDf = spark.read. ...\nval streamingDf = spark.readStream. ...\n\nstreamingDf.join(staticDf, \"type\")          // inner equi-join with a static DF\nstreamingDf.join(staticDf, \"type\", \"right_join\")  // right outer join with a static DF\n```","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"af50c23b-05f3-4f60-b872-e1fe29cae853"},{"version":"CommandV1","origId":207913,"guid":"af70cb59-5bb6-4559-85f3-774ced7c52ef","subtype":"command","commandType":"auto","position":25.9609375,"command":"%md\n### Streaming Deduplication\n\nYou can deduplicate records in data streams using a unique identifier in\nthe events. This is exactly same as deduplication on static using a\nunique identifier column. The query will store the necessary amount of\ndata from previous records such that it can filter duplicate records.\nSimilar to aggregations, you can use deduplication with or without\nwatermarking.\n\n-   *With watermark* - If there is a upper bound on how late a duplicate\n    record may arrive, then you can define a watermark on a event time\n    column and deduplicate using both the guid and the event\n    time columns. The query will use the watermark to remove old state\n    data from past records that are not expected to get any duplicates\n    any more. This bounds the amount of the state the query has\n    to maintain.\n\n-   *Without watermark* - Since there are no bounds on when a duplicate\n    record may arrive, the query stores the data from all the past\n    records as state.\n\n```\n    val streamingDf = spark.readStream. ...  // columns: guid, eventTime, ...\n\n    // Without watermark using guid column\n    streamingDf.dropDuplicates(\"guid\")\n\n    // With watermark using guid and eventTime columns\n    streamingDf\n      .withWatermark(\"eventTime\", \"10 seconds\")\n      .dropDuplicates(\"guid\", \"eventTime\")\n```","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cc2b68b1-023c-425b-adc0-21ee6be9597d"},{"version":"CommandV1","origId":207914,"guid":"720e9dac-d40f-4938-b075-a45464b9c24d","subtype":"command","commandType":"auto","position":26.9609375,"command":"%md\n### Arbitrary Stateful Operations\n\nMany uscases require more advanced stateful operations than\naggregations. For example, in many usecases, you have to track sessions\nfrom data streams of events. For doing such sessionization, you will\nhave to save arbitrary types of data as state, and perform arbitrary\noperations on the state using the data stream events in every trigger.\nSince Spark 2.2, this can be done using the operation\n`mapGroupsWithState` and the more powerful operation\n`flatMapGroupsWithState`. Both operations allow you to apply\nuser-defined code on grouped Datasets to update user-defined state. For\nmore concrete details, take a look at the API documentation\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.GroupState)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/GroupState.html))\nand the examples\n([Scala](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala)/[Java](https://github.com/apache/spark/blob/v2.2.0/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java)).\n\n### Unsupported Operations\n\nThere are a few DataFrame/Dataset operations that are not supported with\nstreaming DataFrames/Datasets. Some of them are as follows.\n\n-   Multiple streaming aggregations (i.e. a chain of aggregations on a\n    streaming DF) are not yet supported on streaming Datasets.\n\n-   Limit and take first N rows are not supported on streaming Datasets.\n\n-   Distinct operations on streaming Datasets are not supported.\n\n-   Sorting operations are supported on streaming Datasets only after an\n    aggregation and in Complete Output Mode.\n\n-   Outer joins between a streaming and a static Datasets are\n    conditionally supported.\n\n    -   Full outer join with a streaming Dataset is not supported\n\n    -   Left outer join with a streaming Dataset on the right is not\n        supported\n\n    -   Right outer join with a streaming Dataset on the left is not\n        supported\n\n-   Any kind of joins between two streaming Datasets is not\n    yet supported.\n\nIn addition, there are some Dataset methods that will not work on\nstreaming Datasets. They are actions that will immediately run queries\nand return results, which does not make sense on a streaming Dataset.\nRather, those functionalities can be done by explicitly starting a\nstreaming query (see the next section regarding that).\n\n-   `count()` - Cannot return a single count from a streaming Dataset.\n    Instead, use `ds.groupBy().count()` which returns a streaming\n    Dataset containing a running count.\n\n-   `foreach()` - Instead use `ds.writeStream.foreach(...)` (see\n    next section).\n\n-   `show()` - Instead use the console sink (see next section).\n\nIf you try any of these operations, you will see an `AnalysisException`\nlike “operation XYZ is not supported with streaming\nDataFrames/Datasets”. While some of them may be supported in future\nreleases of Spark, there are others which are fundamentally hard to\nimplement on streaming data efficiently. For example, sorting on the\ninput stream is not supported, as it requires keeping track of all the\ndata received in the stream. This is therefore fundamentally hard to\nexecute efficiently.\n\nStarting Streaming Queries\n--------------------------\n\nOnce you have defined the final result DataFrame/Dataset, all that is\nleft is for you to start the streaming computation. To do that, you have\nto use the `DataStreamWriter`\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html)/[Python](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter)\ndocs) returned through `Dataset.writeStream()`. You will have to specify\none or more of the following in this interface.\n\n-   *Details of the output sink:* Data format, location, etc.\n\n-   *Output mode:* Specify what gets written to the output sink.\n\n-   *Query name:* Optionally, specify a unique name of the query\n    for identification.\n\n-   *Trigger interval:* Optionally, specify the trigger interval. If it\n    is not specified, the system will check for availability of new data\n    as soon as the previous processing has completed. If a trigger time\n    is missed because the previous processing has not completed, then\n    the system will attempt to trigger at the next trigger point, not\n    immediately after the processing has completed.\n\n-   *Checkpoint location:* For some output sinks where the end-to-end\n    fault-tolerance can be guaranteed, specify the location where the\n    system will write all the checkpoint information. This should be a\n    directory in an HDFS-compatible fault-tolerant file system. The\n    semantics of checkpointing is discussed in more detail in the\n    next section.\n\n#### Output Modes\n\nThere are a few types of output modes.\n\n-   **Append mode (default)** - This is the default mode, where only the\n    new rows added to the Result Table since the last trigger will be\n    outputted to the sink. This is supported for only those queries\n    where rows added to the Result Table is never going to change.\n    Hence, this mode guarantees that each row will be output only once\n    (assuming fault-tolerant sink). For example, queries with only\n    `select`, `where`, `map`, `flatMap`, `filter`, `join`, etc. will\n    support Append mode.\n\n-   **Complete mode** - The whole Result Table will be outputted to the\n    sink after every trigger. This is supported for aggregation queries.\n\n-   **Update mode** - (*Available since Spark 2.1.1*) Only the rows in\n    the Result Table that were updated since the last trigger will be\n    outputted to the sink. More information to be added in\n    future releases.\n\nDifferent types of streaming queries support different output modes.\nHere is the compatibility matrix.\n\n\n<table class=\"table\">\n  <tr>\n    <th>Query Type</th>\n    <th></th>\n    <th>Supported Output Modes</th>\n    <th>Notes</th>        \n  </tr>\n  <tr>\n    <td rowspan=\"2\" style=\"vertical-align: middle;\">Queries with aggregation</td>\n    <td style=\"vertical-align: middle;\">Aggregation on event-time with watermark</td>\n    <td style=\"vertical-align: middle;\">Append, Update, Complete</td>\n    <td>\n        Append mode uses watermark to drop old aggregation state. But the output of a \n        windowed aggregation is delayed the late threshold specified in `withWatermark()` as by\n        the modes semantics, rows can be added to the Result Table only once after they are \n        finalized (i.e. after watermark is crossed). See the\n        <a href=\"structured-streaming-programming-guide.html#handling-late-data-and-watermarking\">Late Data</a> section for more details.\n        <br /><br />\n        Update mode uses watermark to drop old aggregation state.\n        <br /><br />\n        Complete mode does not drop old aggregation state since by definition this mode\n        preserves all data in the Result Table.\n    </td>    \n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\">Other aggregations</td>\n    <td style=\"vertical-align: middle;\">Complete, Update</td>\n    <td>\n        Since no watermark is defined (only defined in other category), \n        old aggregation state is not dropped.\n        <br /><br />\n        Append mode is not supported as aggregates can update thus violating the semantics of \n        this mode.\n    </td>  \n  </tr>\n  <tr>\n    <td colspan=\"2\" style=\"vertical-align: middle;\">Queries with <code>mapGroupsWithState</code></td>\n    <td style=\"vertical-align: middle;\">Update</td>\n    <td style=\"vertical-align: middle;\"></td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\" style=\"vertical-align: middle;\">Queries with <code>flatMapGroupsWithState</code></td>\n    <td style=\"vertical-align: middle;\">Append operation mode</td>\n    <td style=\"vertical-align: middle;\">Append</td>\n    <td style=\"vertical-align: middle;\">\n      Aggregations are allowed after <code>flatMapGroupsWithState</code>.\n    </td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: middle;\">Update operation mode</td>\n    <td style=\"vertical-align: middle;\">Update</td>\n    <td style=\"vertical-align: middle;\">\n      Aggregations not allowed after <code>flatMapGroupsWithState</code>.\n    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" style=\"vertical-align: middle;\">Other queries</td>\n    <td style=\"vertical-align: middle;\">Append, Update</td>\n    <td style=\"vertical-align: middle;\">\n      Complete mode not supported as it is infeasible to keep all unaggregated data in the Result Table.\n    </td>\n  </tr>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"04e16ba6-5615-45f1-b226-2dcc30a7673c"},{"version":"CommandV1","origId":207915,"guid":"fe30c5d1-1f23-4f99-8279-10d90df202fd","subtype":"command","commandType":"auto","position":27.9609375,"command":"%md\n#### Output Sinks\n\nThere are a few types of built-in output sinks.\n\n-   **File sink** - Stores the output to a directory.\n\n```\n    writeStream\n        .format(\"parquet\")        // can be \"orc\", \"json\", \"csv\", etc.\n        .option(\"path\", \"path/to/destination/dir\")\n        .start()\n\n```\n-   **Foreach sink** - Runs arbitrary computation on the records in\n    the output. See later in the section for more details.\n\n```\n    writeStream\n        .foreach(...)\n        .start()\n\n```\n\n-   **Console sink (for debugging)** - Prints the output to the\n    console/stdout every time there is a trigger. Both, Append and\n    Complete output modes, are supported. This should be used for\n    debugging purposes on low data volumes as the entire output is\n    collected and stored in the driver’s memory after every trigger.\n\n```\n    writeStream\n        .format(\"console\")\n        .start()\n```\n\n-   **Memory sink (for debugging)** - The output is stored in memory as\n    an in-memory table. Both, Append and Complete output modes,\n    are supported. This should be used for debugging purposes on low\n    data volumes as the entire output is collected and stored in the\n    driver’s memory. Hence, use it with caution.\n\n```\n    writeStream\n        .format(\"memory\")\n        .queryName(\"tableName\")\n        .start()\n```\n\nSome sinks are not fault-tolerant because they do not guarantee\npersistence of the output and are meant for debugging purposes only. See\nthe earlier section on [fault-tolerance\nsemantics](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#fault-tolerance-semantics).\nHere are the details of all the sinks in Spark.\n\n<table class=\"table\">\n  <tr>\n    <th>Sink</th>\n    <th>Supported Output Modes</th>\n    <th>Options</th>\n    <th>Fault-tolerant</th>\n    <th>Notes</th>\n  </tr>\n  <tr>\n    <td><b>File Sink</b></td>\n    <td>Append</td>\n    <td>\n        <code>path</code>: path to the output directory, must be specified.\n        <br /><br />\n        For file-format-specific options, see the related methods in DataFrameWriter\n        (<a href=\"https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.DataFrameWriter\">Scala</a>/<a href=\"https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/DataFrameWriter.html\">Java</a>/<a href=\"https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter\">Python</a>/<a href=\"https://spark.apache.org/docs/2.2.0/api/R/write.stream.html\">R</a>).\n        E.g. for \"parquet\" format options see <code>DataFrameWriter.parquet()</code>\n    </td>\n    <td>Yes</td>\n    <td>Supports writes to partitioned tables. Partitioning by time may be useful.</td>\n  </tr>\n  <tr>\n    <td><b>Foreach Sink</b></td>\n    <td>Append, Update, Compelete</td>\n    <td>None</td>\n    <td>Depends on ForeachWriter implementation</td>\n    <td>More details in the <a href=\"structured-streaming-programming-guide.html#using-foreach\">next section</a></td>\n  </tr>\n  <tr>\n    <td><b>Console Sink</b></td>\n    <td>Append, Update, Complete</td>\n    <td>\n        <code>numRows</code>: Number of rows to print every trigger (default: 20)\n        <br />\n        <code>truncate</code>: Whether to truncate the output if too long (default: true)\n    </td>\n    <td>No</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><b>Memory Sink</b></td>\n    <td>Append, Complete</td>\n    <td>None</td>\n    <td>No. But in Complete Mode, restarted query will recreate the full table.</td>\n    <td>Table name is the query name.</td>\n  </tr>\n  <tr>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0e12252d-983a-4249-97f5-63924f592349"},{"version":"CommandV1","origId":207916,"guid":"119d1a9e-ee45-4e56-b21e-b6c972addb5d","subtype":"command","commandType":"auto","position":28.9609375,"command":"%md\nNote that you have to call `start()` to actually start the execution of\nthe query. This returns a StreamingQuery object which is a handle to the\ncontinuously running execution. You can use this object to manage the\nquery, which we will discuss in the next subsection. For now, let’s\nunderstand all this with a few examples.\n\n```\n    // ========== DF with no aggregations ==========\n    val noAggDF = deviceDataDf.select(\"device\").where(\"signal > 10\")   \n\n    // Print new data to console\n    noAggDF\n      .writeStream\n      .format(\"console\")\n      .start()\n\n    // Write new data to Parquet files\n    noAggDF\n      .writeStream\n      .format(\"parquet\")\n      .option(\"checkpointLocation\", \"path/to/checkpoint/dir\")\n      .option(\"path\", \"path/to/destination/dir\")\n      .start()\n\n    // ========== DF with aggregation ==========\n    val aggDF = df.groupBy(\"device\").count()\n\n    // Print updated aggregations to console\n    aggDF\n      .writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .start()\n\n    // Have all the aggregates in an in-memory table\n    aggDF\n      .writeStream\n      .queryName(\"aggregates\")    // this query name will be the table name\n      .outputMode(\"complete\")\n      .format(\"memory\")\n      .start()\n\n    spark.sql(\"select * from aggregates\").show()   // interactively query in-memory table\n    ```","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fc3e4bb4-3064-4ccc-9e7a-76d667282c80"},{"version":"CommandV1","origId":207917,"guid":"812c80c9-dbf2-44a4-a6a6-6bda61418de9","subtype":"command","commandType":"auto","position":29.9609375,"command":"%md\n#### Using Foreach\n\nThe `foreach` operation allows arbitrary operations to be computed on\nthe output data. As of Spark 2.1, this is available only for Scala and\nJava. To use this, you will have to implement the interface\n`ForeachWriter`\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.ForeachWriter)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/ForeachWriter.html)\ndocs), which has methods that get called whenever there is a sequence of\nrows generated as output after a trigger. Note the following important\npoints.\n\n-   The writer must be serializable, as it will be serialized and sent\n    to the executors for execution.\n\n-   All the three methods, `open`, `process` and `close` will be called\n    on the executors.\n\n-   The writer must do all the initialization (e.g. opening connections,\n    starting a transaction, etc.) only when the `open` method is called.\n    Be aware that, if there is any initialization in the class as soon\n    as the object is created, then that initialization will happen in\n    the driver (because that is where the instance is being created),\n    which may not be what you intend.\n\n-   `version` and `partition` are two parameters in `open` that uniquely\n    represent a set of rows that needs to be pushed out. `version` is a\n    monotonically increasing id that increases with every trigger.\n    `partition` is an id that represents a partition of the output,\n    since the output is distributed and will be processed on\n    multiple executors.\n\n-   `open` can use the `version` and `partition` to choose whether it\n    needs to write the sequence of rows. Accordingly, it can return\n    `true` (proceed with writing), or `false` (no need to write). If\n    `false` is returned, then `process` will not be called on any row.\n    For example, after a partial failure, some of the output partitions\n    of the failed trigger may have already been committed to a database.\n    Based on metadata stored in the database, the writer can identify\n    partitions that have already been committed and accordingly return\n    false to skip committing them again.\n\n-   Whenever `open` is called, `close` will also be called (unless the\n    JVM exits due to some error). This is true even if `open`\n    returns false. If there is any error in processing and writing the\n    data, `close` will be called with the error. It is your\n    responsibility to clean up state (e.g. connections,\n    transactions, etc.) that have been created in `open` such that there\n    are no resource leaks.\n\nManaging Streaming Queries\n--------------------------\n\nThe `StreamingQuery` object created when a query is started can be used\nto monitor and manage the query.\n\n```\n    val query = df.writeStream.format(\"console\").start()   // get the query object\n\n    query.id          // get the unique identifier of the running query that persists across restarts from checkpoint data\n\n    query.runId       // get the unique id of this run of the query, which will be generated at every start/restart\n\n    query.name        // get the name of the auto-generated or user-specified name\n\n    query.explain()   // print detailed explanations of the query\n\n    query.stop()      // stop the query\n\n    query.awaitTermination()   // block until query is terminated, with stop() or with error\n\n    query.exception       // the exception if the query has been terminated with error\n\n    query.recentProgress  // an array of the most recent progress updates for this query\n\n    query.lastProgress    // the most recent progress update of this streaming query\n```","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"89fab64b-0bfe-4e63-99e3-d511545d33ba"},{"version":"CommandV1","origId":207918,"guid":"70df589a-fc95-46ff-bee4-4cc437ddacb1","subtype":"command","commandType":"auto","position":30.9609375,"command":"%md\nYou can start any number of queries in a single SparkSession. They will\nall be running concurrently sharing the cluster resources. You can use\n`sparkSession.streams()` to get the `StreamingQueryManager`\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryManager)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/StreamingQueryManager.html)/[Python](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.streaming.StreamingQueryManager)\ndocs) that can be used to manage the currently active queries.\n\n```\n    val spark: SparkSession = ...\n\n    spark.streams.active    // get the list of currently active streaming queries\n\n    spark.streams.get(id)   // get a query object by its unique id\n\n    spark.streams.awaitAnyTermination()   // block until any one of them terminates\n```\n\nMonitoring Streaming Queries\n----------------------------\n\nThere are two APIs for monitoring and debugging active queries -\ninteractively and asynchronously.\n\n### Interactive APIs\n\nYou can directly get the current status and metrics of an active query\nusing `streamingQuery.lastProgress()` and `streamingQuery.status()`.\n`lastProgress()` returns a `StreamingQueryProgress` object in\n[Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryProgress)\nand\n[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/StreamingQueryProgress.html)\nand a dictionary with the same fields in Python. It has all the\ninformation about the progress made in the last trigger of the stream -\nwhat data was processed, what were the processing rates, latencies, etc.\nThere is also `streamingQuery.recentProgress` which returns an array of\nlast few progresses.\n\nIn addition, `streamingQuery.status()` returns a `StreamingQueryStatus`\nobject in\n[Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryStatus)\nand\n[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/StreamingQueryStatus.html)\nand a dictionary with the same fields in Python. It gives information\nabout what the query is immediately doing - is a trigger active, is data\nbeing processed, etc.\n\nHere are a few examples.\n\n```\n    val query: StreamingQuery = ...\n\n    println(query.lastProgress)\n\n    /* Will print something like the following.\n\n    {\n      \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\n      \"runId\" : \"88e2ff94-ede0-45a8-b687-6316fbef529a\",\n      \"name\" : \"MyQuery\",\n      \"timestamp\" : \"2016-12-14T18:45:24.873Z\",\n      \"numInputRows\" : 10,\n      \"inputRowsPerSecond\" : 120.0,\n      \"processedRowsPerSecond\" : 200.0,\n      \"durationMs\" : {\n        \"triggerExecution\" : 3,\n        \"getOffset\" : 2\n      },\n      \"eventTime\" : {\n        \"watermark\" : \"2016-12-14T18:45:24.873Z\"\n      },\n      \"stateOperators\" : [ ],\n      \"sources\" : [ {\n        \"description\" : \"KafkaSource[Subscribe[topic-0]]\",\n        \"startOffset\" : {\n          \"topic-0\" : {\n            \"2\" : 0,\n            \"4\" : 1,\n            \"1\" : 1,\n            \"3\" : 1,\n            \"0\" : 1\n          }\n        },\n        \"endOffset\" : {\n          \"topic-0\" : {\n            \"2\" : 0,\n            \"4\" : 115,\n            \"1\" : 134,\n            \"3\" : 21,\n            \"0\" : 534\n          }\n        },\n        \"numInputRows\" : 10,\n        \"inputRowsPerSecond\" : 120.0,\n        \"processedRowsPerSecond\" : 200.0\n      } ],\n      \"sink\" : {\n        \"description\" : \"MemorySink\"\n      }\n    }\n    */\n\n\n    println(query.status)\n\n    /*  Will print something like the following.\n    {\n      \"message\" : \"Waiting for data to arrive\",\n      \"isDataAvailable\" : false,\n      \"isTriggerActive\" : false\n    }\n    */\n```\n\n### Asynchronous API\n\nYou can also asynchronously monitor all queries associated with a\n`SparkSession` by attaching a `StreamingQueryListener`\n([Scala](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryListener)/[Java](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/streaming/StreamingQueryListener.html)\ndocs). Once you attach your custom `StreamingQueryListener` object with\n`sparkSession.streams.attachListener()`, you will get callbacks when a\nquery is started and stopped and when there is progress made in an\nactive query. Here is an example,\n\n```\n    val spark: SparkSession = ...\n\n    spark.streams.addListener(new StreamingQueryListener() {\n        override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {\n            println(\"Query started: \" + queryStarted.id)\n        }\n        override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {\n            println(\"Query terminated: \" + queryTerminated.id)\n        }\n        override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {\n            println(\"Query made progress: \" + queryProgress.progress)\n        }\n    })\n```\n\nRecovering from Failures with Checkpointing\n-------------------------------------------\n\nIn case of a failure or intentional shutdown, you can recover the\nprevious progress and state of a previous query, and continue where it\nleft off. This is done using checkpointing and write ahead logs. You can\nconfigure a query with a checkpoint location, and the query will save\nall the progress information (i.e. range of offsets processed in each\ntrigger) and the running aggregates (e.g. word counts in the [quick\nexample](structured-streaming-programming-guide.html#quick-example)) to\nthe checkpoint location. This checkpoint location has to be a path in an\nHDFS compatible file system, and can be set as an option in the\nDataStreamWriter when [starting a\nquery](structured-streaming-programming-guide.html#starting-streaming-queries).\n\n```\n    aggDF\n      .writeStream\n      .outputMode(\"complete\")\n      .option(\"checkpointLocation\", \"path/to/HDFS/dir\")\n      .format(\"memory\")\n      .start()\n```\n\nWhere to go from here\n=====================\n\n-   Examples: See and run the\n    [Scala](https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/sql/streaming)/[Java](https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/sql/streaming)/[Python](https://github.com/apache/spark/tree/master/examples/src/main/python/sql/streaming)/[R](https://github.com/apache/spark/tree/master/examples/src/main/r/streaming) examples.\n-   Spark Summit 2016 Talk - [A Deep Dive into Structured\n    Streaming](https://spark-summit.org/2016/events/a-deep-dive-into-structured-streaming/)\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7b1b1267-18f8-4161-be27-75fbcfa52970"}],"dashboards":[],"guid":"d7f34cce-3cc9-45c9-9c34-f2e5e5a08acb","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
