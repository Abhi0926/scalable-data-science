---
title: On-Premise Cluster
permalink: /sds/basics/infrastructure/onpremise/
sidebar:
  nav: "lMenu-SDS-2.2"
---

**Advanced**

## How to Work with Apache Spark on your On-Premise Cluster of Computers?

It is important to be able to run Spark locally on your locally available computer cluster.  
Many real-world datasets cannot be analysed on your laptop or a single desktop computer.
 
Some main ways of doing this include:

* [Automating private bare-metal NUC cluster setup with Cobbler](NUCcluster/)
* [Installing Spark-Hadoop-Yarn-Hive-Zeppelin without Root Access](rootless/)
* [Some Setups for your On-Premise Cluster of Computers](setups/)
  * Setting up networking
  * Setting up a Ubuntu Server Master
* [BASH your own Spark-Yarn-HDFS Cluster](bashSparkCluster/)
* Under Study/Work: [kubernetes for container orchestration to create on-premise or public cloud clusters](../kubernetes/)
  * [Spark 2.3 has native Kubernetes support now - March 2018](https://spark.apache.org/docs/latest/running-on-kubernetes.html)

