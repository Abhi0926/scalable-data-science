---
title: Infrastructure
permalink: /sds/basics/infrastructure/
sidebar:
  nav: "lMenu-SDS-2.2"
---

## Distributed Computing Requirements

We need infrastructure for processing big data. 

Many real-world datasets cannot be analysed on your laptop or a single desktop computer.

However, we can learn how to use Apache Spark on a laptop and even analyse big data sets *from* our laptop by running the jobs on a public or private *cloud*, a large cluster of computers that is housed elsewhere but accessible remotely. 

One of the easiest way of doing is through a fully managed Spark cluster databricks cluster:

* [datbricks community edition - Sign Up! It is FREE for all!](https://community.cloud.databricks.com/login.html)
To work on the databricks community edition all you need is a laptop computer with a browser and internet connection.
 
It is important to be able to run Spark locally on your locally available computer cluster.

* Learn [how to work with Spark on or from a laptop](/sds/basics/local/).

We will see some more **advanced** topics on infrastructure and these can be skipped by most readers.

