-------- Forwarded Message --------
Subject:        Youtube
Date:   Tue, 24 Apr 2018 14:35:22 +0100
To:     ICT <ict@article19.org>



Dear all,

YouTube has just released a transparency report on community guideline
violations 

- [https://transparencyreport.google.com/youtube-policy/overview?hl=en](https://transparencyreport.google.com/youtube-policy/overview?hl=en) 

that may be of interest to those working on free expression/hate speech/fake
news topics. Some highlights:

  * 6.7 million (81%) of the 8.7 million videos removed for community
    guideline violations since October 2017 originated from automated
    flagging systems.  1.1 million (14%) came from trusted flaggers. 
    The remaining 5% of removed videos came from user flags and
    government requests. 
  * 9.3 million individual videos received user flags, but only 1.6
    million were removed for user flags (a 17% removal rate)
  * Sexual content (30.1%) and Spam (26.4%) account for the vast
    majority of user flags.  Terrorism accounts for 1.6% and hate speech
    15.6%.
  * "In 2017, automated systems helped flag more violent extremist
    content for human review and our advances in machine learning
    enabled us to take down nearly 70% of violent extremism
    content within 8 hours of upload and nearly half of it in 2 hours.
    Now, we are applying the lessons weâ€™ve learned from our work
    fighting violent extremism content to tackle other problematic content."
  * "As of December 2017, 98% of the videos we removed for violent
    extremism were identified by our machine learning algorithms."

This report demonstrates how reliant content hosting platforms are on
automated detection systems.  The claim that 70% of violent extremism
content is removed within 8 hours of upload and half within 2
hours should raise eyebrows about the level of quality of those human
reviews.
